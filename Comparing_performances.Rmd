---
title: "Comparing model performances"
author: "Florian Huber, Leonard Dubois"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

# Setup, library loading

This part is the general setup, whichever model we are using.

```{r setup}
# function to check if a package is installed, if so, load it, else install and then load it
source("./R/ipak.R")
# set chunk options and load libraries, load custom functions
source("./setup.R")
```

Modify some options for knitr. 

```{r global_options, include = FALSE}
knitr::opts_chunk$set(fig.width = 18, fig.height = 14, cache = TRUE)
theme_set(theme_bw())
theme_update(text = element_text(size = 16))
```


# Data preparation

## Reading in `run_results_from_server` and extracting information

On the cluster, each of the rows of `matrix_container` was run; each row was a different combination 
of models, feature preselections etc and was saved in a separate results object 
(`./run_results_from_server/matrix_container_result`). Since the results objects are too big for 
putting them into `matrix_container` we will add new columns just containing the parts we are 
interested in. 

Note 2018-11-05: All extracted information is now in `matrix_container_ext`. `matrix_container_with_extractions` 
and `stability_assessment` are now deprecated and I deleted the corresponding chunks (commit after 
2b89). This was also necessary for correct auc and precision-recall curve calculation and plotting. 

```{r reading_in, message = FALSE}
# What follows is a loop to extract information from our results objects and adding this into 
# new columns. Since this takes some time, it's written in a way that the matrix_container can be 
# updated. 

# results we already have
if (file.exists("./data/programmatic_output/matrix_container_ext.rds")) {
   matrix_container <- readRDS("./data/programmatic_output/matrix_container_ext.rds")
} else {
   matrix_container$hyperparam_grid_name <- names(matrix_container$hyperparam_grid)
   matrix_container$perf_measures <- list(NA)
   matrix_container$thresh_vs_perf <- list(NA)
   matrix_container$pred_data <- list(NA)
   matrix_container$prob.moa_stabilities <- list(NA)
   matrix_container$pred_data_with_n_signif <- list(NA)
}

# for fastness:
most_ia_mat <- matrix_container$drug_feature_matrices[[180]]
stopifnot(nrow(most_ia_mat) == 78)
the_matrix_complete_strainsgenesremoved <- 
  readRDS("./data/programmatic_output/the_matrix_complete_strainsgenesremoved.rds")

# loop to read in results files
for (rownum in seq_len(nrow(matrix_container))) {
   matrix_container_row <- matrix_container[rownum, ]
   available_files <- list.files("./run_results_from_server/matrix_container_result/", 
     pattern = "xgboost|rf|lasso")
   moas <- c("dna", "cell_wall", "protein_synthesis", "membrane_stress")

  # look for a file corresponding to a row
  targetfile <- make_filename(matrix_container_row)
  my_match <- available_files[available_files == targetfile]
   
   # check if values are not defined: if yes: check if matching file exists or continue
   # if values are already defined means we parsed a corresponding results object previously so we 
   # can continue
   if (length(matrix_container_row$pred_data[[1]]) == 1 && is.na(matrix_container_row$pred_data[[1]])) {
      if (length(my_match) > 0) {
         message("Match for unpopulated row ", rownum, "\t==>\t", my_match, sep = "")
      } else {
         message("Didn't find a match for line ", rownum, ", continuing to next line.", sep = "")
         next
      }
   } else {
      message("Row ", rownum, " already populated, continuing to next line.")
      next
   }
   
   resultsobj <- readRDS(paste0("./run_results_from_server/matrix_container_result/", my_match))
   
   perf_measures <- get_auc_info(resultsobj)
   # this data frame is just for convenience as its information is already contained in perf_measures
   thresh_vs_perf <- map_dfr(perf_measures$thresh_vs_perf, "data")
   
   pred_data <- suppressWarnings(get_pred_data(resultsobj, matrix_container_row))
   pred_data <- add_info_to_pred_data(pred_data, most_ia_mat = most_ia_mat)
   
   prob.moa_stabilities <- get_prob.moa_sds(pred_data)
   
   pred_data_with_n_signif <- get_nsignif(drugfeatmat = matrix_container_row$drug_feature_matrices[[1]], 
     predDataTbl = pred_data, the_matrix = the_matrix_complete_strainsgenesremoved)
   
   matrix_container[rownum, c("perf_measures", "pred_data", "prob.moa_stabilities", 
     "pred_data_with_n_signif", "thresh_vs_perf")] <- 
     list(list(perf_measures), list(pred_data), list(prob.moa_stabilities), 
       list(pred_data_with_n_signif), list(thresh_vs_perf))
   
   rm(resultsobj)
   gc()
}

matrix_container$feat_preselect <- 
   factor(matrix_container$feat_preselect, levels =  c("top5pct", "top10pct", "top15pct", 
     "top20pct", "top25pct", "top30pct", "top40pct", "top50pct", "keepall"))

saveRDS(object = matrix_container, file = "./data/programmatic_output/deprecated/matrix_container_ext.rds")
```



# Data analysis

## Model performances: comparing AUCs

Which models have the best AUCs and how does it change between models, feature_preselections and so 
on. Also allows assessing how stable AUCs are across repetitions of nested CV. 

### AUCs: impact of feat_preselect 

Without chemical features, using most_interactions. "keepall" corresponds to 1712 features.

```{r}
foo <- filter_container(matrix_container, dosgs = "most_interactions", chemfeats = FALSE) %>%
  select(fitted_model, feat_preselect, perf_measures) %>%
  unnest()

plot_auc_vs_moa(foo, xaxis = "moa_modelled", y = "auc", colourvar = "feat_preselect", 
                additional_info = "(no chem. feats, most_interactions)")
```

Conclusions: it depends on the MoA:

- `cell_wall` performs best overall and is relatively stable. top15pct seems to be better for RF 
whereas top5pct is best for lasso. For lasso performance seems to drop slightly with the higher 
number of features that are kept. 
- `dna` also has a decent performance (a bit better for RF) which is again quite stable. For lasso 
again the performance seems to drop slightly with the increasing number of features. But perhaps 
it also just reflects the instability of the lasso model as performance goes up again later on. 
- `membrane_stress` has overall the worst performance and is also the least stable. 
- `protein_synthesis` performs poorly for RF and lasso with top5 and top10pct but performance goes 
up with top15pct (RF) and top20pct (lasso), respectively. 

When looking closely: cell_wall and dna are a bit more stable with RF. 

But how does this compare to partial AUCs (fpr.stop = 0.1)? Note that for this threshold random 
chance is a partial auc of `r round((0.1^2) / 2, digits = 3)`. 

```{r}
plot_auc_vs_moa(foo, xaxis = "moa_modelled", y = "part_auc_01", colourvar = "feat_preselect", 
                additional_info = "(no chem. feats, most_interactions)")
```

The trends are the same for cell_wall and dna. For protein synthesis, it seems more clear that more 
features = better - take at least top20pct. Membrane_stress performs poorly and partial AUCs suggest 
that some of the curves do not perform better than chance at a low fpr cutoff. 

XGBOOST models don't seem to bring a particular performance benefit. As they vary a lot depending 
on the hyperparameter grid but are difficult to understand I'd rather prefer lasso/RF models. 


### AUCs: all dosages vs. most_interactions

```{r}
foo <- filter_container(matrix_container, chemfeats = FALSE) %>%
  select(fitted_model, drug_dosages, feat_preselect, perf_measures) %>%
  unnest()

plot_auc_vs_moa(foo, xaxis = "feat_preselect", y = "auc", colourvar = "drug_dosages", 
                facet_expr = quote(facet_grid(fitted_model ~ moa_modelled)), 
                additional_info = "(no chem. feats)")
```


```{r, for_poster, echo = FALSE}
# this is just for the poster
foo <- filter(foo, fitted_model %in% c("classif.randomForest", "classif.glmnet"))

foo$moa_modelled <- 
   fct_recode(foo$moa_modelled, 
              "Cell Wall" = "cell_wall", 
              "Protein Synthesis" = "protein_synthesis", 
              "Membrane Stress" = "membrane_stress", 
              "DNA" = "dna", 
              "Other" = "oxidative_stress", 
              "Other" = "pmf", 
              "Other" = "protein_qc")

foo$fitted_model <- 
   fct_recode(foo$fitted_model, 
              "Random forests" = "classif.randomForest", 
              "Lasso" = "classif.glmnet")

# foofilt <- filter(foo, fitted_model == "Lasso", moa_modelled == "DNA")
foo_by_repndosg <- 
   group_by(foo, fitted_model, moa_modelled, drug_dosages, feat_preselect) %>%
   summarise(median_auc = median(auc), 
             lower = boxplot.stats(auc)$stats[2], 
             upper = boxplot.stats(auc)$stats[4])
foo_by_repndosg

ggplot(foo, aes(x = feat_preselect, y = auc, colour = drug_dosages)) + 
   # geom_boxplot(position = position_dodge(), outlier.shape = NA) + 
   # geom_point(position = position_jitterdodge(jitter.width = 0.1), size = 1, alpha = 0.5) + 
   geom_pointrange(data = foo_by_repndosg, aes(y = median_auc, ymin = lower, ymax = upper), fatten = 1) + 
   geom_line(data = foo_by_repndosg, aes(y = median_auc, group = drug_dosages)) + 
   # stat_summary(geom = "line", group = "drug_dosages", fun.y = "median") + # this doesn't work, it will not draw separate line per drug_dosages - why?
   facet_grid(fitted_model ~ moa_modelled) + 
   labs(x = "Subset of features kept", y = "AUC", title = "Performances of MoA predictions", 
        subtitle = "Medians +- IQR across CV repeats indicated") + 
   theme_bw() + 
   theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10), text = element_text(size = 14)) + 
   scale_colour_manual("Drug concentrations", values = c("#1b9e77", "#d95f02"), 
                       labels = c("All measured", "Conc. with \nmost interactions"))

ggsave("./plots/POSTER_performance_comp.pdf", width = 9, height = 5)
```

Conclusions: RF: most_interactions performs a bit better for cell_wall and dna, all dosages 
clearly improves performances for membrane_stress and, for lower feature numbers, protein_synthesis. 
Similar trend for lasso and membrane_stress/protein_synthesis. For lasso and dna/cell_wall it's less 
uniform, all dosages performs better for cell_wall and high feature numbers. 

Let's check again for partial AUCs:

```{r}
plot_auc_vs_moa(foo, xaxis = "feat_preselect", y = "part_auc_01", colourvar = "drug_dosages", 
                facet_expr = quote(facet_grid(fitted_model ~ moa_modelled)), 
                additional_info = "(no chem. feats)")
```

Trends in favour of all dosages are even clearer. cell_wall and RF has an inverted trend with all 
dosages now performing better. Overall, performance measures argue for using all dosages. 

There doesn't seem to be a big difference. 


### AUCs: with and without chemical features

```{r}
foo <- filter_container(matrix_container, dosgs = "all") %>%
  select(fitted_model, drug_dosages, chemical_feats, feat_preselect, perf_measures) %>%
  unnest()

plot_auc_vs_moa(foo, xaxis = "feat_preselect", y = "auc", colourvar = "chemical_feats", 
                facet_expr = quote(facet_grid(fitted_model ~ moa_modelled)), 
                additional_info = "(all dosages)")

plot_auc_vs_moa(foo, xaxis = "feat_preselect", y = "part_auc_01", colourvar = "chemical_feats", 
                facet_expr = quote(facet_grid(fitted_model ~ moa_modelled)), 
                additional_info = "(all dosages)")
```

Conclusions: chemical features bring big improvement in performance for protein_synthesis and, 
for occasional repeats, for membrane_stress. Interestingly, when choosing "most_interactions", the 
performance improvement for protein_synthesis is only there for low feature numbers:

```{r}
foo <- filter_container(matrix_container, dosgs = "most_interactions") %>%
  select(fitted_model, drug_dosages, chemical_feats, feat_preselect, perf_measures) %>%
  unnest()

plot_auc_vs_moa(foo, xaxis = "feat_preselect", y = "auc", colourvar = "chemical_feats", 
                facet_expr = quote(facet_grid(fitted_model ~ moa_modelled)), 
                additional_info = "(most interactions)")

plot_auc_vs_moa(foo, xaxis = "feat_preselect", y = "part_auc_01", colourvar = "chemical_feats", 
                facet_expr = quote(facet_grid(fitted_model ~ moa_modelled)), 
                additional_info = "(most interactions)")
```

With partial AUCs.

```{r}
plot_auc_vs_moa(foo, xaxis = "feat_preselect", y = "part_auc_01", colourvar = "chemical_feats", 
                facet_expr = quote(facet_grid(fitted_model ~ moa_modelled)), 
                additional_info = "(most interactions)")
```


But caution: physicochemical features may not be truly predictive - compare file 
drug_drug_similarities_RDKit.pdf. 

From the plots above we would probably say: take all dosages, with chemical features, top20pct/
top25pct, slight preference for RF. 


## Model stabilities (of probabilities)

Stability here means: how much do prediction probabilities fluctuate for drugs across different ways 
of splitting the data (i.e. each repetition of the nested CV). 


### 'Ultimate plot'

Shows the distribution of probabilities for each drug across repeats of the nested CV. Can plot 
only one model at a time. Grey rectangles indicate the dosage with the most interactions. 

```{r, ultimate_plots}
tmp <- filter_container(matrix_container, feats = "keepall", chemfeats = FALSE, 
  models = "classif.randomForest", dosgs = "all")

ultimate_plot_new(tmp, filename = "./plots/ultimate_plot_RF_keepall_alldosages_nochemfeats.pdf")


# let's try also for one of the lasso models
tmp <- filter_container(matrix_container, feats = "keepall", chemfeats = FALSE, 
  models = "classif.glmnet", dosgs = "all")

ultimate_plot_new(tmp, filename = "./plots/ultimate_plot_LASSO_keepall_alldosages_nochemfeats.pdf")


# and now both of them with just the "most_interactions" dosage
tmp <- filter_container(matrix_container, feats = "keepall", chemfeats = FALSE, 
  models = "classif.randomForest", dosgs = "most_interactions")

ultimate_plot_new(tmp, filename = "./plots/ultimate_plot_RF_keepall_mostinteractions_nochemfeats.pdf")


tmp <- filter_container(matrix_container, feats = "keepall", chemfeats = FALSE, 
  models = "classif.glmnet", dosgs = "most_interactions")

ultimate_plot_new(tmp, filename = "./plots/ultimate_plot_LASSO_keepall_mostinteractions_nochemfeats.pdf")
```


### Stabilities of probabilities: impact of feat_preselect

Fixing the following:

- chemical_feats == FALSE
- drug_dosages == "most_interactions"
- moa_modelled == truth

What is the impact of feat_preselect on model stability, depending on the model type and the 
hyperparameter grid?

```{r}
foo <- filter_container(matrix_container, chemfeats = FALSE, dosgs = "most_interactions") %>%
  select(prob.moa_stabilities, feat_preselect, chemical_feats, fitted_model, drug_dosages) %>%
  unnest() %>%
  filter(moa_modelled_is_truth)

plot_auc_vs_moa(foo, xaxis = "process_broad", yaxis = "prob.moa_sd2", colourvar = "feat_preselect", 
                facet_expr = quote(facet_grid(fitted_model ~ .)), 
                additional_info = "no chem. feats, most_interactions, MoA modelled = MoA truth")


#### TO DO: we need a more systematic comparison here
# # What about 'classif.xgboost'? - Possible that stabilities depend on hyperparameter grid?
# foo <- foo[foo$fitted_model == "classif.xgboost", ]
# plot_auc_vs_moa(foo, xaxis = "process_broad", yaxis = "prob.moa_sd", colourvar = "feat_preselect", 
#                 facet_expr = quote(facet_grid(hyperparam_grid_name ~ .)), 
#                 additional_info = "classif.xgboost only\nno chem. feats, most_interactions, MoA modelled = MoA truth")
# 
# #ggsave(plot = p, filename = "./plots/Prob_stabilities_by_feat_preselect_onlyxgboost.pdf")
```


```{r, for_poster_again, echo = FALSE}
# just for the poster
foo <- 
   matrix_container %>%
   select(drug_dosages, feat_preselect, chemical_feats, fitted_model, prob.moa_stabilities) %>%
   unnest() %>%
   filter(fitted_model %in% c("classif.randomForest", "classif.glmnet"), moa_modelled_is_truth, 
          drug_dosages == "all", !chemical_feats)

foo$process_broad <- 
   fct_recode(foo$process_broad, 
              "Cell Wall" = "cell_wall", 
              "Protein Synthesis" = "protein_synthesis", 
              "Membrane Stress" = "membrane_stress", 
              "DNA" = "dna", 
              "Other" = "oxidative_stress", 
              "Other" = "pmf", 
              "Other" = "protein_qc")

foo$fitted_model <- 
   fct_recode(foo$fitted_model, 
              "Random forests" = "classif.randomForest", 
              "Lasso" = "classif.glmnet")

ggplot(foo, aes(x = feat_preselect, y = prob.moa_sd2, colour = drug_dosages)) + 
   stat_summary(geom = "pointrange", group = "drug_dosages", fun.y = "median", fatten = 1, 
                fun.ymin = function(x) {fivenum(x)[2]}, 
                fun.ymax = function(x) {fivenum(x)[4]}) + 
   stat_summary(geom = "line", fun.y = median, group = "drug_dosages") + 
   facet_grid(fitted_model ~ process_broad) + 
   labs(x = "Subset of features kept", y = "Median s.d. of prediction probability", 
        title = "Stabilities of MoA prediction probabilities", 
        subtitle = "Indicates s.d. of probabilities across nested CV repeats") + 
   theme_bw() + 
   theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10), text = element_text(size = 14)) + 
   scale_colour_manual("Drug concentrations", values = c("#1b9e77", "#d95f02"), 
                       labels = c("All measured", "Conc. with \nmost interactions"))

ggsave("./plots/POSTER_stability_comp.pdf", width = 9, height = 5)
```


Conclusion: No apparent correlation between feat_preselect and stabilities of probabilities. 
Random forests is the most stable model, lasso the least stable one although the latter 
has a big difference between individual drugs: some of them are as stable as with RF. These trends 
are also apparent in the ultimate plot. 

Boosted trees is in between. For boosted trees the don't seem to differ between different 
hyperparameter grids. 


### Stabilities of probabilities: impact of all dosages vs. most_interactions

Just like above, except that we now want to see whether models become more or less stable if all 
dosages are used or only one dosage with most interactions. 

```{r}
foo <- filter_container(matrix_container, chemfeats = FALSE) %>%
  select(prob.moa_stabilities, feat_preselect, chemical_feats, fitted_model, drug_dosages) %>%
  unnest() %>%
  filter(moa_modelled_is_truth)

plot_auc_vs_moa(foo, xaxis = "feat_preselect", yaxis = "prob.moa_sd2", colourvar = "drug_dosages", 
                facet_expr = quote(facet_grid(fitted_model ~ process_broad)), 
                additional_info = "no chem. feats, MoA modelled = MoA truth")


# # focus again only on classif.xgboost
# foo <- foo[foo$fitted_model == "classif.xgboost", ]
# 
# plot_auc_vs_moa(foo, xaxis = "process_broad", yaxis = "prob.moa_sd2", colourvar = "drug_dosages", 
#                 facet_expr = quote(facet_grid(hyperparam_grid_name ~ feat_preselect)), 
#                 additional_info = "no chem. feats, MoA modelled = MoA truth")
```


### Stabilities of probabilities: impact of whether MoA modelled = truth

Like above but this time we do not focus on the probabilities where the MoA that was being modelled 
is the same as the MoA of the drug. So we ask whether drugs are more stably predicted to be their 
MoA or whether they are more stably predicted to be not their MoA. Check once for all dosages and 
once for most_interactions. 

```{r}
foo <- filter_container(matrix_container, chemfeats = FALSE, dosgs = "most_interactions") %>%
  select(prob.moa_stabilities, feat_preselect, chemical_feats, fitted_model, drug_dosages) %>%
  unnest() %>%
  filter(process_broad %in% c("cell_wall", "dna", "membrane_stress", "protein_synthesis"))

plot_auc_vs_moa(foo, xaxis = "feat_preselect", yaxis = "prob.moa_sd2", 
                colourvar = "moa_modelled_is_truth", 
                facet_expr = quote(facet_grid(fitted_model ~ process_broad)), 
                additional_info = "(no chem. feats, most_interactions)")


foo <- filter_container(matrix_container, chemfeats = FALSE, dosgs = "all") %>%
  select(prob.moa_stabilities, feat_preselect, chemical_feats, fitted_model, drug_dosages) %>%
  unnest() %>%
  filter(process_broad %in% c("cell_wall", "dna", "membrane_stress", "protein_synthesis"))

plot_auc_vs_moa(foo, xaxis = "feat_preselect", yaxis = "prob.moa_sd2", 
                colourvar = "moa_modelled_is_truth", 
                facet_expr = quote(facet_grid(fitted_model ~ process_broad)), 
                additional_info = "(no chem. feats, all dosages)")
```

Conclusion: predictions where drug MoA == MoA modelled are more stable except for lasso where it's 
vice versa. Interestingly, with all dosages, predictions where MoA == moa_modelled get a lot more 
unstable for RF. 


## Selected ROC/precision-recall curves

Random forests and lasso. 
Moreover, take with chemical features and using all dosages. Finally, top25pct/keepall seemed to 
be decent feature preselections. So what do the ROC curves look like? 

```{r}
tmp <- filter(matrix_container, 
              fitted_model %in% c("classif.randomForest", "classif.glmnet"), 
              feat_preselect %in% c("top25pct", "keepall"), 
              drug_dosages == "all")

plot_perf_from_container(tmp[!tmp$chemical_feats, ], what = "ROC", col_var = "feat_preselect", 
  row_var = "fitted_model")

plot_perf_from_container(tmp[tmp$chemical_feats, ], what = "ROC", col_var = "feat_preselect", 
  row_var = "fitted_model")
```

Note: for cell_wall/dna/protein_synthesis: even though lasso and random forests are very similar 
for total AUCs (check top25pct), ROC curve shapes and, consequently, partial AUCs (see also above) 
show that random forests performs a lot better. Only for membrane_stress, lasso is the clear winner. 
But note the issue with probabilities, showing ROC curves for each repeat might still be useful. 

To be complete, here the precision-recall curves: 

```{r}
tmp <- tmp[!tmp$chemical_feats, ]

plot_perf_from_container(tmp, what = "prec-recall", col_var = "feat_preselect", 
  row_var = "fitted_model")
```

## Selected AUC/partial AUC comparison

There may be reasons though to work without chemical features and fewer dosages (see below). This 
would impact our performances as follows. I'm taking only top25pct and keepall for simplicity as 
they are the best feat_preselect cases depending on the MoA. 

```{r}
tmp <- filter_container(matrix_container, models = c("classif.randomForest", "classif.glmnet"), 
  dosgs = "all", feats = c("top25pct", "keepall")) %>%
  select(fitted_model, drug_dosages, feat_preselect, chemical_feats, perf_measures) %>%
  unnest()

plot_auc_vs_moa(tmp, xaxis = "moa_modelled", yaxis = "auc", colourvar = "chemical_feats", 
  facet_expr = quote(facet_grid(feat_preselect ~ fitted_model)), additional_info = "")
```

So one may be able to compensate for the loss of chemical features by using lasso. Here are the 
partial AUCs:

```{r}
plot_auc_vs_moa(tmp, xaxis = "moa_modelled", yaxis = "part_auc_01", colourvar = "chemical_feats", 
  facet_expr = quote(facet_grid(feat_preselect ~ fitted_model)), additional_info = "")
```


## Further investigations on drug dosages

### Distributions of dosages among drugs

It looks like using all dosages is better than most_interactions. At the same time, it seems 
plausible that some dosages will be "wrong" in terms of having a too low drug dosage. Moreover, 
some drugs were tested with only 1 dosage, others with 6, which would lead to an overweighting of 
the latter drugs. Perhaps there is a compromise such as using the 2 dosages with most interactions? 
Also, it'd be interesting to see if there is a correlation of the number of interactions and 
prediction probabilities. 

How does number of dosages distribute among the different drugs and MoA?

```{r}
# take a drug_feature_matrix which has all dosages and all drugs etc. 
tmp <- matrix_container$drug_feature_matrices[[1]]
tmp <- 
   select(tmp, drugname_typaslab, conc, process_broad) %>%
   group_by(drugname_typaslab, process_broad) %>%
   summarise(ndosages = n())

ggplot(tmp, aes(x = process_broad, y = ndosages, colour = process_broad)) + 
   ggbeeswarm::geom_beeswarm(groupOnX = TRUE, size = 2, cex = 1.5) + 
   theme_bw() + 
   theme(text = element_text(size = 18), axis.text.x = element_text(size = 18, angle = 45, hjust = 1))

tapply(tmp$ndosages, tmp$process_broad, mean)
```


### Correlating the number of significant interactions per drug with prediction probabilities

From the ultimate plot it seems like the dosages with the most interactions might also be the ones 
that are best at predicting a particular MoA. However, the trend is not crystal clear. Moreover, we 
don't know how many interactions this actually corresponds to. In addition, one may wonder whether 
some poorly predicted drugs are poorly predicted because they show only very few interactions even 
with "most_interactions". 


#### Investigating number of interactions and class probabilities

General question: is there something like a minimum number of significant interactions to get 
useful predictions? 

We check here for both random forests and lasso trained with all features, and without chemical 
features.

```{r}
the_matrix <- readRDS("./data/programmatic_output/the_matrix_complete_strainsgenesremoved.rds")

tmp <- filter_container(matrix_container, models = "classif.randomForest", feats = "keepall", 
  chemfeats = FALSE, dosgs = "all")
stopifnot(nrow(tmp) == 1)
tmp <- tmp$pred_data_with_n_signif[[1]]

ggplot(tmp, aes(x = n_signif, y = median_prob.moa)) + 
   geom_point(shape = 1) + 
   facet_grid(moa_modelled ~ moa_modelled_is_truth) + 
   theme_bw() + 
   theme(text = element_text(size = 18)) + 
   labs(x = "Number of significant interactions", y = "Median probability for MoA for each drug", 
        title = "Probabilities (y-axis) for MoA (right side) vs. number of sign. IAs (x-axis)\n(RF, all dosages, no chem feats, keepall)")


tmp <- filter_container(matrix_container, models = "classif.glmnet", feats = "keepall", 
  chemfeats = FALSE, dosgs = "all")
stopifnot(nrow(tmp) == 1)
tmp <- tmp$pred_data_with_n_signif[[1]]

ggplot(tmp, aes(x = n_signif, y = median_prob.moa)) + 
   geom_point(shape = 1) + 
   facet_grid(moa_modelled ~ moa_modelled_is_truth) + 
   theme_bw() + 
   theme(text = element_text(size = 18)) + 
   labs(x = "Number of significant interactions", y = "Median probability for MoA for each drug", 
        title = "Probabilities (y-axis) for MoA (right side) vs. number of sign. IAs (x-axis)\n
        (Lasso, all dosages, no chem feats, keepall)")
```

So there seems to be a trend, but only for cell_wall and dna. Interestingly, lasso seems to have 
different levels of a "background" probability for certain MoAs if moa_modelled != actual MoA. 

But what about _within_ drugs? Do predictions profit from taking concentration with higher number 
of significant interactions? Exclude drugs with just one dosage. 

First check how n_signif ranks and conc_ranks correlate.

```{r}
# exclude cases where moa_modelled != truth and drugs with just one concentration
tmp2 <- tmp[tmp$moa_modelled_is_truth & tmp$n_conc > 1, ]

ggplot(tmp2, aes(x = conc_rank, y = n_signif_rank)) + 
   geom_count() + 
   labs(title = "Ranks of number of significant interactions vs. ranks of concentrations, all drugs
        \n(cases with just one concentration excluded)")

cor(tmp2$conc_rank, tmp2$n_signif_rank, method = "spearman")

# novobiocin, for example:
filter(tmp2, drugname_typaslab == "NOVOBIOCIN")
```

Now compare prediction probabilities vs. ranks.

```{r}
ggplot(tmp2, aes(x = n_signif_rank, y = median_prob.moa)) + 
   geom_point(shape = 1) + 
   geom_line(aes(group = drugname_typaslab)) + 
   facet_grid(moa_modelled ~ n_conc) + 
   labs(title = "Median probability vs. concentration rank. Each line is a drug.")

ggplot(tmp2, aes(x = conc_rank, y = median_prob.moa)) + 
   geom_point(shape = 1) + 
   geom_line(aes(group = drugname_typaslab)) + 
   facet_grid(moa_modelled ~ n_conc) + 
   labs(title = "Median probability vs. n_signif interaction rank. Each line is a drug.")
```

It looks like it would actually be better to just take the two highest concentrations. This is also 
what a correlation test says:

```{r}
cor(x = tmp2$n_signif_rank, y = tmp2$median_prob.moa, method = "spearman")
cor(x = tmp2$conc_rank, y = tmp2$median_prob.moa, method = "spearman")
```

How do average probabilities change if we take the two dosages with most interactions/the two 
highest concentrations vs. the other dosages/concentrations? 

```{r}
tmp3 <- 
   tmp2 %>%
   group_by(moa_modelled, drugname_typaslab) %>%
   mutate(most2interactions = n_signif_rank %in% sort(n_signif_rank, decreasing = TRUE)[1:2]) %>% 
   # split into 2 groups: two highest interactions and all lower dosages
   filter(n() > 2) %>% # keep only drugs where we can actually compare the effects
   group_by(drugname_typaslab, moa_modelled, most2interactions) %>% # summarise within those groups
   summarise(mean.median_prob.moa = mean(median_prob.moa)) %>%
   ungroup()

ggplot(tmp3, aes(x = most2interactions, y = mean.median_prob.moa, colour = most2interactions)) + 
   geom_boxplot() + 
   geom_point() +
   geom_line(aes(group = drugname_typaslab), colour = "black") + 
   facet_wrap( ~ moa_modelled) + 
   labs(title = "Effect of using 2 dosages with most interactions (= label 'TRUE')\nvs. other 
        dosages (= label 'FALSE')")


tmp3 <- 
   tmp2 %>%
   group_by(moa_modelled, drugname_typaslab) %>%
   mutate(two_highest_dosgs = conc_rank %in% sort(conc_rank, decreasing = TRUE)[1:2]) %>% 
   # split into 2 groups: two highest concentrations and all lower dosages
   filter(n() > 2) %>% # keep only drugs where we can actually compare the effects
   group_by(drugname_typaslab, moa_modelled, two_highest_dosgs) %>% # summarise within those groups
   summarise(mean.median_prob.moa = mean(median_prob.moa)) %>%
   ungroup()

ggplot(tmp3, aes(x = two_highest_dosgs, y = mean.median_prob.moa, colour = two_highest_dosgs)) + 
   geom_boxplot() + 
   geom_point() +
   geom_line(aes(group = drugname_typaslab), colour = "black") + 
   facet_wrap( ~ moa_modelled) + 
   labs(title = "Effect of using 2 highest concentrations (= label 'TRUE')\nvs. other 
     concentrations (= label 'FALSE')")
```

Another argument against using all dosages is that it leads to a higher degree of correlation of 
the features between the observations: within dosage correlation is significantly higher than 
between dosage correlation (see notebook "Misc.Rmd") so our degree of dependency between 
observations is likely to drop. 


## Further investigations on chemical features

First thing to note is that there is a considerable correlation between the chemical featues we use 
for the drugs in our data set. High correlation between features is problematic for a number of 
reasons: first, they indicate redundant information. Second, it can lead to model instability (cf. 
pages 12, 46 in Kuhn & Johnson, 2013). 

To show how strongly the chemical features are correlated:

```{r}
tmp <- matrix_container[180, ]
stopifnot(nrow(tmp) == 1 & tmp$drug_dosages == "most_interactions" & tmp$chemical_feats)
tmp <- tmp$drug_feature_matrices[[1]]

# we have 13 chemical features
tmp_m <- select(tmp, (ncol(tmp)-12):ncol(tmp))
tmp_m <- as.matrix(tmp_m)
row.names(tmp_m) <- tmp$drugname_typaslab
tmp_m_cor <- cor(tmp_m)

corrplot::corrplot(tmp_m_cor, method = "color", order = "hclust", tl.cex = 0.7)
corrplot::corrplot(tmp_m_cor, type = "upper", method = "number", order = "hclust", tl.cex = 0.4)
```

As can also be seen in the plot "drug_drug_similarities_RDKit.pdf" (notebook Misc) some of 
our chemical features are biased towards distinguishing protein_synthesis from the other features. 
However, numbe of amide bonds favours cell wall, number of aromatic rings might favour DNA. 

```{r}
select(tmp, drugname_typaslab, process_broad, (ncol(tmp)-12):ncol(tmp)) %>%
   filter(process_broad %in% c("cell_wall", "dna", "protein_synthesis", "membrane_stress")) %>%
   gather(-drugname_typaslab, -process_broad, key = "chemical_feature", value = "value") %>%
   ggplot(aes(x = process_broad, y = value, colour = process_broad)) + 
      ggbeeswarm::geom_beeswarm(groupOnX = TRUE, cex = 2, shape = 1) + 
      facet_wrap( ~ chemical_feature, ncol = 5, scales = "free_y") + 
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
      labs(title = "Distribution of chemical features across MoAs")
```

Check which drugs have high number of aliphatic rings and of saturated rings. This seems to bias 
the models using chemical features towards protein synthesis. 

```{r}
tmp$drugname_typaslab[tmp$NumAliphaticRings > 2.5]
tmp$drugname_typaslab[tmp$NumSaturatedRings > 2.5]
```


# System and session info

```{r, session_info}
R.version
sessionInfo()
```
