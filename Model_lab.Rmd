---
title: "Model lab: current 'production' models and experiments to improve them"
author: "Florian Huber"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

# Setup, library loading

```{r setup}
rm(list = ls())
# function to check if a package is installed, if so, load it, else install and then load it
source("./R/ipak.R")
# set chunk options and load libraries
source("./setup.R")
knitr::opts_chunk$set(message = FALSE, cache = TRUE)

matrix_container_ext <- readRDS("./data/programmatic_output/matrix_container_ext.rds")
moa_cols <- c("#e66101", "#fdb863", "#b2abd2", "#5e3c99")
moas <- c("cell_wall", "dna", "membrane_stress", "protein_synthesis")
my_moa <- "cell_wall"
datadir <- "./run_results_from_server/matrix_container_result"
```

# Section 1: Current production models + data

## Current 1-vs-rest 'production' model

To recapitulate from "Comparing_performances.Rmd": random forests was better than lasso because of 
model stabilities. Top25pct of features seems to have worked best. Only for protein synthesis the 
lasso seems to have done fairly well.

```{r}
(the_line <- which(matrix_container_ext$fitted_model == "classif.randomForest" & 
    matrix_container_ext$feat_preselect == "top25pct" & 
    ! matrix_container_ext$chemical_feats &
    matrix_container_ext$drug_dosages == "all"))
stopifnot(length(the_line) == 1)

sub_matrix_container <- matrix_container_ext[the_line, ]
my_data <- matrix_container_ext$drug_feature_matrices[[the_line]]
(unique(sub_matrix_container$drug_feature_matrices[[1]]$process_broad))

plot_perf_from_container(sub_matrix_container)
plot_perf_from_container(sub_matrix_container, what = "prec-recall")

# AUCs:
sub_matrix_container %>%
  select(feat_preselect, fitted_model, perf_measures) %>%
  unnest() %>% 
  group_by(fitted_model, feat_preselect, moa_modelled) %>%
  summarise(mean_auc = mean(auc))

# retrieve our results object:
chosen_res <- readRDS(file.path(datadir, make_filename(matrix_container_ext[the_line, ])))
```


## Retrieving most commonly used hyperparameters

Will be used for future model runs so that we don't have to do hyperparameter tuning anymore. 

```{r}
# first try to figure out which values were used for mtry and ntree in our favourite result
# we can get the optimal hyperparameters using getLearnerModel():
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)

# to access directly:
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)$learner.model$ntree
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)$learner.model$mtry

# hyperparameters that were tested
matrix_container_ext$hyperparam_grid[[the_line]]

# dimensions of the drug_feature_matrix:
dim(matrix_container_ext$drug_feature_matrices[[the_line]])

# which parameters were used/'won'?
(ntrees <- map_dbl(chosen_res, ~ getLearnerModel(.x[["Outer fold 1"]][["model_dna"]])$learner.model$ntree))

# for ntree it seems like only 200 or 500 trees were used:
all_ntrees <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, param = "ntree", 
  simplify = FALSE, USE.NAMES = TRUE)
lapply(all_ntrees, table)

all_mtrys <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, param = "mtry", 
  simplify = FALSE, USE.NAMES = TRUE)

lapply(all_mtrys, table)
```

`dna` and `cell_wall` clearly favour mtry = 106, membrane_stress and protein_synthesis: less clear, 
but the tendency there is more towards 200. Let's settle for 150, which is around a third of the 
observations, like it is recommended for regression trees in Kuhn et al. Moreover, we will use 
ntree = 1000 and mtry = 150. 

```{r}
my_ntree <- 1000
my_mtry <- 150
```


## Currently used multi-class model

### Training/loading the model

Get resampling instance from Untracked.Rmd.

```{r}
rep_rin <- readRDS("./data/programmatic_output/rep_rin_empirical.rds")

# we need to remove all but the 4 main modes of action:
my_data_multiclass <- filter(my_data, process_broad %in% moas)

length(unique(my_data_multiclass$drugname_typaslab))
unique(my_data_multiclass$process_broad)

(my_data_multiclass_train <- select(my_data_multiclass, -one_of(c("conc", "drugname_typaslab"))))
(rf_multi.task <- makeClassifTask(data = my_data_multiclass_train, target = "process_broad"))
(rf_multi.learner <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = my_ntree, mtry = my_mtry)))

my_filename <- "./data/programmatic_output/resampled_multiclass.rds"
if (file.exists(my_filename)) {
  resampled_multiclass <- readRDS(my_filename)
} else {
  resampled_multiclass <- resample(learner = rf_multi.learner, task = rf_multi.task, 
    measures = mmce, resampling = rep_rin, models = TRUE, keep.pred = TRUE)
  # add drugname and concentration information to $pred$data slot
  resampled_multiclass$pred$data <- 
    bind_cols(my_data_multiclass[resampled_multiclass$pred$data$id, c(1, 2)], 
      resampled_multiclass$pred$data) %>%
    arrange(drugname_typaslab, conc, iter)
  saveRDS(resampled_multiclass, file = my_filename)
}
```


### Performance and predictions of the model

#### Some metrics

```{r}
resampled_multiclass$aggr
```

So the mmce is ~ 1/3. With cell-wall model above it was ~ 7.5%. But not clear what kind of 
misclassification error we would get with our five models. Would we get more errors from 
aggregation?

#### Confusion matrix

```{r}
(cm <- get_wide_confmat(resampled_multiclass))
plot_wide_confmat(cm, title = "'Naive' multiclass model")
```

#### Investigating predictions

For example, the predictions + IQRs for A22:

```{r}
multicl_pred <- resampled_multiclass$pred$data
(multicl_pred_melt <- melt_pred_data(resampled_multiclass, model_type = "multiclass"))

filter(multicl_pred_melt, drugname_typaslab == "A22") %>% 
  ggplot(aes(x = factor(conc), y = prob.med, fill = predicted_prob)) + 
  geom_pointrange(aes(ymin = prob.min, ymax = prob.max, colour = predicted_prob), alpha = 0.75) + 
  geom_line(aes(group = predicted_prob, colour = predicted_prob)) + 
  facet_wrap( ~ truth) + 
  labs(x = "Concentration", y = "Prediction probability", title = "A22, multi-class prediction")
```

And now for all drugs:

```{r}
# may also want to show this as a heatmap, compare fig. 11.4 in Kuhn book
p <- ggplot(multicl_pred_melt, aes(x = factor(conc), y = prob.med, colour = predicted_prob)) + 
  geom_pointrange(aes(ymin = prob.min, ymax = prob.max), alpha = 0.75, size = 2, fatten = 1) + 
  geom_line(aes(group = predicted_prob), size = 1) + 
  facet_wrap( ~ truth + drugname_typaslab, scales = "free_x") + 
  geom_hline(yintercept = c(0.5), linetype = c("solid")) + 
  scale_colour_manual("Predicted probability\nfor class", 
    labels = c("Cell wall", "DNA", "Membrane stress", "Protein synthesis"), 
    values = c("#1b9e77", "#d95f02", "#7570b3", "#e7298a")) + 
  coord_cartesian(ylim = c(0, 1))

suppressMessages(ggsave(filename = "./plots/Multiclass_probabilities.pdf", plot = p, width = 30, 
  height = 20))
```

<a href="./plots/Multiclass_probabilities.pdf">LINK FOR THE PLOT</a>

An attempt to characterise model quality of the multi-class model: 'winner' approach. One can see 
that this improves accuracy by about 10 percent - but probably should compare this to random 
guessing.

```{r}
# one simple approach: the highest median probability across all dosages wins
# then calculate the accuracy:
(winners <- 
    group_by(multicl_pred_melt, drugname_typaslab, truth) %>%
    arrange(desc(prob.med), .by_group = TRUE) %>%
    slice(1) %>%
    mutate(response = str_extract(predicted_prob, 
      pattern = "cell_wall|dna|membrane_stress|protein_synthesis")))

# we should compare this to random guessing
mean(winners$response == winners$truth)
mean(winners$response != winners$truth)
```

Notably, stuff that is predicted badly is the same as in the previous models. 

```{r}
winners[winners$response != winners$truth, ]
```

#### Probability calibration plot

From Kuhn, chapter 11, p294ff.: probability calibration plot. Should also compare with our original 
1-vs-others approach. 

```{r}
# 10 bins with probabilities from 0 - 100, compare with actually observed frequencies
# one plot per MoA (?) - in Kuhn book: only use this for binary classification
plot_prob_calib(resampled_multiclass, title = "'naive' multi-class model")
ggsave("./plots/probability_calibration.pdf")
```


## Multi-class model performance is similar to several 1-vs-all models

### Comparing multi-class probabilities with 1-vs-rest probabilities

For each mode of action: compare probabilities for each observation returned by either 
(i) multi-class model or (ii) the respective 1-vs-all model.

```{r}
(onevsall_pred_melt <- melt_pred_data(matrix_container_ext$pred_data[[the_line]], 
  model_type = "onevsrest"))

prob_comparison <- compare_probabilities(
  list("'naive' multiclass" = multicl_pred_melt, "'naive' 1-vs-rest" = onevsall_pred_melt))
prob_comparison$plot
prob_comparison$spearman_corrs
```

### Comparing multi-class feature importances with 1-vs-rest feature importances

Note that the importance is not too high: which is perhaps not too surprising because some features 
will be considered importance in the multi-class model but might be considered important in only 
*one* 1-vs-rest model.

```{r}
(feat_imps_multi <- summarise_feat_imps(get_feat_imps(resampled_multiclass)))
(feat_imps_1vsrest <- summarise_feat_imps(get_feat_imps(chosen_res)))

(tmp <- left_join(feat_imps_multi, feat_imps_1vsrest[, c("moa", "gene", "median_importance")], 
  by = "gene", suffix = c("_multi", "_1vsall")))

ggplot(tmp, aes(x = log2(median_importance_1vsall), y = log2(median_importance_multi))) + 
  geom_point() + 
  facet_wrap( ~ moa) + 
  labs(title = "Comparison of feature importances 1-vs-all and multi-class model")
```

<!--
## Comparing with baseline models

### KNN

Has certain advantages: less black and white; assumption matches intuition that similar MoA = close 
in chemical genomics space. Will fail however when other features are included. 

```{r, echo = FALSE, eval = FALSE}
getLearnerParamSet("classif.knn")

(knn_params <- makeParamSet(
   makeDiscreteParam("k", values = 1:25)
))
class(knn_params)

(ctrl <- makeTuneControlGrid())
class(ctrl)

knn.learner <- makeLearner("classif.knn")

# rf.task was with cell_wall only
res_knn <- tuneParams(learner = knn.learner, task = rf.task, resampling = rep_rin, 
  measures = list(mmce), par.set = knn_params, control = ctrl)

res_knn
res_knn$x # this is a bit higher than for the random forests model above:
r2_correct$aggr 


data <- generateHyperParsEffectData(res_knn)
plotHyperParsEffect(data, x = "k", y = "mmce.test.mean", plot.type = "line")
```
-->


# Section 2: Improving models

First we need a quick and informative way of comparing models with each other. In the sections 
above we have a set of 1-vs-rest and one multi-model 'production' model. Both are likely to change 
because of data processing steps and certain things we wanna try to improve. 

So first we want to have a good way of comparing models.


## Demonstration of how models can be compared

```{r}
# so first let's get on the same page with which models we are dealing with:
# View(resampled_multiclass)
# View(chosen_res)
pryr::object_size(resampled_multiclass)
pryr::object_size(chosen_res)
# - quite unwieldy, about 0.5 and 2.1 GB, respectively 

class(resampled_multiclass)
class(chosen_res)
```

For a pair of one-vs-rest models:

```{r}
model1 <- matrix_container_ext[9, ]
model2 <- matrix_container_ext[10, ]

compare_onevsrest_models(model1 = model1, model2 = model2, 
  model1_title = "RF without chem. feats", 
  model2_title = "RF with chem. feats", file = "./plots/compare_one-vs-rest.pdf")
```


For a pair of multi-class models:

```{r}
# we only have one multiclass model so far, let's use it twice:
model1_mc <- resampled_multiclass
model2_mc <- resampled_multiclass

compare_multiclass_models(model1_mc, model2_mc, 
  model1_mc_title = "a", model2_mc_title = "b", file = "./plots/compare_multiclass_test.pdf")
```

## Trying to improve classification

<!-- 
Structure not yet clear, but start with the following:

1) IMPROVING STUFF:
  - require a minimum number of interactions, exclude strange genes
  - correlate dosages/predictions with MIC data (special attention to protein synthesis)
  - exclude some drugs? 
  - weighting observations? 
  - remove/merge correlated features (could try PCA for predictor collinearity)
2) TRYING OUT STUFF:
  - permuted labels
  - switching to LOOCV
  - binarise predictors, take q-values into consideration somehow
  - feature selection in inner loop
3) CLASSIFICATION OF SUBGROUPS
4) UNSUPERVISED APPROACHES
5) BENCHMARK EXPERIMENT
-->

### Checking MICs

Looking at the MICs does support the idea that the closer a drug dosage is to the MIC the more 
likely it is to get a high probability for the correct class. It could be interesting to plot on 
top of that the number of significant interactions per drug-dosage combination. Could be useful 
to only keep the 2 dosages which are closest to the MIC. Weighting? 

```{r, check_mics}
# # already run:
# source("./parse_MIC_data.R")
head(multicl_pred_melt)
head(mics <- read_delim("./data/programmatic_output/MICs.csv", delim = ";"))

multicl_pred_melt_micinfo <- 
  left_join(multicl_pred_melt, mics)

# use slice() instead of top_n(), the latter implies some type of ordering!
(helper_frame <- multicl_pred_melt_micinfo %>%
    group_by(drugname_typaslab) %>%
    slice(1) %>%
    mutate(label = as.character(mic_curated)))

helper_frame$label[is.na(helper_frame$label)] <- "NA"
helper_frame$label <- paste0("MIC: ", helper_frame$label, "\n", "resistant: ", 
  ifelse(helper_frame$resistant, "yes", "no"))

# note that hjust = 0 is needed for geom_text() for left justification
# geom_text will respect
p <- 
  multicl_pred_melt_micinfo %>%
  #filter(drugname_typaslab %in% c("NOVOBIOCIN", "BACITRACIN", "AMIKACIN")) %>%
  ggplot(aes(x = factor(conc), y = prob.med, colour = predicted_prob)) + 
  geom_hline(yintercept = c(0.5), linetype = c("solid")) + 
  geom_pointrange(aes(ymin = prob.min, ymax = prob.max), alpha = 0.75, size = 2, fatten = 1) + 
  geom_line(aes(group = predicted_prob), size = 1) + 
  facet_wrap( ~ truth + drugname_typaslab, scales = "free_x") + 
    geom_text(data = helper_frame, #helper_frame[helper_frame$drugname_typaslab %in% c("AMIKACIN", "NOVOBIOCIN", "BACITRACIN"), ], 
      aes(x = 1, y = 0.9, label = label), hjust = 0, inherit.aes = FALSE) + 
  scale_colour_manual("Predicted probability\nfor class", 
                     labels = c("Cell wall", "DNA", "Membrane stress", "Protein synthesis"), 
                     values = c("#1b9e77", "#d95f02", "#7570b3", "#e7298a")) + 
  coord_cartesian(ylim = c(0, 1))

suppressMessages(ggsave(filename = "./plots/Multiclass_probabilities_micinfo.pdf", plot = p, 
  width = 30, height = 20))
```

<!--
The way I see this going forward: 
1) Check how to remove collinear features (perhaps merge them)
2) Do all this bad feature, bad drugs removal, feature merging, weighting (?), and, on *that* 
   data, do the topx% selection. 
3) Do one run with only (2), then one run with (2) and +- the 2 "best" dosages, then one run with a 
feature selection wrapper (again perhaps +-) - revival of matrix container? 
-->

### Removing/merging (?) collinear features

First we need to select a cutoff. How do pairs correlate: 

* Between different dosages of one drug (within drug correlation)
* Between drugs (between drug correlation)
* Between genes of the same protein complex


```{r, gene_cplx_stuff}
drug_feats <- filter_container(matrix_container_ext, dosgs = "all", feats = "keepall", 
  chemfeats = FALSE, models = "classif.randomForest")$drug_feature_matrices[[1]]

feats <- drug_feats[, -c(1:3)]

# ----------------------------------------------------------------------------------------
# NOTE: will remove here already features with many NA values - will be removed at an earlier 
# stage in later runs (see also notebook "Inspecting_the_matrix.Rmd"):
(NA_feats <- readRDS("./data/programmatic_output/feats_to_remove_many_NAs.rds"))
feats <- select(feats, -one_of(NA_feats))
# ----------------------------------------------------------------------------------------

feats_cor <- cor(feats, method = "spearman")

# quite a large and unwieldy plot - but still shows a few useful clusters:
if (!file.exists("./plots/Correlation_all_by_all.png")) {
  png("./plots/Correlation_all_by_all.png", width = 90, height = 90, units = "cm", res = 500)
  corrplot::corrplot(feats_cor, method = "color", order = "hclust", tl.cex = 0.1)
  dev.off()
}

# to get a distribution of the correlations:
# get all possible combinations:

corpairs <- melt_cormat_to_pairs(feats_cor)
arrange(corpairs, desc(cor))

# retrieving all protein complexes, code from notebook 1 (Leonard) -----------------------
cmplx = read_tsv(file = "data/All_instances_of_Protein-Complexes_in_Escherichia_coli_K-12_substr._MG1655.txt")
(load("../dbsetup/data/genesWithEG_ID.RData"))
gene_synonyms$synonym = toupper(gene_synonyms$synonym)

genes_cmplx = lapply(cmplx$`Genes of polypeptide, complex, or RNA`,
        FUN = function(x){
            genenames = str_match_all(string = x, pattern = "[a-zA-Z]{3,4}")
            genenames = toupper(unlist(genenames))
       })

names(genes_cmplx) = cmplx$`Protein-Complexes`
genes_cmplx = genes_cmplx[!is.na(genes_cmplx)]

lens = lapply(genes_cmplx, function(x){length(x)})
genes_cmplx = genes_cmplx[lens != 1]
saveRDS(genes_cmplx, file = "./data/programmatic_output/genes_cmplx.rds")

# add to each pair the information if the two genes are also part of the same protein complex
genes_cmplx_v <- unique(unname(unlist(genes_cmplx)))

corpairs$in_same_cmplx <- 
  map2_lgl(corpairs$featA, corpairs$featB, function(.featA, .featB) {
    drugpair <- c(.featA, .featB)
    if (!all(c(.featA, .featB) %in% genes_cmplx_v)) {
      FALSE
      } else {
        any(map_lgl(genes_cmplx, ~ (sum(.x %in% drugpair) == 2)))
      }
    })
# only 395 pairs
View(corpairs[corpairs$in_same_cmplx, ])

ggplot(corpairs, aes(x = cor)) + 
  geom_histogram(bins = 30) + 
  facet_wrap( ~ in_same_cmplx, scales = "free") + 
  labs(title = "Spearman correlations of all mutant pairs.\nLeft: not part of the same protein 
    complex\nRight: within the same protein complex")

# two selected examples (highest and lowest correlation above cutoff):
ggplot(feats, aes(x = CYOC, y = CYOD)) + 
  geom_point() + 
  labs(title = "S-scores across all conditions (Spearman = 0.9)")

ggplot(feats, aes(x = NUOE, y = NUOJ)) + 
  geom_point() + 
  labs(title = "S-scores across all conditions (Spearman = 0.75)")

ggplot(feats, aes(x = FLIF, y = FLIQ)) + 
  geom_point() + 
  labs(title = "S-scores across all conditions (Spearman = 0.6)")
```

When choosing a cutoff of 0.6 and requiring that genes are in the same complex we get 80 pairs. 
A lot of the genes are part of the NADH:quinone oxidoreductase complex (nuoX genes). Other prominent 
complexes are:

* the terminal oxidase complex (cyoX)
* sulfate adenylyltransferase (cysD, cysN)
* acrAB-tolC 
* ATP synthase (atpX)
* the Tol-Pal system (Pal, TolB, TolR, TolQ)
* TatB-TatC (part of the twin arginine translocation (Tat) complex for the export of folded proteins)
* some flagellum-related genes (FlhA, FLiF, FliQ)

```{r}
# so we'd probably choose a cutoff of 0.6 or so 
View(corpairs[corpairs$in_same_cmplx & corpairs$cor > 0.6, ])
sum(corpairs$in_same_cmplx & corpairs$cor > 0.6)
sum(corpairs$cor > 0.6)
```

If we don't require them to be in the same complex we get `r sum(corpairs$cor > 0.6)` pairs with 
lots of the following: 

* ribonucleotide biosynthesis (pyrX, purX)
* LPS biosynthesis: rfaX
* and many others, see here:

```{r}
View(corpairs[corpairs$cor > 0.6, ])
sum(corpairs$cor > 0.6)
```

Anticorrelation is very rare, only 16 pairs have a Spearman correlation of less than -0.6. 

Notably, Pur and Pyr genes are anticorrelated with RsxB (together with rseC turns off SoxR-mediated 
induction of SoxS), YrbA (= ibaG, defends against acid stress), and UbiE/F (ubiquinone biosynthesis). 
Moreover, GuaB (IMP dehydrogenase) is anticorrelated with LipA (lipoate biosynthesis) and Rph 
(an RNAse). 

```{r}
View(corpairs[corpairs$cor < -0.6, ])
sum(corpairs$cor < -0.6)
```


Now merge correlated predictors. Follow procedure from Kuhn et al, p47. 

```{r}
# removed_correlated_feats <- character()
cor_thresh <- 0.6
continue <- TRUE

# involved data sets:
# 1) feats = all features in a tibble
# 2) feats_cor = correlation matrix derived from 1)
# 3) corpairs = melted version of 2)

my_backup <- list(corpairs = corpairs, feats = feats, feats_cor = feats_cor)
corpairs <- my_backup$corpairs
feats <- my_backup$feats
feats_cor <- my_backup$feats_cor

while (continue) {
  list_of_pairs <- list()
  corpairs <- arrange(corpairs, desc(cor))
  above_thresh <- sum(corpairs$cor > cor_thresh)
  
  for (r in seq_len(above_thresh)) {
    pair <- c(corpairs$featA[r], corpairs$featB[r])
    if (any(pair %in% unlist(list_of_pairs))) {
      # we don't want to go through the whole procedure of merging features for every pair 
      # as long as pairs don't overlap we can merge several pairs at once
      break
    } else {
      list_of_pairs <- c(list_of_pairs, list(pair))
    }
  }
  
  # now merge the feats
  for (el in list_of_pairs) {
    # just to be safe ...
    stopifnot(length(el) == 2)
    newfeat <- paste(el[[1]], el[[2]], sep = "_")
    feats[[newfeat]] <- apply(feats[el], 1, mean)
  }
  
  # and calculate everything again
  feats <- select(feats, -one_of(unlist(list_of_pairs)))
  feats_cor <- cor(feats, method = "spearman")
  corpairs <- melt_cormat_to_pairs(feats_cor)
  
  # if not TRUE means we're done
  continue <- max(corpairs$cor > cor_thresh)
}

## code below was used for removing correlated features ----------

# while (!stop) {
#   maxcor <- which.max(corpairs$cor)
#   
#   if (corpairs$cor[maxcor] <= cor_thresh) {
#     stop <- TRUE
#   } else {
#     a <- corpairs$featA[maxcor]
#     b <- corpairs$featB[maxcor]
#     where_a <- corpairs$featA %in% a | corpairs$featB %in% a
#     where_b <- corpairs$featB %in% b | corpairs$featB %in% b
#     
#     a_cor <- mean(corpairs$cor[where_a])
#     b_cor <- mean(corpairs$cor[where_b])
#     
#     if (a_cor >= b_cor) {
#       corpairs <- corpairs[!where_a, ]
#       removed_correlated_feats <- c(removed_correlated_feats, a)
#     } else {
#       corpairs <- corpairs[!where_b, ]
#       removed_correlated_feats <- c(removed_correlated_feats, b)
#     }
#   }
# }
```

This procedure removed `r length(removed_correlated_feats)` features 
(out of `r length(unique(c(corpairs$featA, corpairs$featB)))`):

```{r}
removed_correlated_feats
saveRDS(removed_correlated_feats, file = "./data/programmatic_output/feats_to_remove_collinear_thresh06.rds")
```


## Multi-class model after improved data pre-processing

### No NA values, collinear features merged

Note that removing the collinear features had virtually no effect on model performance. 

Things changed:
1. Features with lots of NA values were removed
2. Collinear features were merged (averaged)

```{r}
# Taken from chunk further above: 
# rep_rin
# my_data_multiclass = data that was used in the "winning" 1-vs-all model

my_data_mcl_preproc <- my_data_multiclass
# 106 out of 174 features that were removed because of collinearity are still present in this set:
sum(removed_correlated_feats %in% names(my_data_mcl_preproc))
my_data_mcl_preproc <- select(my_data_mcl_preproc, -one_of(c(removed_correlated_feats, NA_feats)))

# still, more than 100 features were removed:
ncol(my_data_multiclass)
ncol(my_data_mcl_preproc)

# training the next model: 
my_data_mcl_preproc_train <- select(my_data_mcl_preproc, -one_of(c("conc", "drugname_typaslab")))
rf_multi.task <- makeClassifTask(data = my_data_mcl_preproc_train, target = "process_broad")
rf_multi.learner <- makeLearner("classif.randomForest", predict.type = "prob",
  par.vals = list(ntree = my_ntree, mtry = my_mtry))

my_filename <- "./data/programmatic_output/resampled_multiclass_noNAs_corrs_merged.rds"
if (file.exists(my_filename)) {
  resampled_multiclass_noNAs <- readRDS(my_filename)
} else {
  resampled_multiclass_noNAs <- resample(learner = rf_multi.learner, task = rf_multi.task,
    measures = mmce, resampling = rep_rin, models = TRUE, keep.pred = TRUE)
  # add drugname and concentration information to $pred$data slot
  resampled_multiclass_noNAs$pred$data <-
    bind_cols(my_data_mcl_preproc[resampled_multiclass_noNAs$pred$data$id, c(1, 2)],
      resampled_multiclass_noNAs$pred$data) %>%
    arrange(drugname_typaslab, conc, iter)
  saveRDS(resampled_multiclass_noNAs, file = my_filename)
}
```

Comparing this with the previous multi-class model:

```{r}
compare_multiclass_models(model1_mc = resampled_multiclass, model2_mc = resampled_multiclass_noNAs, 
  model1_mc_title = "'naive' multiclass model", model2_mc_title = "multiclass model w/o NAs, corr 
  feats merged", file = "./plots/compare_multiclass_noNAs_corrs_merged.pdf")
```


### "Try 2"

In addition to "Try 1": 
1. Remove drug-dosage combinations with less than a certain number of significant interactions?
2. Keep just the two highest dosages (?)
3. Remove some selected drugs (?)

*Concerning (1)*: probably not such a good idea because we would lose a number of drugs: doxorubicin, 
gentamicin, levofloxacin, methotrexate. Only methotrexate at 25 should be removed because it has 0 
interactions. A lot of the other stuff is taken care of by keeping only the two highest dosages. 

```{r}
# remove drug-dosage combinations with less than 11 significant interactions 
(n_signif <- matrix_container_ext$pred_data_with_n_signif[[the_line]])

print(filter(n_signif, moa_modelled_is_truth, n_signif < 10) %>%
  arrange(drugname_typaslab), n = 100)

# remove methotrexate, 25, it has no interactions:
nrow(my_data_mcl_preproc)
my_data_mcl_try2 <- my_data_mcl_preproc
my_data_mcl_try2 <- my_data_mcl_try2[!(my_data_mcl_try2$drugname_typaslab == "METHOTREXATE" & 
    my_data_mcl_try2$conc == 25), ]
```


```{r}
# Keeping just the two highest dosages, leaves us with only 126 rows:
(my_data_mcl_try2 <- 
  group_by(my_data_mcl_preproc, drugname_typaslab) %>%
  top_n(2, conc) %>%
  ungroup())

my_data_mcl_try2_train <- select(my_data_mcl_try2, -one_of(c("conc", "drugname_typaslab")))
rf_multi.task <- makeClassifTask(data = my_data_mcl_try2_train, target = "process_broad")
rf_multi.learner <- makeLearner("classif.randomForest", predict.type = "prob",
  par.vals = list(ntree = my_ntree, mtry = my_mtry))
```

<!-- TO BE REMOVED ----------------->

(need a new resampling instance)

```{r, eval = FALSE, echo = FALSE}
rf.rdesc <- makeResampleDesc("CV", iters = 8) 

rep_rin_try2 <- makeResampleInstance(makeResampleDesc(method = "RepCV", reps = 10, folds = 8), 
  rf_multi.task)

(ref_fracs <- table(my_data_mcl_try2$process_broad) / length(my_data_mcl_try2$process_broad))
cv_instances <- list()
counter <- 0
max_deviance <- 0.05

while (length(cv_instances) < 10) {
   rin <- makeResampleInstance(rf.rdesc, rf_multi.task)
   counter <- counter + 1 # to keep track of how often we had to try :)
   for (split in c(1:8)) {
      train_fracs <- table(my_data_mcl_try2$process_broad[rin$train.inds[[split]]]) / length(rin$train.inds[[split]])
      # we don't want any "empty" classes in the test set:
      if (length(table(my_data_mcl_try2$process_broad[rin$test.inds[[split]]])) != length(moas)) {
         break
      }
      # and relative fractions of classes shouldn't differ too much in the training set
      if (any(abs(ref_fracs - train_fracs) > max_deviance)) {
         break
      }
      # only happens upon success
      if (split == 8) {
         cat("Success at attempt ", counter, "! - ", date(), " (maximum deviance was ", 
           round(max(abs(ref_fracs - train_fracs)), digits = 3), ")\n")
         cv_instances <- c(cv_instances, list(rin))
      }
   }
}

rep_rin_try2$train.inds <- flatten(map(cv_instances, "train.inds"))
rep_rin_try2$test.inds <- flatten(map(cv_instances, "test.inds"))

```

<!-- -------------------->


```{r}
my_filename <- "./data/programmatic_output/resampled_multiclass_try2.rds"

if (file.exists(my_filename)) {
  resampled_multiclass_try2 <- readRDS(my_filename)
} else {
  resampled_multiclass_try2 <- resample(learner = rf_multi.learner, task = rf_multi.task,
    measures = mmce, resampling = rep_rin_try2, models = TRUE, keep.pred = TRUE)
  # add drugname and concentration information to $pred$data slot
  resampled_multiclass_try2$pred$data <-
    bind_cols(my_data_mcl_try2[resampled_multiclass_try2$pred$data$id, c(1, 2)],
      resampled_multiclass_try2$pred$data) %>%
    arrange(drugname_typaslab, conc, iter)
  saveRDS(resampled_multiclass_try2, file = my_filename)
}
```

Compare!

```{r}
compare_multiclass_models(model1_mc = resampled_multiclass_noNAs, 
  model1_mc_title = "Multiclass, NA values and coll. feats removed", 
  model2_mc = resampled_multiclass_try2, 
  model2_mc_title = "Multiclass, only 2 highest dosages", 
  "./plots/compare_multiclass_try1-vs-try2.pdf")

multicl_pred_melt <- melt_pred_data(resampled_multiclass_try2, model_type = "multiclass")

p <- ggplot(multicl_pred_melt, aes(x = factor(conc), y = prob.med, colour = predicted_prob)) + 
  geom_pointrange(aes(ymin = prob.min, ymax = prob.max), alpha = 0.75, size = 2, fatten = 1) + 
  geom_line(aes(group = predicted_prob), size = 1) + 
  facet_wrap( ~ truth + drugname_typaslab, scales = "free_x") + 
  geom_hline(yintercept = c(0.5), linetype = c("solid")) + 
  scale_colour_manual("Predicted probability\nfor class", 
    labels = c("Cell wall", "DNA", "Membrane stress", "Protein synthesis"), 
    values = c("#1b9e77", "#d95f02", "#7570b3", "#e7298a")) + 
  coord_cartesian(ylim = c(0, 1))

suppressMessages(ggsave(filename = "./plots/Multiclass_probabilities_try2.pdf", plot = p, 
  width = 30, height = 20))
```



# Session info 

```{r session_info}
R.version
sessionInfo()
```

