---
title: "Model lab: current 'production' models and experiments to improve them"
author: "Florian Huber"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

# Setup, library loading

```{r setup, cache = FALSE}
rm(list = ls())
# function to check if a package is installed, if so, load it, else install and then load it
source("./R/ipak.R")
# set chunk options and load libraries
source("./setup.R")
knitr::opts_chunk$set(message = FALSE, cache = TRUE)

datadir <- "./data/programmatic_output"
resultsdir <- "./run_results_from_server/matrix_container_result_2019"

mc_ext <- readRDS(file.path(datadir, "matrix_container_ext_2019.rds"))
mc_ext$feat_preselect %<>% as.character()
moas <- c("cell_wall", "dna", "membrane_stress", "protein_synthesis")
moa_cols <- c("#1b9e77", "#d95f02", "#7570b3", "#e7298a")
my_moa <- "cell_wall"
```

# Section 1: Current production models + data

## Current 1-vs-rest 'production' model

To recapitulate from "Comparing_performances.Rmd": random forests was better 
than lasso because of model stabilities. Top25pct of features seems to have 
worked best. Only for protein synthesis the lasso seems to have done fairly 
well.

```{r}
(the_line <- which(mc_ext$fitted_model == "classif.randomForest" & 
    mc_ext$feat_preselect == "top20pct" & 
    !mc_ext$chemical_feats &
    mc_ext$drug_dosages == "all"))
stopifnot(length(the_line) == 1)

mc_ext_sub <- mc_ext[the_line, ]
mc_ext_dfm <- mc_ext$drug_feature_matrices[[the_line]]
unique(mc_ext_dfm$process_broad)

plot_perf_from_container(mc_ext_sub)
plot_perf_from_container(mc_ext_sub, what = "prec-recall")

# AUCs:
mc_ext_sub %>%
  select(feat_preselect, fitted_model, perf_measures) %>%
  unnest() %>% 
  group_by(fitted_model, feat_preselect, moa_modelled) %>%
  summarise(mean_auc = mean(auc))

# retrieve our results object:
chosen_res <- readRDS(file.path(resultsdir, make_filename(mc_ext[the_line, ])))

# mmces:
main_mmces <- map(moas, function(moa) {
  unlist(map(chosen_res, function(repetition) {
    flatten_dbl(map(map(repetition, paste0("prediction_", moa)), performance))
  }))
})
names(main_mmces) <- moas
sapply(main_mmces, mean)
```


## Retrieving most commonly used hyperparameters

Will be used for future model runs so that we don't have to do hyperparameter 
tuning anymore. 

```{r}
# first try to figure out which values were used for mtry and ntree in our 
# favourite result
# we can get the optimal hyperparameters using getLearnerModel():
chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna

# to access directly:
chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna$learner.model$ntree
chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna$learner.model$mtry

# hyperparameters that were tested
mc_ext$hyperparam_grid[[the_line]]

# dimensions of the drug_feature_matrix:
dim(mc_ext$drug_feature_matrices[[the_line]])

# which parameters were used/'won'?
(ntrees <- map_dbl(chosen_res, ~ .x[["Outer fold 1"]][["model_dna"]]$learner.model$ntree))

# for ntree it seems like only 200 or 500 trees were used:
all_ntrees <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, 
  param = "ntree", simplify = FALSE, USE.NAMES = TRUE)
lapply(all_ntrees, table)

all_mtrys <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, 
  param = "mtry", simplify = FALSE, USE.NAMES = TRUE)

lapply(all_mtrys, table)
```


```{r}
my_ntree <- 1000
my_mtry <- 111
```


## Currently used multi-class model

### Training/loading the model

Make default version of fitting functions.  

```{r}
# make default version of the functions with common default values
fit_multiclass_rf_def <- partial(fit_multiclass_rf, targetvar = "process_broad", 
  blockingvar = "drugname_typaslab", paramlist = list(ntree = my_ntree, 
    mtry = my_mtry), cols_to_exclude = c("drugname_typaslab", "conc"))

fit_multiclass_rf_loo_def <- partial(fit_multiclass_rf_loo, 
  targetvar = "process_broad", blockingvar = "drugname_typaslab", 
  paramlist = list(ntree = my_ntree, mtry = my_mtry), 
  cols_to_exclude = c("drugname_typaslab", "conc"))
```

Fit the model, once with normal resampling, once with LOOCV. 

```{r}
resampled_mcl_2019 <- fit_or_load(varname = "resampled_mcl_2019", 
  directory = datadir, dfm = mc_ext_dfm, fitting_fun = fit_multiclass_rf_def)

resampled_mcl_loo <- fit_or_load(varname = "resampled_mcl_loo", 
  directory = datadir, dfm = mc_ext_dfm, fitting_fun = fit_multiclass_rf_loo_def)
```


```{r}
resampled_mcl_2019$aggr
resampled_mcl_loo$aggr
```

#### Comparison nested resampling with LOOCV

```{r, loocv}
compare_multiclass_models(model1_mc = resampled_mcl_2019, 
  model1_mc_title = "Multiclass, 4 main modes, rep nest CV", 
  model2_mc = resampled_mcl_loo, 
  model2_mc_title = "Multiclass, 4 main modes, LOOCV", 
  "./plots/Compare_multiclass_loocv.pdf")

# therefore ...
(multicl_pred_melt <- melt_pred_data(resampled_mcl_loo, 
  model_type = "multiclass"))
```


## Multi-class model performance is similar to several 1-vs-all models

### Comparing multi-class probabilities with 1-vs-rest probabilities

For each mode of action: compare probabilities for each observation returned by 
either (i) multi-class model or (ii) the respective 1-vs-all model.

```{r}
(onevsall_pred_melt <- melt_pred_data(mc_ext$pred_data[[the_line]], 
  model_type = "onevsrest"))

prob_comparison <- compare_probabilities(list("Multiclass" = multicl_pred_melt, 
  "1-vs-rest" = onevsall_pred_melt))
prob_comparison$plot
prob_comparison$spearman_corrs
```


### Comparing multi-class feature importances with 1-vs-rest feature importances

Note that the importance is not too high: which is perhaps not too surprising 
because some features will be considered importance in the multi-class model 
but might be considered important in only *one* 1-vs-rest model.

```{r}
(feat_imps_multi <- summarise_feat_imps(get_feat_imps(resampled_mcl_loo)))
(feat_imps_1vsrest <- summarise_feat_imps(get_feat_imps(chosen_res)))

(tmp <- left_join(feat_imps_multi, feat_imps_1vsrest[, c("moa", "gene", "median_importance")], 
  by = "gene", suffix = c("_multi", "_1vsall")))

ggplot(tmp, aes(x = log2(median_importance_1vsall), y = log2(median_importance_multi))) + 
  geom_point() + 
  facet_wrap( ~ moa) + 
  labs(title = "Comparison of feature importances 1-vs-all and multi-class model")
```


## Closer inspection of multiclass model

Only showing for LOOCV.

### Metrics, ROC curves

```{r}
resampled_mcl_loo$aggr
```

So the mmce is ~ 1/3. With cell-wall model above it was ~ 7.5%. But not clear 
what kind of misclassification error we would get with our five models. Would 
we get more errors from aggregation? 

However, ROC curves look similar to old models, or even better. 

```{r}
map(moas, plot_roc_mcl, resampled_multiclass = resampled_mcl_loo)
```


### Confusion matrix

```{r}
(cm <- get_wide_confmat(resampled_mcl_loo))
plot_wide_confmat(cm, title = "Multiclass, 4 main modes, LOO")
```


### Probability calibration plot

From Kuhn, chapter 11, p294ff.: probability calibration plot.

```{r}
# 10 bins with probabilities from 0 - 100, compare with actually observed frequencies
# one plot per MoA (?) - in Kuhn book: only use this for binary classification
plot_prob_calib(resampled_mcl_loo, title = "'naive' multi-class model")
ggsave("./plots/Probability_calibration_LOO.pdf")
```


<!--
#### ROC-curve for multi-class models

##### A few reflections

This can only be defined for a mode-of-action - not-mode-of-action pair. 
Otherwise, how should "true negative" be defined - this only makes sense if one 
class is defined as positive. The following chunks demonstrates the problem:

```{r, eval = FALSE, echo = FALSE}
# see also below
head(resampled_multiclass$pred$data)
(mcl_melt <- melt_pred_data(resampled_multiclass, model_type = "multiclass"))
mcl_melt$predicted_prob <- str_replace(string = mcl_melt$predicted_prob, 
  pattern = "prob\\.", replacement = "")
mcl_melt$truth <- as.character(mcl_melt$truth)

# average concentrations
(mcl_melt_concavg <- 
  mcl_melt %>%
  select(drugname_typaslab, conc, predicted_prob, prob.med, truth) %>%
  group_by(drugname_typaslab, truth, predicted_prob) %>%
  summarise(prob.med_avg = mean(prob.med)))

# note: truth = actual class label
(mcl_melt_roc_input <- 
  mcl_melt_concavg %>%
  group_by(drugname_typaslab) %>%
  mutate(highest_vs_2nd = prob.med_avg - max(prob.med_avg[-which.max(prob.med_avg)])) %>%
  top_n(1, prob.med_avg) %>%
  rename(class_highest_p = predicted_prob, highest_p = prob.med_avg))
```

Now consider what happens when we decide on a threshold at which we call an 
observation to belong to the class that has the highest probability, e.g. 0.2:

```{r, eval = FALSE, echo = FALSE}
mcl_melt_roc_input$response <- 
  ifelse(mcl_melt_roc_input$highest_vs_2nd >= 0.2, 
    mcl_melt_roc_input$class_highest_p, 
    paste0("not_", mcl_melt_roc_input$class_highest_p))
head(mcl_melt_roc_input)
```

Obviously, such an output is useless for TP, FN etc. rate calculations. 

So instead we write a function that defines one class as positive and the 
others as negative.

```{r, eval = FALSE, echo = FALSE}
get_responses_mcl <- function(dfr, positive, thresh) {
  # positive = MoA that is considered positive
  dfr$positive <- positive
  dfr$thresh <- thresh
  negative <- paste0("not_", positive)
  dfr$truth <- ifelse(dfr$truth == positive, positive, negative)
  # give answer according to class_highest_p
  dfr$response <- ifelse(dfr$highest_vs_2nd >= thresh, dfr$class_highest_p, 
    paste0("not_", dfr$class_highest_p))
  # now it gets philosophical: based on this routine an observation might be 
  # "not_membrane_stress" while the positive class is "cell_wall". Strictly 
  # speaking it's not clear if this is true negative or even true positive. 
  # But we might argue that "not_membrane_stress" would also mean "not_" for 
  # any other class - so we can still classify it as true negative. 
  #### TO DO: discuss this or read up on this
  dfr$response <- ifelse(dfr$response == positive, positive, negative)
  dfr$tp <- (dfr$response == dfr$truth) & (dfr$truth == positive)
  dfr$fn <- (dfr$response != dfr$truth) & (dfr$truth == positive)
  dfr$fp <- (dfr$response != dfr$truth) & (dfr$truth != positive)
  dfr$tn <- (dfr$response == dfr$truth) & (dfr$truth != positive)
  stopifnot(sum(unlist(dfr[, c("tp", "fn", "fp", "tn")])) == nrow(dfr))
  return(dfr)
}

(tmp <- get_responses_mcl(mcl_melt_roc_input, positive = "cell_wall", thresh = 0.2))

get_metrics_mcl <- function(dfr) {
  stopifnot(length(unique(dfr$thresh)) == 1)
  dfr <- group_by(dfr, thresh, positive) %>%
    summarise(tp = sum(tp), fn = sum(fn), tn = sum(tn), fp = sum(fp), 
      tpr = tp / (tp + fn), fpr = fp / (fp + tn), ppv = tp / (tp + fp))
  return(dfr)
}

get_metrics_mcl(tmp)

get_thresh_vs_perf_mcl <- function(dfr, positive, thresholds) {
  thresh_vs_perf <- map_dfr(thresholds, function(.thresh) {
    get_metrics_mcl(get_responses_mcl(dfr, positive, .thresh))
  })
  thresh_vs_perf <- arrange(thresh_vs_perf, desc(thresh))
  return(thresh_vs_perf)
}

(from <- range(mcl_melt_roc_input$highest_vs_2nd)[1])
(to <- range(mcl_melt_roc_input$highest_vs_2nd)[2])
tmp <- get_thresh_vs_perf_mcl(mcl_melt_roc_input, positive = "cell_wall", 
  thresholds = seq(from = from, to = to, length.out = 100))

print(tmp, n = 100)

ggplot(tmp, aes(x = fpr, y = tpr)) + 
  geom_path(aes(group = 1)) + 
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1))
```

What happened? The issue is that we use `highest_vs_2nd` as the metric to put a 
threshold on. This works fine at the beginning: we have a high confidence 
(i.e. high `highest_vs_2nd` levels) so the tpr goes up without compromising the 
fpr. However, when the threshold approaches 0 there are quite a few 
observations that never "get the chance" to be put into the positive class. 
Why? Because their probability for the positive class (`cell_wall` in this 
case) is lower than the 2nd highest probability so they are just all classified 
as negative cases. Therefore, neither the TPR nor the FPR can ever go up again. 
Conclusion: `highest_vs_2nd` is not a sensible metric. 


##### Better version

Go back to simply putting a threshold on the probabilities themselves.

```{r, eval = FALSE, echo = FALSE}
# just to demonstrate the different steps contained in plot_roc_mcl()
source("./R/ROC_mcl_models.R") # don't remove this line!
(tmp <- prep_roc(resampled_multiclass, positive = "cell_wall"))
(tmp <- get_responses_mcl(tmp, positive = "cell_wall", thresh = 0.5))
(tmp <- get_metrics_mcl(tmp))

(tmp <- get_thresh_vs_perf_mcl(resampled_multiclass, positive = "cell_wall"))
print(tmp, n = 20)

plots <- map(moas, plot_roc_mcl, resampled_multiclass = resampled_multiclass)
names(plots) <- moas

plots$cell_wall
plots$dna
plots$membrane_stress
plots$protein_synthesis
```

-->

### Investigating predictions

For example, the predictions + IQRs for A22:

```{r}
filter(multicl_pred_melt, drugname_typaslab == "A22") %>% 
  ggplot(aes(x = factor(conc), y = prob.med, fill = predicted_prob)) + 
  geom_pointrange(aes(ymin = prob.min, ymax = prob.max, colour = predicted_prob), 
    alpha = 0.75) + 
  geom_line(aes(group = predicted_prob, colour = predicted_prob)) + 
  facet_wrap( ~ truth) + 
  labs(x = "Concentration", y = "Prediction probability", 
    title = "A22, multi-class prediction")
```

Add MIC information

```{r}
# # already run:
# source("./parse_MIC_data.R")
head(mics <- read_delim("./data/programmatic_output/MICs.csv", delim = ";"))
multicl_pred_melt_micinfo <- left_join(multicl_pred_melt, mics)

# use slice() instead of top_n(), the latter implies some type of ordering!
(helper_frame <- multicl_pred_melt_micinfo %>%
    group_by(drugname_typaslab) %>%
    slice(1) %>%
    mutate(label = as.character(mic_curated)))

helper_frame$label[is.na(helper_frame$label)] <- "NA"
helper_frame$label <- paste0("MIC: ", helper_frame$label, "\n", "resistant: ", 
  ifelse(helper_frame$resistant, "yes", "no"))
```

Line display:

```{r}
# note that hjust = 0 is needed for geom_text() for left justification
# geom_text will respect
p <- plot_mcl_probs_lines(multicl_pred_melt_micinfo, labels = moas, 
  colours = moa_cols, printplot = FALSE)

p <- p + geom_text(data = helper_frame, aes(x = 1, y = 0.9, label = label), 
      hjust = 0, inherit.aes = FALSE)

ggsave(filename = "./plots/Multiclass_probabilities_LOO.pdf", 
  plot = p, width = 30, height = 20)
```

<a href="./plots/Multiclass_probabilities_LOO.pdf">LINK</a>

Heatmap display:

```{r}
plot_mcl_probs_heatmap(melted_pred_data = multicl_pred_melt, mics = mics) %>% 
  ggsave(filename = "./plots/Multiclass_probabilities_LOO_heatmap.pdf", 
  width = 20, height = 20)
```

<a href="./plots/Multiclass_probabilities_LOO_heatmap.pdf">LINK</a>


Playing around with "winner approach"

An attempt to characterise model quality of the multi-class model: 'winner' 
approach. One can see that this improves accuracy by > 10 percent - but 
probably should compare this to random guessing.

```{r}
# one simple approach: the highest median probability across all dosages wins
# then calculate the accuracy:
(winners <- multicl_pred_melt %>%
    group_by(drugname_typaslab, truth) %>%
    arrange(desc(prob.med), .by_group = TRUE) %>%
    slice(1) %>%
    mutate(response = str_extract(predicted_prob, 
      pattern = "cell_wall|dna|membrane_stress|protein_synthesis")))

# we should compare this to random guessing
mean(winners$response == winners$truth)
mean(winners$response != winners$truth)
```

Notably, stuff that is predicted badly is the same as in the previous models. 

```{r}
winners[winners$response != winners$truth, ]
```


# Section 2: Improving models

First we need a quick and informative way of comparing models with each other. 
In the sections above we have a set of 1-vs-rest and one multi-model 
'production' model. Both are likely to change because of data processing steps 
and certain things we wanna try to improve. 

So first we want to have a good way of comparing models.

## Demonstration of how models can be compared

```{r}
# so first let's get on the same page with which models we are dealing with:
# View(resampled_multiclass)
# View(chosen_res)
pryr::object_size(resampled_mcl_loo)
pryr::object_size(chosen_res)
# - quite unwieldy, about 0.5 and 2.1 GB, respectively 

class(resampled_mcl_loo)
class(chosen_res)
```

### For a pair of one-vs-rest models

For example, random forests without and with chemical features.

```{r}
model1 <- mc_ext[7, ]
model2 <- mc_ext[8, ]

compare_onevsrest_models(model1 = model1, model2 = model2, 
  model1_title = "RF without chem. feats", 
  model2_title = "RF with chem. feats", 
  file = "./plots/Compare_one-vs-rest_chemfeats.pdf")
```


### For a pair of multi-class models

See comparison of CV instance based and resampling-based multi-class model 
above. 


## Trying to improve classification

### Multi-class model after improved data pre-processing

#### No NA values, collinear features merged

_Deprecated since commit bd6a8c4_

<!--
Note that removing the collinear features had virtually no effect on model 
performance. 

Things changed:
1. Features with lots of NA values were removed
2. Collinear features were merged (averaged)
-->

```{r, eval = FALSE, echo = FALSE}
# # Taken from chunk further above: 
# # rep_rin
# # feats: containing the merged features
# stopifnot(nrow(my_data_mcl) == nrow(feats))
# ncol(my_backup$feats) - ncol(feats)
# 
# my_data_mcl_preproc <- my_data_mcl
# (drugdata <- my_data_mcl_preproc[, c(1:3)])
# # columns that are new due to merging:
# (to_add <- feats[, nchar(colnames(feats)) > 4])
# # exclude columns that are not in feats: these are the ones that were merged
# my_data_mcl_preproc <- my_data_mcl_preproc[, colnames(my_data_mcl_preproc) %in% colnames(feats)]
# # add merged features back together
# my_data_mcl_preproc <- cbind(drugdata, my_data_mcl_preproc, to_add)
# 
# ncol(my_data_mcl)
# ncol(my_data_mcl_preproc)
# 
# # training the next model: 
# my_data_mcl_preproc_task <- select(my_data_mcl_preproc, -one_of(c("conc", "drugname_typaslab")))
# rf_multi.task <- makeClassifTask(data = my_data_mcl_preproc_task, target = "process_broad")
# rf_multi.learner <- makeLearner("classif.randomForest", predict.type = "prob",
#   par.vals = list(ntree = my_ntree, mtry = my_mtry))
```

_Note deprecation note above._

```{r, eval = FALSE, echo = FALSE}
# my_filename <- "./data/programmatic_output/resampled_multiclass_noNAs_corrs_merged.rds"
# 
# if (file.exists(my_filename)) {
#   resampled_multiclass_preproc <- readRDS(my_filename)
# } else {
#   # resampled_multiclass_preproc <- fit_multiclass_rf(learner = rf_multi.learner, 
#   #   task = rf_multi.task, measures = mmce, rin = rep_rin_mcl, 
#   #   task_data_all_cols = my_data_mcl_preproc)
#   # saveRDS(resampled_multiclass_preproc, file = my_filename)
# }
```

<!-- Comparing this with the previous multi-class model: -->

```{r}
# compare_multiclass_models(model1_mc = resampled_multiclass, model2_mc = resampled_multiclass_preproc, 
#   model1_mc_title = "'naive' multiclass model", model2_mc_title = "multiclass model w/o NAs, corr 
#   feats merged", file = "./plots/compare_multiclass_noNAs_corrs_merged.pdf")
```


### "Two highest"

_Note: also to be deprecated soon since most of the issues probably resolved after commit bd6a8c4._ 

Keep just two highest dosages and see what happens. Removing drugs with low 
number of significant interactions: not necessary anymore, is now taken care of 
in data preprocessing steps. 

```{r}
# remove drug-dosage combinations with less than 11 significant interactions 
(n_signif <- mc_ext$pred_data_with_n_signif[[the_line]])

print(filter(n_signif, moa_modelled_is_truth, n_signif < 10) %>%
  arrange(drugname_typaslab), n = 100)
```


```{r}
mc_ext_dfm_2highest <- 
  group_by(mc_ext_dfm, drugname_typaslab) %>%
  top_n(2, conc) %>%
  ungroup()

resampled_mcl_loo_2highest <- fit_or_load("resampled_mcl_loo_2highest", 
  directory = datadir, dfm = mc_ext_dfm_2highest, fitting_fun = 
    fit_multiclass_rf_loo_def)
```

Comparison with multi-class model using all dosages. 

```{r}
compare_multiclass_models(model1_mc = resampled_mcl_loo, 
  model1_mc_title = "Multiclass, all dosgs, LOO", 
  model2_mc = resampled_mcl_loo_2highest, 
  model2_mc_title = "Multiclass, 2 highest dosg, LOO", 
  "./plots/Compare_multiclass_alldosg_vs_2highest.pdf")
```

Probability details.

```{r}
multicl_2highest_melt <- melt_pred_data(resampled_mcl_loo_2highest, 
  model_type = "multiclass")

plot_mcl_probs_lines(multicl_2highest_melt, labels = moas, colours = moa_cols, 
  printplot = FALSE) %>%
  ggsave(filename = "./plots/Multiclass_probabilities_LOO_2highest.pdf", 
    width = 30, height = 20)

plot_mcl_probs_heatmap(melted_pred_data = multicl_2highest_melt, mics = mics) %>% 
  ggsave(filename = "./plots/Multiclass_probabilities_LOO_2highest_heatmap.pdf", 
  width = 20, height = 20)
```



# Predicting MoA subgroups

```{r, subgroups}
(mode_of_action <- read_delim("./data/programmatic_output/drug_moa_gdrive.csv", 
  delim = ",", na = c("-")))
mode_of_action <- mode_of_action[, c("drug", "moa_broad", "moa_subgroup", 
  "dataset")]
colnames(mode_of_action) <- c("drugname_typaslab", "process_broad", 
  "process_subgroup", "dataset")
mode_of_action <- mode_of_action[1:153, ]
table(mode_of_action$process_subgroup)
```


```{r}
# use same top20pct data frame, but could also try with keepall just to try 
(mc_ext_dfm_subgr <- 
  select(mc_ext_dfm, -process_broad) %>%
  left_join(mode_of_action[, c("drugname_typaslab", "process_subgroup")]) %>%
  select(drugname_typaslab, conc, process_subgroup, everything()))

table(mc_ext_dfm_subgr$process_subgroup)

resampled_mcl_loo_subg <- 
  fit_multiclass_rf_loo(data = mc_ext_dfm_subgr, targetvar = "process_subgroup", 
  blockingvar = "drugname_typaslab", paramlist = list(ntree = my_ntree, 
    mtry = my_mtry), cols_to_exclude = c("drugname_typaslab", "conc"))

```


```{r}
resampled_mcl_loo_subg$aggr

plot_wide_confmat(get_wide_confmat(resampled_mcl_loo_subg), 
  title = "Multiclass RF, LOO, subgroups")

ggsave("./plots/Subgroup_prediction.pdf")
```



# Apply model to other data sets

## Lucia's data (newsize)

Prepare the data.

```{r}
# get already preprocessed data: 
newsize <- readRDS("./data/programmatic_output/kept_datasets_wide_featsndrugs_removed_noNAs_featsmerged.rds")
newsize <- newsize$newsize.sscores

# keep only features that are in the_matrix
newsize <- newsize[, colnames(newsize) %in% colnames(mc_ext_dfm)]
newsize <- 
  left_join(newsize, mode_of_action[, c("drugname_typaslab", "process_broad")]) %>%
  select(drugname_typaslab, conc, process_broad, everything())

# newsize keeping only overlapping conditions (dosage also needs to be the 
# same)
newsize_overlap <- semi_join(newsize, mc_ext_dfm, 
  by = c("drugname_typaslab", "conc"))
(shared_drugs <- unique(newsize_overlap$drugname_typaslab))
```

```{r}
# Retrain 'old' model but only with non-overlapping drugs
# training in mlr: call train() on a learner + suitable task
retrain_nichols <- mc_ext_dfm
retrain_nichols$process_broad %<>% factor()
my_rle <- rle(retrain_nichols$drugname_typaslab)
my_blocks <- factor(rep(seq_along(my_rle$lengths), my_rle$lengths))

retrain_task <- makeClassifTask(data = retrain_nichols[, -c(1, 2)], 
  target = "process_broad", blocking = my_blocks)

retrain_learner <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = my_ntree, mtry = my_mtry))

retrained_model <- train(learner = retrain_learner, task = retrain_task)

getLearnerModel(retrained_model)

sort(unlist(getFeatureImportance(retrained_model)$res), decreasing = TRUE) %>% 
  head(n = 20)
```

Predict with retrained model.

```{r}
(newsize_preds <- predict(retrained_model, newdata = newsize_overlap))
performance(newsize_preds, measures = mmce)

newsize_preds$data <- as_tibble(cbind(newsize_overlap[, 1:2], 
  as.data.frame(newsize_preds)))
View(newsize_preds$data)
```

Comparing old with new predictions.

```{r}
newsize_preds_melt <- melt_pred_data(newsize_preds, "multiclass")
p <- plot_mcl_probs_heatmap(newsize_preds_melt, mics = mics)

ggsave(filename = "./plots/Prediction_heatmap_newsize.pdf", plot = p, 
  width = 15, height = 6)
```




# Session info 

```{r session_info}
R.version
sessionInfo()
```

