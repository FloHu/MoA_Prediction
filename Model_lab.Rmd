---
title: "Model lab: current 'production' models and experiments to improve them"
author: "Florian Huber"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

# Setup, library loading

```{r setup}
rm(list = ls())
# function to check if a package is installed, if so, load it, else install and then load it
source("./R/ipak.R")
# set chunk options and load libraries
source("./setup.R")
knitr::opts_chunk$set(message = FALSE, cache = TRUE)

matrix_container_ext <- readRDS("./data/programmatic_output/matrix_container_ext.rds")
moa_cols <- c("#e66101", "#fdb863", "#b2abd2", "#5e3c99")
moas <- c("cell_wall", "dna", "membrane_stress", "protein_synthesis")
my_moa <- "cell_wall"
datadir <- "./run_results_from_server/matrix_container_result"
```

# Section 1: Current production models + data

## Current 1-vs-rest 'production' model

To recapitulate from "Comparing_performances.Rmd": random forests was better than lasso because of 
model stabilities. Top25pct of features seems to have worked best. Only for protein synthesis the 
lasso seems to have done fairly well.

```{r}
(the_line <- which(matrix_container_ext$fitted_model == "classif.randomForest" & 
    matrix_container_ext$feat_preselect == "top25pct" & 
    ! matrix_container_ext$chemical_feats &
    matrix_container_ext$drug_dosages == "all"))
stopifnot(length(the_line) == 1)

sub_matrix_container <- matrix_container_ext[the_line, ]
my_data <- matrix_container_ext$drug_feature_matrices[[the_line]]
(unique(sub_matrix_container$drug_feature_matrices[[1]]$process_broad))

plot_perf_from_container(sub_matrix_container)
plot_perf_from_container(sub_matrix_container, what = "prec-recall")

# AUCs:
sub_matrix_container %>%
  select(feat_preselect, fitted_model, perf_measures) %>%
  unnest() %>% 
  group_by(fitted_model, feat_preselect, moa_modelled) %>%
  summarise(mean_auc = mean(auc))

# retrieve our results object:
chosen_res <- readRDS(file.path(datadir, make_filename(matrix_container_ext[the_line, ])))
```


## Retrieving most commonly used hyperparameters

Will be used for future model runs so that we don't have to do hyperparameter tuning anymore. 

```{r}
# first try to figure out which values were used for mtry and ntree in our favourite result
# we can get the optimal hyperparameters using getLearnerModel():
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)

# to access directly:
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)$learner.model$ntree
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)$learner.model$mtry

# hyperparameters that were tested
matrix_container_ext$hyperparam_grid[[the_line]]

# dimensions of the drug_feature_matrix:
dim(matrix_container_ext$drug_feature_matrices[[the_line]])

# which parameters were used/'won'?
(ntrees <- map_dbl(chosen_res, ~ getLearnerModel(.x[["Outer fold 1"]][["model_dna"]])$learner.model$ntree))

# for ntree it seems like only 200 or 500 trees were used:
all_ntrees <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, param = "ntree", 
  simplify = FALSE, USE.NAMES = TRUE)
lapply(all_ntrees, table)

all_mtrys <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, param = "mtry", 
  simplify = FALSE, USE.NAMES = TRUE)

lapply(all_mtrys, table)
```

`dna` and `cell_wall` clearly favour mtry = 106, membrane_stress and protein_synthesis: less clear, 
but the tendency there is more towards 200. Let's settle for 150, which is around a third of the 
observations, like it is recommended for regression trees in Kuhn et al. Moreover, we will use 
ntree = 1000 and mtry = 150. 

```{r}
my_ntree <- 1000
my_mtry <- 150
```


## Currently used multi-class model

### Training/loading the model

Get resampling instance from Untracked.Rmd.

```{r}
rep_rin <- readRDS("./data/programmatic_output/rep_rin_empirical.rds")

# we need to remove all but the 4 main modes of action:
my_data_multiclass <- filter(my_data, process_broad %in% moas)

length(unique(my_data_multiclass$drugname_typaslab))
unique(my_data_multiclass$process_broad)

(my_data_multiclass_train <- select(my_data_multiclass, -one_of(c("conc", "drugname_typaslab"))))
(rf_multi.task <- makeClassifTask(data = my_data_multiclass_train, target = "process_broad"))
(rf_multi.learner <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = my_ntree, mtry = my_mtry)))

my_filename <- "./data/programmatic_output/resampled_multiclass.rds"
if (file.exists(my_filename)) {
  resampled_multiclass <- readRDS(my_filename)
} else {
  resampled_multiclass <- resample(learner = rf_multi.learner, task = rf_multi.task, 
    measures = mmce, resampling = rep_rin, models = TRUE, keep.pred = TRUE)
  saveRDS(resampled_multiclass, file = my_filename)
}
```


### Performance and predictions of the model

#### Some metrics

```{r}
resampled_multiclass$aggr
```

So the mmce is ~ 1/3. With cell-wall model above it was ~ 7.5%. But not clear what kind of 
misclassification error we would get with our five models. Would we get more errors from 
aggregation?

#### Confusion matrix

```{r}
prd_obj <- resampled_multiclass$pred
cm <- calculateConfusionMatrix(prd_obj)
cm <- data.frame(cm$result[-5, -5], row.names = NULL, stringsAsFactors = FALSE)
cm$true <- colnames(cm)
cm <- gather(cm, cell_wall:protein_synthesis, key = "predicted", value = "n_obs")
cm$predicted <- factor(cm$predicted, levels = c("cell_wall", "dna", "membrane_stress", "protein_synthesis"))
cm$true <- factor(cm$true, levels = rev(levels(cm$predicted)))
cm$byclass_recall <- cm$n_obs / sapply(split(cm$n_obs, cm$true), sum)[cm$true]
cm$byclass_recall <- cut(cm$byclass_recall, breaks = seq(from = 0, to = 1, by = 0.1))

ggplot(cm, aes(x = predicted, y = true)) + 
  geom_tile(aes(fill = byclass_recall)) + 
  geom_text(aes(label = n_obs)) + 
  labs(x = "Predicted class, colours = recall/false-negative rate\n(normalisation by row)", 
    y = "True class") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 0)) + 
  scale_x_discrete(position = "top") + 
  scale_fill_brewer()
```

#### Investigating predictions

```{r}
resampled_multiclass_pred <- resampled_multiclass$pred$data

# merge with original information:
resampled_multiclass_pred <- bind_cols(my_data_multiclass[resampled_multiclass_pred$id, c(1, 2)], 
  resampled_multiclass_pred)
resampled_multiclass_pred <- arrange(resampled_multiclass_pred, drugname_typaslab, conc, iter)
head(resampled_multiclass_pred)
```

For example, the predictions + IQRs for A22:

```{r}
a22 <- filter(resampled_multiclass_pred, drugname_typaslab == "A22")
(a22 <- gather(a22, prob.cell_wall:prob.protein_synthesis, key = "class", value = "probability"))
a22$repetition <- cut(a22$iter, breaks = seq(from = 0.5, by = 8, length.out = 11), 
  labels = as.character(1:10))
(a22 <- 
   group_by(a22, conc, class, truth) %>%
   summarise(ymin = boxplot.stats(probability)$stats[2], 
             ymax = boxplot.stats(probability)$stats[4], 
             ymed = median(probability)))

ggplot(a22, aes(x = factor(conc), y = ymed, fill = class)) + 
  geom_pointrange(aes(ymin = ymin, ymax = ymax, colour = class), alpha = 0.75) + 
  geom_line(aes(group = class, colour = class)) + 
  facet_wrap( ~ truth) + 
  labs(x = "Concentration", y = "Prediction probability", title = "A22, multi-class prediction")
```

And now for all drugs:

```{r}
(multi_melted <- gather(resampled_multiclass_pred, prob.cell_wall:prob.protein_synthesis, 
  key = "class", value = "probability"))
multi_melted$repetition <- cut(multi_melted$iter, breaks = seq(from = 0.5, by = 8, length.out = 11), 
  labels = as.character(1:10))
(multi_melted <- 
   group_by(multi_melted, conc, class, truth, drugname_typaslab) %>%
   summarise(ymin = boxplot.stats(probability)$stats[2], 
             ymax = boxplot.stats(probability)$stats[4], 
             ymed = median(probability)))

# may also want to show this as a heatmap, compare fig. 11.4 in Kuhn book

p <- ggplot(multi_melted, aes(x = factor(conc), y = ymed, colour = class)) + 
   geom_pointrange(aes(ymin = ymin, ymax = ymax), alpha = 0.75, size = 2, fatten = 1) + 
   geom_line(aes(group = class), size = 1) + 
   facet_wrap( ~ truth + drugname_typaslab, scales = "free_x") + 
   geom_hline(yintercept = c(0.25, 0.5, 0.75), linetype = "dotted") + 
   scale_colour_manual("Predicted probability\nfor class", 
                       labels = c("Cell wall", "DNA", "Membrane stress", "Protein synthesis"), 
                       values = c("#1b9e77", "#d95f02", "#7570b3", "#e7298a"))

suppressWarnings(ggsave(filename = "./plots/Multiclass_probabilities.pdf", plot = p, width = 30, height = 20))
```

An attempt to characterise model quality of the multi-class model: 'winner' approach. One can see 
that this improves accuracy by about 10 percent - but probably should compare this to random 
guessing.

```{r}
# one simple approach: the highest median probability across all dosages wins
# then calculate the accuracy:

(winners <- 
    group_by(multi_melted, drugname_typaslab, truth) %>%
    arrange(desc(ymed), .by_group = TRUE) %>%
    slice(1) %>%
    mutate(response = str_extract(class, pattern = "cell_wall|dna|membrane_stress|protein_synthesis")))

# we should compare this to random guessing
mean(winners$response == winners$truth)
mean(winners$response != winners$truth)
```

Notably, stuff that is predicted badly is the same as in the previous models. 

```{r}
winners[winners$response != winners$truth, ]
```

#### Probability calibration plot

From Kuhn, chapter 11, p294ff.: probability calibration plot. Should also compare with our original 
1-vs-others approach. 

```{r}
# 10 bins with probabilities from 0 - 100, compare with actually observed frequencies
# one plot per MoA (?) - in Kuhn book: only use this for binary classification
prob_calib <- multi_melted
prob_calib$prob_bin <- cut(prob_calib$ymed, breaks = seq(from = 0, to = 1, by = 0.1))
levels(prob_calib$prob_bin)
prob_calib$prob_is_for <- 
   str_extract(prob_calib$class, pattern = "cell_wall|dna|membrane_stress|protein_synthesis")
prob_calib$truth <- as.character(prob_calib$truth)
prob_calib <- 
   group_by(prob_calib, prob_is_for, prob_bin) %>%
   summarise(true_fraction = mean(truth == prob_is_for))
prob_calib

p0 <- ggplot(prob_calib, aes(x = as.numeric(prob_bin) - 0.05, y = true_fraction)) + 
   geom_point() + 
   geom_line(aes(group = prob_is_for)) + 
   geom_abline(slope = 1/10, linetype = "dotted") + 
   facet_wrap( ~ prob_is_for, ncol = 2) + 
   coord_cartesian(xlim = c(0, 10), ylim = c(0, 1)) + 
   theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
   scale_x_continuous(breaks = seq(from = 0.5, to = 9.5, by = 1), 
                      labels = as.character(10 * seq(from = 0, to = 9, by = 1))) + 
   labs(x = "Probability bin midpoint (10% bins)", y = "Observed event percentage", 
        title = "Probability calibration plot for multi-class RF")
p0

ggsave("./plots/probability_calibration.pdf")
```



## Multi-class model performance is similar to several 1-vs-all models

### Comparing multi-class probabilities with 1-vs-rest probabilities

For each mode of action: compare probabilities for each observation returned by either 
(i) multi-class model or (ii) the respective 1-vs-all model. 

```{r}
# do for each MoA separately
head(multi_melted)
head(pred_data_1vsall <- matrix_container_ext$pred_data[[the_line]])

filter(multi_melted, drugname_typaslab == "LEVOFLOXACIN")
filter(pred_data_1vsall, drugname_typaslab == "LEVOFLOXACIN")

# so the original data hasn't been aggregated across repeats yet
# the final table should be: drug - conc - multi_or_1vsall median_prob
# probability of moa_modelled is the probability prob.moa_modelled in the multi-class model
prob_comp <- ungroup(multi_melted)
prob_comp$moa_modelled <- str_extract(prob_comp$class, 
  pattern = "cell_wall|dna|protein_synthesis|membrane_stress")
prob_comp <- rename(prob_comp, 'prob.moa_median' = 'ymed')
prob_comp <- select(prob_comp, drugname_typaslab, conc, moa_modelled, prob.moa_median)
head(prob_comp)
nrow(prob_comp)

# note that this has more drugs because in our multi-class model we only considered the 4 main 
# modes of action
# that's also the reason for the left_join
pred_data_1vsall <- group_by(pred_data_1vsall, drugname_typaslab, conc, moa_modelled) %>%
  summarise(prob.moa_median = median(prob.moa))
head(pred_data_1vsall)
nrow(prob_comp)

prob_comp <- 
  left_join(prob_comp, pred_data_1vsall, by = c("drugname_typaslab", "conc", "moa_modelled"), 
  suffix = c('_multi', '_1vsall'))

p1 <- ggplot(prob_comp, aes(x = prob.moa_median_1vsall, y = prob.moa_median_multi)) + 
  geom_point() + 
  facet_wrap( ~ moa_modelled, ncol = 2) + 
  labs(title = "Comparison of probabilities 1-vs-all and multi-class model")
p1

by(prob_comp, INDICES = prob_comp$moa_modelled, FUN = function(data) {
  cor(data$prob.moa_median_1vsall, y = data$prob.moa_median_multi, method = "spearman")
})
```

<!--
### Comparing feature importances
-->

```{r, echo = FALSE, eval = FALSE}
getFeatureImportance(resampled_multiclass$models[[1]])

feat_imps_multi <- imap_dfr(resampled_multiclass$models, function(val, pos) {
  f <- getFeatureImportance(val)$res
  f <- tibble(gene = names(f), importance = unlist(f), modelnum = pos)
})

head(feat_imps_multi)
```

<!--
In that case we don't have any features that only occur in some models. Perhaps because we are 
using more trees? So in that case, one histogram is enough
-->

```{r, echo = FALSE, eval = FALSE}
all(rle(sort(feat_imps_multi$gene))$lengths == 80)
colnames(my_data_multiclass)[! colnames(my_data_multiclass) %in% feat_imps_multi$gene]

# so we can easily summarise
feat_imps_multi <- 
  group_by(feat_imps_multi, gene) %>%
  summarise(median_importance = median(importance))

ggplot(feat_imps_multi, aes(x = log2(median_importance))) + 
  geom_histogram()

fingerprint1_multi <- feat_imps_multi$gene[log2(feat_imps_multi$median_importance) > -2.5]
fingerprint2_multi <- feat_imps_multi$gene[log2(feat_imps_multi$median_importance) > 0]

# compare with fingerprints from 1-vs-all models
fingerprints_onevsall <- readRDS("./data/programmatic_output/fingerprints_1vsall.rds")

library(VennDiagram)
venn.plot <- venn.diagram(
  list("fingerprint1_1vsall" = fingerprints_onevsall$fingerprint1, 
    "fingerprint1_multi" = fingerprint1_multi), 
  imagetype = "png", 
  "./plots/fingerprint1.png"
)

venn.plot <- venn.diagram(
  list("fingerprint2_1vsall" = fingerprints_onevsall$fingerprint2, 
    "fingerprint2_multi" = fingerprint2_multi), 
  imagetype = "png", 
  "./plots/fingerprint2.png"
)

feat_imps_nest <- fingerprints_onevsall$feat_imps_nest
# alternatively, we can plot a correlation:
tmp <- left_join(feat_imps_multi, feat_imps_nest[, c("moa", "gene", "median_importance")], 
  by = "gene", suffix = c("_multi", "_1vsall"))

ggplot(tmp, aes(x = log2(median_importance_1vsall), y = log2(median_importance_multi))) + 
  geom_point() + 
  facet_wrap( ~ moa) + 
  labs(title = "Comparison of feature importances 1-vs-all and multi-class model")

by(data = tmp, tmp$moa, function(data) {
  cor(x = data$median_importance_1vsall, y = data$median_importance_multi, method = "spearman")
})
```


<!--
## Comparing with baseline models

### KNN

Has certain advantages: less black and white; assumption matches intuition that similar MoA = close 
in chemical genomics space. Will fail however when other features are included. 

```{r, echo = FALSE, eval = FALSE}
getLearnerParamSet("classif.knn")

(knn_params <- makeParamSet(
   makeDiscreteParam("k", values = 1:25)
))
class(knn_params)

(ctrl <- makeTuneControlGrid())
class(ctrl)

knn.learner <- makeLearner("classif.knn")

# rf.task was with cell_wall only
res_knn <- tuneParams(learner = knn.learner, task = rf.task, resampling = rep_rin, 
  measures = list(mmce), par.set = knn_params, control = ctrl)

res_knn
res_knn$x # this is a bit higher than for the random forests model above:
r2_correct$aggr 


data <- generateHyperParsEffectData(res_knn)
plotHyperParsEffect(data, x = "k", y = "mmce.test.mean", plot.type = "line")
```
-->


# Section 2: Improving models

First we need a quick and informative way of comparing models with each other. In the sections 
above we have a set of 1-vs-rest and one multi-model 'production' model. Both are likely to change 
because of data processing steps and certain things we wanna try to improve. 

<!--
Both 1-vs-rest and multi-class models will come in several variants (improved preprocessing and 
other things). So we need an easy way to find out which one is better but also to peek a bit into 
the models. 

Ways of comparing: metrics, ROC curves, probabilities, feature importances.

Comparisons: 
(i) When using many models, use a 'large-scale' type of comparison that generates tables 
or other kinds of overview, like in Comparing_performances.Rmd.
(ii) Small scale comparisons when looking at 1-vs-1, 1-vs-multi, multi-vs-multi.
(iii) Make also some report function to get a quick overview on a specific model. 

Needed: a quick way of making new models. We already have one for the new multi-class model, we 
would also need one for new instances of the 1-vs-rest models - either locally or on the cluster. 

Perhaps we should think about using generic functions here? 

Btw ... to directly embed something in Rmarkdown use:
![my](./plots/cell_wall_probs.pdf)

to just link to a local file use:
<a href="./plots/cell_wall_probs.pdf">link</a>
-->

```{r}
# so first let's get on the same page with which models we are dealing with:
# View(resampled_multiclass)
# View(chosen_res)
pryr::object_size(resampled_multiclass)
pryr::object_size(chosen_res)
# - quite unwieldy, about 0.5 and 2.1 GB, respectively 

class(resampled_multiclass)
class(chosen_res)

report_on_model <- function(modelobj) UseMethod("report_on_model")

report_on_model.list <- function(modelobj) {
  # cat(deparse(substitute(modelobj)), " in function for list\n")
  # print(pryr::object_size(modelobj))
}

report_on_model.ResampleResult <- function(modelobj) {
  # cat(deparse(substitute(modelobj)), " in function for ResampleResult\n")
  # print(pryr::object_size(modelobj))
}

report_on_model(resampled_multiclass)
report_on_model(chosen_res)

report_on_model <- function() {
  # input: either a 1-vs-rest model or a resampled_multiclass model 
  # consider making this a generic function
  
  # if 1-vs-rest model:
    # - report on a number of metrics
    # - plot ROC and prec-recall curves (including (partial) AUCs)
    # ? plot prediction probabilities
    # - plot feature importances
  
  # if multi-class model:
    # - report on a number of metrics
    # ??? plot ROC curve?
    # - plot confusion matrix
    # - plot probability calibration plot
    # - plot probabilities
    # plot feature importances
    # etc.
}

compare_2_models <- function() {
  # play around with multiple dispatch
  # first compare certain metrics, depending on combination
  # comparison plots:
  
  # 1-vs-rest + 1-vs-rest:
  # can simply compare ROC and precision-recall curves, can also compare probabilities
  
  # 1-vs-rest + multi-class:
  # see notebook
  
  # multi-class vs. multi-class:
  # perhaps confusion matrices and probabilities? 
  
  # generate plots using report_on_model, then arrange them side by side using cowplot
  # after that, try manually adding annotations
  
  # INPUT: 
    # matrix_container_ext rows - can also come from two different matrix_container_ext objects
}

model1 <- matrix_container_ext[9, ]
model2 <- matrix_container_ext[10, ]
file <- "./plots/compare_1vsrest.pdf"

theme_update()

compare_1vsrest_models <- function(model1, model2, file) {
  # INPUT: 2 matrix_container_ext rows
  # OUTPUT: a pdf file 
  stopifnot(nrow(model1) == 1 & nrow(model2) == 1)
  # ROC curves:
  model1_ROC <- plot_perf_from_container(model1)
  model2_ROC <- plot_perf_from_container(model2)
  # precision-recall curves:
  model1_prc <- plot_perf_from_container(model1, what = "prec-recall")
  model2_prc <- plot_perf_from_container(model2, what = "prec-recall")
  # probabilities:
  model1_probs <- plot_highest_probs(model1$pred_data[[1]], dosg_to_plot = "highest_prob", 
    which_moa = "cell_wall")
  model2_probs <- plot_highest_probs(model2$pred_data[[1]], dosg_to_plot = "highest_prob", 
    which_moa = "cell_wall")
  
  output <- cowplot::plot_grid(model1_ROC, model2_ROC, model1_prc, model2_prc, 
    labels = c("A", "B", "C", "D"))
  cowplot::save_plot(filename = file, plot = output, ncol = 2, nrow = 2, 
    base_width = 6, base_height = 6)
}

compare_1vsrest_models(model1, model2, file)

```


```{r}
# try those things:
# problem: if several ROC curves are plotted, cannot easily put the AUC on top of the plot
# because ggplot2 doesn't allow separate legends - would have to use some grid arrangement stuff 
# but then the rest of the code would get more complicated
# use tables in that case
l <- plot_perf_from_container(matrix_container_ext[c(9, 10), ], col_var = "chemical_feats")
l <- plot_perf_from_container(matrix_container_ext[9, ])
l <- plot_perf_from_container(matrix_container_ext[c(1:4), ], col_var = "chemical_feats", 
  row_var = "feat_preselect")

l

# Multiple plots on the same or different pages
pdf(paper = "a4", file = "./plots/test.pdf")
print(p0)
print(p1)
dev.off()

# multipage:
pdf(paper = "a4", file = "./plots/test2.pdf")
cowplot::plot_grid(p0, p1, nrow = 2)
dev.off()
```


# Session info 

```{r session_info}
R.version
sessionInfo()
```

