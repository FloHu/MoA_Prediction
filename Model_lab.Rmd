---
title: "Model lab: current 'production' models and experiments to improve them"
author: "Florian Huber"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

# Setup, library loading

```{r setup, cache = FALSE}
rm(list = ls())
# function to check if a package is installed, if so, load it, else install and then load it
source("./R/ipak.R")
# set chunk options and load libraries
source("./setup.R")
knitr::opts_chunk$set(message = FALSE, cache = TRUE)

datadir <- "./data/programmatic_output"
resultsdir <- "./run_results_from_server/matrix_container_result_2019"

mc_ext <- readRDS(file.path(datadir, "matrix_container_ext_2019.rds"))
mc_ext$feat_preselect %<>% as.character()

moa_cols <- c(cell_wall = "#1b9e77", dna = "#d95f02", 
  membrane_stress = "#7570b3", protein_synthesis = "#e7298a")
moas <- names(moa_cols)

moa_repl <- c(cell_wall = "Cell Wall", dna = "DNA", 
  membrane_stress = "Membrane Stress", protein_synthesis = "Protein Synthesis")

moa_repl2 <- c(prob.cell_wall = "Cell Wall", prob.dna = "DNA", 
  prob.membrane_stress = "Membrane Stress", 
  prob.protein_synthesis = "Protein Synthesis")

my_moa <- "cell_wall"

head(mics <- read_delim("./data/programmatic_output/MICs.csv", delim = ";"))
```

# Section 1: Current production models + data

## Current 1-vs-rest 'production' model

To recapitulate from "Comparing_performances.Rmd": random forests was better 
than lasso because of model stabilities. Top25pct of features seems to have 
worked best. Only for protein synthesis the lasso seems to have done fairly 
well.

```{r}
(the_line <- which(mc_ext$fitted_model == "classif.randomForest" & 
    mc_ext$feat_preselect == "top15pct" & 
    !mc_ext$chemical_feats &
    mc_ext$drug_dosages == "all"))
stopifnot(length(the_line) == 1)
save(the_line, file = "./data/programmatic_output/the_line.RData")

mc_ext_sub <- mc_ext[the_line, ]
mc_ext_dfm <- mc_ext$drug_feature_matrices[[the_line]]
unique(mc_ext_dfm$process_broad)

plot_perf_from_container(mc_ext_sub)
plot_perf_from_container(mc_ext_sub, what = "prec-recall")

# AUCs:
mc_ext_sub %>%
  select(feat_preselect, fitted_model, perf_measures) %>%
  unnest() %>% 
  group_by(fitted_model, feat_preselect, moa_modelled) %>%
  summarise(mean_auc = mean(auc))

# retrieve our results object:
chosen_res <- readRDS(file.path(resultsdir, make_filename(mc_ext[the_line, ])))
# saveRDS(chosen_res, file = "./data/programmatic_output/chosen_res.rds")

# mmces:
main_mmces <- map(moas, function(moa) {
  unlist(map(chosen_res, function(repetition) {
    flatten_dbl(map(map(repetition, paste0("prediction_", moa)), performance))
  }))
})
names(main_mmces) <- moas
sapply(main_mmces, mean)
```


## Retrieving most commonly used hyperparameters

Will be used for future model runs so that we don't have to do hyperparameter 
tuning anymore. 

```{r}
# first try to figure out which values were used for mtry and ntree in our 
# favourite result
# we can get the optimal hyperparameters using getLearnerModel():
chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna

# to access directly:
chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna$learner.model$ntree
chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna$learner.model$mtry

# hyperparameters that were tested
mc_ext$hyperparam_grid[[the_line]]

# dimensions of the drug_feature_matrix:
dim(mc_ext$drug_feature_matrices[[the_line]])

# which parameters were used/'won'?
(ntrees <- map_dbl(chosen_res, ~ .x[["Outer fold 1"]][["model_dna"]]$learner.model$ntree))

# for ntree it seems like only 200 or 500 trees were used:
all_ntrees <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, 
  param = "ntree", simplify = FALSE, USE.NAMES = TRUE)
lapply(all_ntrees, table)

all_mtrys <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, 
  param = "mtry", simplify = FALSE, USE.NAMES = TRUE)

lapply(all_mtrys, table)
```


```{r}
my_ntree <- 1000
my_mtry <- 83
```


## Currently used multi-class model

### Training/loading the model

Make default version of fitting functions.  

```{r}
# make default version of the functions with common default values
fit_multiclass_rf_def <- partial(fit_multiclass_rf, targetvar = "process_broad", 
  blockingvar = "drugname_typaslab", paramlist = list(ntree = my_ntree, 
    mtry = my_mtry), cols_to_exclude = c("drugname_typaslab", "conc"))

fit_multiclass_rf_loo_def <- partial(fit_multiclass_rf_loo, 
  targetvar = "process_broad", blockingvar = "drugname_typaslab", 
  paramlist = list(ntree = my_ntree, mtry = my_mtry), 
  cols_to_exclude = c("drugname_typaslab", "conc"))
```

Fit the model, once with normal resampling, once with LOOCV. 

```{r}
resampled_mcl_2019 <- fit_or_load(varname = "resampled_mcl_2019", 
  directory = datadir, dfm = mc_ext_dfm, fitting_fun = fit_multiclass_rf_def)

resampled_mcl_2019_melt <- melt_pred_data(resampled_mcl_2019, 
  model_type = "multiclass")

plot_mcl_probs_heatmap(melted_pred_data = resampled_mcl_2019_melt, mics = mics) %>% 
  ggsave(filename = "./plots/Prediction_heatmap_mcl_normal_resmp.pdf", 
    width = 20, height = 20)
```


```{r}
resampled_mcl_loo <- fit_or_load(varname = "resampled_mcl_loo", 
  directory = datadir, dfm = mc_ext_dfm, fitting_fun = fit_multiclass_rf_loo_def)

resampled_mcl_loo_melt <- melt_pred_data(resampled_mcl_loo, 
  model_type = "multiclass")

plot_mcl_probs_heatmap(melted_pred_data = resampled_mcl_loo_melt, mics = mics) %>% 
  ggsave(filename = "./plots/Prediction_heatmap_multiclassLOO.pdf", 
    width = 12, height = 5)
```


```{r}
resampled_mcl_2019$aggr
resampled_mcl_loo$aggr
```

#### Comparison nested resampling with LOOCV

```{r, loocv}
compare_multiclass_models(model1_mc = resampled_mcl_2019, 
  model1_mc_title = "Multiclass, 4 main modes, rep nest CV", 
  model2_mc = resampled_mcl_loo, 
  model2_mc_title = "Multiclass, 4 main modes, LOOCV", 
  "./plots/Compare_multiclass_loocv.pdf")

# therefore ...
(multicl_pred_melt <- melt_pred_data(resampled_mcl_loo, 
  model_type = "multiclass"))
```


## Multi-class model performance is similar to several 1-vs-all models

### Comparing multi-class probabilities with 1-vs-rest probabilities

For each mode of action: compare probabilities for each observation returned by 
either (i) multi-class model or (ii) the respective 1-vs-all model.

```{r}
(onevsall_pred_melt <- melt_pred_data(mc_ext$pred_data[[the_line]], 
  model_type = "onevsrest"))

prob_comparison <- compare_probabilities(list("Multiclass" = multicl_pred_melt, 
  "1-vs-rest" = onevsall_pred_melt))
prob_comparison$plot
prob_comparison$spearman_corrs

ggsave("./plots/Compare_probs_multi-vs-1vsrest.pdf", width = 6, height = 5)
```


### Comparing multi-class feature importances with 1-vs-rest feature importances

Note that the correlation is not too high: which is perhaps not too surprising 
because some features will be considered importance in the multi-class model 
but might be considered important in only *one* 1-vs-rest model.

```{r}
(feat_imps_multi <- summarise_feat_imps(get_feat_imps(resampled_mcl_loo)))
(feat_imps_1vsrest <- summarise_feat_imps(get_feat_imps(chosen_res)))

(tmp <- left_join(feat_imps_multi, 
  feat_imps_1vsrest[, c("moa", "gene", "med_imp")], 
  by = "gene", suffix = c("_multi", "_1vsall")))

ggplot(tmp, aes(x = log2(med_imp_1vsall), 
  y = log2(med_imp_multi))) + 
  geom_point() + 
  facet_wrap( ~ moa) + 
  labs(title = "Comparison of feature importances 1-vs-all and multi-class model")
```


## Closer inspection of multiclass model

Only showing for LOOCV.

### Metrics, ROC curves

```{r}
resampled_mcl_loo$aggr
```

So the mmce is ~ 1/3. With cell-wall model above it was ~ 7.5%. But not clear 
what kind of misclassification error we would get with our five models. Would 
we get more errors from aggregation? 

However, ROC curves look similar to old models, or even better. 

```{r}
plot_roc_mcl(resampled_multiclass = resampled_mcl_loo, positives = moas)
ggsave("./plots/ROC_MULTICLASS_RF_top15pct_all_nochem.pdf", width = 7, 
  height = 4.5)
```


### Confusion matrix

```{r}
(cm <- get_wide_confmat(resampled_mcl_loo))
plot_wide_confmat(cm, title = "Multiclass, 4 main modes, LOO")
ggsave("./plots/Confusion_matrix_mainmodes_LOO.pdf", width = 6, height = 5)
```


### Probability calibration plot

From Kuhn, chapter 11, p294ff.: probability calibration plot.

```{r}
# 10 bins with probabilities from 0 - 100, compare with actually observed frequencies
# one plot per MoA (?) - in Kuhn book: only use this for binary classification
plot_prob_calib(resampled_mcl_loo, title = "LOO multi-class model")
ggsave("./plots/Probability_calibration_LOO.pdf", width = 5, height = 5)
```


<!--
#### ROC-curve for multi-class models

##### A few reflections

This can only be defined for a mode-of-action - not-mode-of-action pair. 
Otherwise, how should "true negative" be defined - this only makes sense if one 
class is defined as positive. The following chunks demonstrates the problem:

```{r, eval = FALSE, echo = FALSE}
# see also below
head(resampled_multiclass$pred$data)
(mcl_melt <- melt_pred_data(resampled_multiclass, model_type = "multiclass"))
mcl_melt$predicted_prob <- str_replace(string = mcl_melt$predicted_prob, 
  pattern = "prob\\.", replacement = "")
mcl_melt$truth <- as.character(mcl_melt$truth)

# average concentrations
(mcl_melt_concavg <- 
  mcl_melt %>%
  select(drugname_typaslab, conc, predicted_prob, prob.med, truth) %>%
  group_by(drugname_typaslab, truth, predicted_prob) %>%
  summarise(prob.med_avg = mean(prob.med)))

# note: truth = actual class label
(mcl_melt_roc_input <- 
  mcl_melt_concavg %>%
  group_by(drugname_typaslab) %>%
  mutate(highest_vs_2nd = prob.med_avg - max(prob.med_avg[-which.max(prob.med_avg)])) %>%
  top_n(1, prob.med_avg) %>%
  rename(class_highest_p = predicted_prob, highest_p = prob.med_avg))
```

Now consider what happens when we decide on a threshold at which we call an 
observation to belong to the class that has the highest probability, e.g. 0.2:

```{r, eval = FALSE, echo = FALSE}
mcl_melt_roc_input$response <- 
  ifelse(mcl_melt_roc_input$highest_vs_2nd >= 0.2, 
    mcl_melt_roc_input$class_highest_p, 
    paste0("not_", mcl_melt_roc_input$class_highest_p))
head(mcl_melt_roc_input)
```

Obviously, such an output is useless for TP, FN etc. rate calculations. 

So instead we write a function that defines one class as positive and the 
others as negative.

```{r, eval = FALSE, echo = FALSE}
get_responses_mcl <- function(dfr, positive, thresh) {
  # positive = MoA that is considered positive
  dfr$positive <- positive
  dfr$thresh <- thresh
  negative <- paste0("not_", positive)
  dfr$truth <- ifelse(dfr$truth == positive, positive, negative)
  # give answer according to class_highest_p
  dfr$response <- ifelse(dfr$highest_vs_2nd >= thresh, dfr$class_highest_p, 
    paste0("not_", dfr$class_highest_p))
  # now it gets philosophical: based on this routine an observation might be 
  # "not_membrane_stress" while the positive class is "cell_wall". Strictly 
  # speaking it's not clear if this is true negative or even true positive. 
  # But we might argue that "not_membrane_stress" would also mean "not_" for 
  # any other class - so we can still classify it as true negative. 
  #### TO DO: discuss this or read up on this
  dfr$response <- ifelse(dfr$response == positive, positive, negative)
  dfr$tp <- (dfr$response == dfr$truth) & (dfr$truth == positive)
  dfr$fn <- (dfr$response != dfr$truth) & (dfr$truth == positive)
  dfr$fp <- (dfr$response != dfr$truth) & (dfr$truth != positive)
  dfr$tn <- (dfr$response == dfr$truth) & (dfr$truth != positive)
  stopifnot(sum(unlist(dfr[, c("tp", "fn", "fp", "tn")])) == nrow(dfr))
  return(dfr)
}

(tmp <- get_responses_mcl(mcl_melt_roc_input, positive = "cell_wall", thresh = 0.2))

get_metrics_mcl <- function(dfr) {
  stopifnot(length(unique(dfr$thresh)) == 1)
  dfr <- group_by(dfr, thresh, positive) %>%
    summarise(tp = sum(tp), fn = sum(fn), tn = sum(tn), fp = sum(fp), 
      tpr = tp / (tp + fn), fpr = fp / (fp + tn), ppv = tp / (tp + fp))
  return(dfr)
}

get_metrics_mcl(tmp)

get_thresh_vs_perf_mcl <- function(dfr, positive, thresholds) {
  thresh_vs_perf <- map_dfr(thresholds, function(.thresh) {
    get_metrics_mcl(get_responses_mcl(dfr, positive, .thresh))
  })
  thresh_vs_perf <- arrange(thresh_vs_perf, desc(thresh))
  return(thresh_vs_perf)
}

(from <- range(mcl_melt_roc_input$highest_vs_2nd)[1])
(to <- range(mcl_melt_roc_input$highest_vs_2nd)[2])
tmp <- get_thresh_vs_perf_mcl(mcl_melt_roc_input, positive = "cell_wall", 
  thresholds = seq(from = from, to = to, length.out = 100))

print(tmp, n = 100)

ggplot(tmp, aes(x = fpr, y = tpr)) + 
  geom_path(aes(group = 1)) + 
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1))
```

What happened? The issue is that we use `highest_vs_2nd` as the metric to put a 
threshold on. This works fine at the beginning: we have a high confidence 
(i.e. high `highest_vs_2nd` levels) so the tpr goes up without compromising the 
fpr. However, when the threshold approaches 0 there are quite a few 
observations that never "get the chance" to be put into the positive class. 
Why? Because their probability for the positive class (`cell_wall` in this 
case) is lower than the 2nd highest probability so they are just all classified 
as negative cases. Therefore, neither the TPR nor the FPR can ever go up again. 
Conclusion: `highest_vs_2nd` is not a sensible metric. 


##### Better version

Go back to simply putting a threshold on the probabilities themselves.

```{r, eval = FALSE, echo = FALSE}
# just to demonstrate the different steps contained in plot_roc_mcl()
source("./R/ROC_mcl_models.R") # don't remove this line!
(tmp <- prep_roc(resampled_multiclass, positive = "cell_wall"))
(tmp <- get_responses_mcl(tmp, positive = "cell_wall", thresh = 0.5))
(tmp <- get_metrics_mcl(tmp))

(tmp <- get_thresh_vs_perf_mcl(resampled_multiclass, positive = "cell_wall"))
print(tmp, n = 20)

plots <- map(moas, plot_roc_mcl, resampled_multiclass = resampled_multiclass)
names(plots) <- moas

plots$cell_wall
plots$dna
plots$membrane_stress
plots$protein_synthesis
```

-->

### Investigating predictions

For example, the predictions + IQRs for A22:

```{r}
filter(multicl_pred_melt, drugname_typaslab == "A22") %>% 
  ggplot(aes(x = factor(conc), y = prob.med, fill = predicted_prob)) + 
  geom_pointrange(aes(ymin = prob.min, ymax = prob.max, colour = predicted_prob), 
    alpha = 0.75) + 
  geom_line(aes(group = predicted_prob, colour = predicted_prob)) + 
  facet_wrap( ~ truth) + 
  labs(x = "Concentration", y = "Prediction probability", 
    title = "A22, multi-class prediction")
```

Add MIC information

```{r}
# # already run:
# source("./parse_MIC_data.R")
multicl_pred_melt_micinfo <- left_join(multicl_pred_melt, mics)

# use slice() instead of top_n(), the latter implies some type of ordering!
(helper_frame <- multicl_pred_melt_micinfo %>%
    group_by(drugname_typaslab) %>%
    slice(1) %>%
    mutate(label = as.character(mic_curated)))

helper_frame$label[is.na(helper_frame$label)] <- "NA"
helper_frame$label <- paste0("MIC: ", helper_frame$label, "\n", "resistant: ", 
  ifelse(helper_frame$resistant, "yes", "no"))
```

Line display:

```{r}
# note that hjust = 0 is needed for geom_text() for left justification
# geom_text will respect
p <- plot_mcl_probs_lines(multicl_pred_melt_micinfo, labels = moas, 
  colours = moa_cols, printplot = FALSE)

p <- p + geom_text(data = helper_frame, aes(x = 1, y = 0.9, label = label), 
      hjust = 0, inherit.aes = FALSE)

ggsave(filename = "./plots/Probability_lines_multiclass_LOO.pdf", 
  plot = p, width = 30, height = 20)
```

<a href="./plots/Multiclass_probabilities_LOO.pdf">LINK</a>


Playing around with "winner approach"

An attempt to characterise model quality of the multi-class model: 'winner' 
approach. One can see that this improves accuracy by > 10 percent - but 
probably should compare this to random guessing.

```{r}
# one simple approach: the highest median probability across all dosages wins
# then calculate the accuracy:
(winners <- multicl_pred_melt %>%
    group_by(drugname_typaslab, truth) %>%
    arrange(desc(prob.med), .by_group = TRUE) %>%
    slice(1) %>%
    mutate(response = str_extract(predicted_prob, 
      pattern = "cell_wall|dna|membrane_stress|protein_synthesis")))

# we should compare this to random guessing
mean(winners$response == winners$truth)
mean(winners$response != winners$truth)
```

Notably, stuff that is predicted badly is the same as in the previous models. 

```{r}
winners[winners$response != winners$truth, ]
```


# Section 2: Improving models

First we need a quick and informative way of comparing models with each other. 
In the sections above we have a set of 1-vs-rest and one multi-model 
'production' model. Both are likely to change because of data processing steps 
and certain things we wanna try to improve. 

So first we want to have a good way of comparing models.

## Demonstration of how models can be compared

```{r}
# so first let's get on the same page with which models we are dealing with:
# View(resampled_multiclass)
# View(chosen_res)
pryr::object_size(resampled_mcl_loo)
pryr::object_size(chosen_res)
# - quite unwieldy, about 0.5 and 2.1 GB, respectively 

class(resampled_mcl_loo)
class(chosen_res)
```

### For a pair of one-vs-rest models

For example, random forests without and with chemical features.

```{r}
model1 <- mc_ext[5, ]
model2 <- mc_ext[6, ]

compare_onevsrest_models(model1 = model1, model2 = model2, 
  model1_title = "RF without chem. feats", 
  model2_title = "RF with chem. feats", 
  file = "./plots/Compare_one-vs-rest_chemfeats.pdf")
```


### For a pair of multi-class models

See comparison of CV instance based and resampling-based multi-class model 
above. 


## Trying to improve classification

### Multi-class model after improved data pre-processing

#### No NA values, collinear features merged

_Deprecated since commit bd6a8c4_

<!--
Note that removing the collinear features had virtually no effect on model 
performance. 

Things changed:
1. Features with lots of NA values were removed
2. Collinear features were merged (averaged)
-->

```{r, eval = FALSE, echo = FALSE}
# # Taken from chunk further above: 
# # rep_rin
# # feats: containing the merged features
# stopifnot(nrow(my_data_mcl) == nrow(feats))
# ncol(my_backup$feats) - ncol(feats)
# 
# my_data_mcl_preproc <- my_data_mcl
# (drugdata <- my_data_mcl_preproc[, c(1:3)])
# # columns that are new due to merging:
# (to_add <- feats[, nchar(colnames(feats)) > 4])
# # exclude columns that are not in feats: these are the ones that were merged
# my_data_mcl_preproc <- my_data_mcl_preproc[, colnames(my_data_mcl_preproc) %in% colnames(feats)]
# # add merged features back together
# my_data_mcl_preproc <- cbind(drugdata, my_data_mcl_preproc, to_add)
# 
# ncol(my_data_mcl)
# ncol(my_data_mcl_preproc)
# 
# # training the next model: 
# my_data_mcl_preproc_task <- select(my_data_mcl_preproc, -one_of(c("conc", "drugname_typaslab")))
# rf_multi.task <- makeClassifTask(data = my_data_mcl_preproc_task, target = "process_broad")
# rf_multi.learner <- makeLearner("classif.randomForest", predict.type = "prob",
#   par.vals = list(ntree = my_ntree, mtry = my_mtry))
```

_Note deprecation note above._

```{r, eval = FALSE, echo = FALSE}
# my_filename <- "./data/programmatic_output/resampled_multiclass_noNAs_corrs_merged.rds"
# 
# if (file.exists(my_filename)) {
#   resampled_multiclass_preproc <- readRDS(my_filename)
# } else {
#   # resampled_multiclass_preproc <- fit_multiclass_rf(learner = rf_multi.learner, 
#   #   task = rf_multi.task, measures = mmce, rin = rep_rin_mcl, 
#   #   task_data_all_cols = my_data_mcl_preproc)
#   # saveRDS(resampled_multiclass_preproc, file = my_filename)
# }
```

<!-- Comparing this with the previous multi-class model: -->

```{r}
# compare_multiclass_models(model1_mc = resampled_multiclass, model2_mc = resampled_multiclass_preproc, 
#   model1_mc_title = "'naive' multiclass model", model2_mc_title = "multiclass model w/o NAs, corr 
#   feats merged", file = "./plots/compare_multiclass_noNAs_corrs_merged.pdf")
```


### "Two highest"

_Note: also to be deprecated soon since most of the issues probably resolved after commit bd6a8c4._ 

Keep just two highest dosages and see what happens. Removing drugs with low 
number of significant interactions: not necessary anymore, is now taken care of 
in data preprocessing steps. 

```{r}
(n_signif <- mc_ext$pred_data_with_n_signif[[the_line]])

print(filter(n_signif, moa_modelled_is_truth, n_signif < 10) %>%
  arrange(drugname_typaslab), n = 100)
```


```{r}
mc_ext_dfm_2highest <- 
  group_by(mc_ext_dfm, drugname_typaslab) %>%
  top_n(2, conc) %>%
  ungroup()

resampled_mcl_loo_2highest <- fit_or_load("resampled_mcl_loo_2highest", 
  directory = datadir, dfm = mc_ext_dfm_2highest, fitting_fun = 
    fit_multiclass_rf_loo_def)

resampled_mcl_loo_2highest$aggr
```

Comparison with multi-class model using all dosages. 

```{r}
compare_multiclass_models(model1_mc = resampled_mcl_loo, 
  model1_mc_title = "Multiclass, all dosgs, LOO", 
  model2_mc = resampled_mcl_loo_2highest, 
  model2_mc_title = "Multiclass, 2 highest dosg, LOO", 
  "./plots/Compare_multiclass_alldosg_vs_2highest.pdf")
```

Probability details.

```{r}
multicl_2highest_melt <- melt_pred_data(resampled_mcl_loo_2highest, 
  model_type = "multiclass")

plot_mcl_probs_lines(multicl_2highest_melt, labels = moas, colours = moa_cols, 
  printplot = FALSE) %>%
  ggsave(filename = "./plots/Prediction_lines_LOO_2highest.pdf", 
    width = 30, height = 20)

plot_mcl_probs_heatmap(melted_pred_data = multicl_2highest_melt, mics = mics) %>% 
  ggsave(filename = "./plots/Prediction_heatmap_LOO_2highest.pdf", 
  width = 20, height = 20)
```



# Predicting MoA subgroups

```{r, subgroups}
(mode_of_action <- read_delim("./data/programmatic_output/drug_moa_gdrive.csv", 
  delim = ",", na = c("-")))
mode_of_action <- mode_of_action[, c("drug", "moa_broad", "moa_subgroup", 
  "dataset")]
colnames(mode_of_action) <- c("drugname_typaslab", "process_broad", 
  "process_subgroup", "dataset")
mode_of_action <- mode_of_action[1:153, ]
table(mode_of_action$process_subgroup)
```


```{r}
(mc_ext_dfm_subgr <- 
  select(mc_ext_dfm, -process_broad) %>%
  left_join(mode_of_action[, c("drugname_typaslab", "process_subgroup")]) %>%
  select(drugname_typaslab, conc, process_subgroup, everything()))

# conditions
table(mc_ext_dfm_subgr$process_subgroup)
# considering only distinct drugs
mc_ext_dfm_subgr %>%
  select(drugname_typaslab, conc, process_subgroup) %>%
  distinct(drugname_typaslab, .keep_all = TRUE) %$%
  table(process_subgroup)

resampled_mcl_loo_subg <- 
  fit_multiclass_rf_loo(data = mc_ext_dfm_subgr, targetvar = "process_subgroup", 
  blockingvar = "drugname_typaslab", paramlist = list(ntree = my_ntree, 
    mtry = my_mtry), cols_to_exclude = c("drugname_typaslab", "conc"))

saveRDS(resampled_mcl_loo_subg, "./data/programmatic_output/resampled_mcl_loo_subg.rds")
```


```{r}
resampled_mcl_loo_subg$aggr

plot_wide_confmat(get_wide_confmat(resampled_mcl_loo_subg), 
  title = "Multiclass RF, LOO, subgroups")

ggsave("./plots/Confusion_matrix_subgroups.pdf")
```


# Integrate other data sets

## Applying model to Lucia's data (newsize)

### Data import

```{r}
# get already preprocessed data: 
sscores <- readRDS("./data/programmatic_output/kept_datasets_wide_featsndrugs_removed_noNAs_featsmerged.rds")
newsize <- sscores$newsize.sscores
nichols <- sscores$nichols_2011.sscores

# keep only features that are in the_matrix and then sort them in the same way
newsize <- newsize[, colnames(newsize) %in% colnames(mc_ext_dfm)]
nichols <- nichols[, colnames(nichols) %in% colnames(mc_ext_dfm)]

newsize <- 
  left_join(newsize, mode_of_action[, c("drugname_typaslab", "process_broad")]) %>%
  select(drugname_typaslab, conc, process_broad, everything()) %>% 
  select(colnames(mc_ext_dfm))

nichols <- 
  left_join(nichols, mode_of_action[, c("drugname_typaslab", "process_broad")]) %>%
  select(drugname_typaslab, conc, process_broad, everything()) %>% 
  select(colnames(mc_ext_dfm))
```

### Generating subsets of the data

From Lucia's data, extract: (i) drugs with unknown MoA (to be predicted later), 
(ii) "validation set 2": with overlapping drugs, only point is to see if 
biological variability might screw up the predictions, (iii) test set, only 
annotated drugs from Lucia's data, use to assess accuracy, confusion matrix. 

```{r}
# extract unknown drugs for prediction later on
unknown <- filter(newsize, process_broad == "unknown")
(unknown <- bind_rows(unknown, 
  filter(nichols, process_broad == "unknown")))

# extract main moas for broad classification
(newsize_main <- filter(newsize, process_broad %in% moas))

# extract overlapping drugs: "validation set 2"
(valset2 <- semi_join(newsize_main, mc_ext_dfm, 
  by = "drugname_typaslab"))

# test set: labelled drugs in Lucia's data
(testset <- anti_join(newsize_main, mc_ext_dfm, 
  by = "drugname_typaslab"))

stopifnot(nrow(testset) + nrow(valset2) == nrow(newsize_main))

(shared_drugs <- unique(valset2$drugname_typaslab))
```

Retrain Nichols model on the complete data set.

```{r}
retr_nichols <- mc_ext_dfm
retr_nichols$process_broad %<>% factor()
my_rle <- rle(retr_nichols$drugname_typaslab)
my_blocks <- factor(rep(seq_along(my_rle$lengths), my_rle$lengths))

retr_nichols_tsk <- makeClassifTask(data = retr_nichols[, -c(1, 2)], 
  target = "process_broad", blocking = my_blocks)

retr_nichols_lrn <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = my_ntree, mtry = my_mtry))

retr_nichols_model <- train(learner = retr_nichols_lrn, task = retr_nichols_tsk)

getLearnerModel(retr_nichols_model)

sort(unlist(getFeatureImportance(retr_nichols_model)$res), decreasing = TRUE) %>% 
  head(n = 20)
```

Predict with retrained model: validation set.

```{r}
(valset2_preds <- predict(retr_nichols_model, newdata = valset2))

valset2_preds$data <- as_tibble(cbind(valset2[, 1:2], 
  as.data.frame(valset2_preds)))
# View(valset2_preds$data)

# Compare with original model:
performance(valset2_preds, measures = mmce)
resampled_mcl_loo$aggr

# actually, one would need to compare the performance on the same subset:
resampled_mcl_loo$pred$data %>%
  filter(drugname_typaslab %in% valset2$drugname_typaslab) %>%
  summarise(mmce = 1 - (sum(response == truth) / n()))

valset2_preds_melt <- melt_pred_data(valset2_preds, "multiclass")
plot_mcl_probs_heatmap(valset2_preds_melt, mics = mics)

ggsave(filename = "./plots/Prediction_heatmap_valset2.pdf", plot = p, 
  width = 15, height = 6)
```

Predict with retrained model: test set.

```{r}
perfs <- vector("numeric", length = 10)

for (i in 1:length(perfs)) {
  retr_nichols_model <- train(learner = retr_nichols_lrn, task = retr_nichols_tsk)
  (valset2_preds <- predict(retr_nichols_model, newdata = valset2))
  perfs[i] <- performance(valset2_preds, measures = mmce)
}

retr_nichols_model <- train(learner = retr_nichols_lrn, task = retr_nichols_tsk)
(testset_preds <- predict(retr_nichols_model, newdata = testset))
performance(testset_preds, measures = mmce)
resampled_mcl_loo$aggr

testset_preds$data <- as_tibble(cbind(testset[, 1:2], 
  as.data.frame(testset_preds)))
View(testset_preds$data)

melt_pred_data(testset_preds, "multiclass") %>%
  plot_mcl_probs_heatmap(mics = mics) %>% 
  ggsave(filename = "./plots/Prediction_heatmap_testset.pdf", width = 15, 
  height = 10)
```

Predict with retrained model: unknown drugs.

```{r}
(unknown_preds <- predict(retr_nichols_model, newdata = unknown))

unknown_preds$data <- as_tibble(cbind(unknown[, 1:2], 
  as.data.frame(unknown_preds)))
View(unknown_preds$data)

melt_pred_data(unknown_preds, "multiclass") %>%
  plot_mcl_probs_heatmap(mics = mics) %>% 
  ggsave(filename = "./plots/Prediction_heatmap_unknown.pdf", width = 9, 
  height = 6.5)
```






















<!-------------------------------------------------------------->
<!-- rewrite - same as above - put into function --> 
<!-------------------------------------------------------------->

```{r}
# get already preprocessed data: 
sscores <- readRDS("./data/programmatic_output/kept_datasets_wide_featsndrugs_removed_noNAs_featsmerged.rds")
newsize <- sscores$newsize.sscores
nichols <- sscores$nichols_2011.sscores

# keep only features that are in the_matrix and then sort them in the same way
newsize <- newsize[, colnames(newsize) %in% c("drugname_typaslab", "conc", "process_broad", my_fingerprint_mcl)]
nichols <- nichols[, colnames(nichols) %in% c("drugname_typaslab", "conc", "process_broad", my_fingerprint_mcl)]

### NB! ------------------------
mc_ext_dfm_red <- mc_ext_dfm[, c("drugname_typaslab", "conc", "process_broad", my_fingerprint_mcl)]
### NB! ------------------------

newsize <- 
  left_join(newsize, mode_of_action[, c("drugname_typaslab", "process_broad")]) %>%
  select(drugname_typaslab, conc, process_broad, everything())

nichols <- 
  left_join(nichols, mode_of_action[, c("drugname_typaslab", "process_broad")]) %>%
  select(drugname_typaslab, conc, process_broad, everything())
```

### Generating subsets of the data

From Lucia's data, extract: (i) drugs with unknown MoA (to be predicted later), 
(ii) "validation set 2": with overlapping drugs, only point is to see if 
biological variability might screw up the predictions, (iii) test set, only 
annotated drugs from Lucia's data, use to assess accuracy, confusion matrix. 

```{r}
# extract unknown drugs for prediction later on
unknown <- filter(newsize, process_broad == "unknown")
(unknown <- bind_rows(unknown, 
  filter(nichols, process_broad == "unknown")))

# extract main moas for broad classification
(newsize_main <- filter(newsize, process_broad %in% moas))

# extract overlapping drugs: "validation set 2"
(valset2 <- semi_join(newsize_main, mc_ext_dfm, 
  by = "drugname_typaslab"))

# test set: labelled drugs in Lucia's data
(testset <- anti_join(newsize_main, mc_ext_dfm, 
  by = "drugname_typaslab"))

stopifnot(nrow(testset) + nrow(valset2) == nrow(newsize_main))

(shared_drugs <- unique(valset2$drugname_typaslab))
```

Retrain Nichols model on the complete data set.

```{r}
retr_nichols <- mc_ext_dfm_red
retr_nichols$process_broad %<>% factor()
my_rle <- rle(retr_nichols$drugname_typaslab)
my_blocks <- factor(rep(seq_along(my_rle$lengths), my_rle$lengths))

retr_nichols_tsk <- makeClassifTask(data = retr_nichols[, -c(1, 2)], 
  target = "process_broad", blocking = my_blocks)

# need to use default for mtry
retr_nichols_lrn <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = my_ntree))

retr_nichols_model <- train(learner = retr_nichols_lrn, task = retr_nichols_tsk)

getLearnerModel(retr_nichols_model)
```

Predict with retrained model: validation set.

```{r}
perfs <- vector("numeric", length = 10)

for (i in 1:length(perfs)) {
  retr_nichols_model <- train(learner = retr_nichols_lrn, task = retr_nichols_tsk)
  (testset_preds <- predict(retr_nichols_model, newdata = testset))
  perfs[i] <- performance(testset_preds, measures = mmce)
}

(valset2_preds <- predict(retr_nichols_model, newdata = valset2))

valset2_preds$data <- as_tibble(cbind(valset2[, 1:2], 
  as.data.frame(valset2_preds)))
#View(valset2_preds$data)

# Compare with original model:
performance(valset2_preds, measures = mmce)
resampled_mcl_loo$aggr

# actually, one would need to compare the performance on the same subset:
resampled_mcl_loo$pred$data %>%
  filter(drugname_typaslab %in% valset2$drugname_typaslab) %>%
  summarise(mmce = 1 - (sum(response == truth) / n()))

valset2_preds_melt <- melt_pred_data(valset2_preds, "multiclass")
plot_mcl_probs_heatmap(valset2_preds_melt, mics = mics)

ggsave(filename = "./plots/Prediction_heatmap_valset2_parsimonious.pdf",  
  width = 15, height = 6)
```

Predict with retrained model: test set.

```{r}
(testset_preds <- predict(retr_nichols_model, newdata = testset))
performance(testset_preds, measures = mmce)
resampled_mcl_loo$aggr

testset_preds$data <- as_tibble(cbind(testset[, 1:2], 
  as.data.frame(testset_preds)))
#View(testset_preds$data)

melt_pred_data(testset_preds, "multiclass") %>%
  plot_mcl_probs_heatmap(mics = mics) %>% 
  ggsave(filename = "./plots/Prediction_heatmap_testset_parsimonious.pdf", width = 15, 
  height = 10)
```

Predict with retrained model: unknown drugs.

```{r}
(unknown_preds <- predict(retr_nichols_model, newdata = unknown))

unknown_preds$data <- as_tibble(cbind(unknown[, 1:2], 
  as.data.frame(unknown_preds)))
#View(unknown_preds$data)

melt_pred_data(unknown_preds, "multiclass") %>%
  plot_mcl_probs_heatmap(mics = mics) %>% 
  ggsave(filename = "./plots/Prediction_heatmap_unknown_parsimonious.pdf", width = 15, 
  height = 10)
```

<!-------------------------------------------------------------->
<!-------------------------------------------------------------->




















```{r, eval = FALSE}
# Retrain 'old' model but only with non-overlapping drugs
# training in mlr: call train() on a learner + suitable task
retrain_nichols <- mc_ext_dfm
retrain_nichols$process_broad %<>% factor()
my_rle <- rle(retrain_nichols$drugname_typaslab)
my_blocks <- factor(rep(seq_along(my_rle$lengths), my_rle$lengths))

retrain_task <- makeClassifTask(data = retrain_nichols[, -c(1, 2)], 
  target = "process_broad", blocking = my_blocks)

retrain_learner <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = my_ntree, mtry = my_mtry))

retrained_model <- train(learner = retrain_learner, task = retrain_task)

getLearnerModel(retrained_model)

sort(unlist(getFeatureImportance(retrained_model)$res), decreasing = TRUE) %>% 
  head(n = 20)
```

Predict with retrained model.

```{r, eval = FALSE}
(newsize_preds <- predict(retrained_model, newdata = newsize_overlap))
performance(newsize_preds, measures = mmce)

newsize_preds$data <- as_tibble(cbind(newsize_overlap[, 1:2], 
  as.data.frame(newsize_preds)))
View(newsize_preds$data)
```

Comparing old with new predictions.

```{r, eval = FALSE}
newsize_preds_melt <- melt_pred_data(newsize_preds, "multiclass")
p <- plot_mcl_probs_heatmap(newsize_preds_melt, mics = mics)

ggsave(filename = "./plots/Prediction_heatmap_newsize.pdf", plot = p, 
  width = 15, height = 6)

# how does it compare? 
compare_preds <- 
  left_join(newsize_preds_melt, multicl_pred_melt, 
    by = c("drugname_typaslab", "conc", "predicted_prob", "truth"), 
    suffix = c(".newsize", ".nichols")) %>% 
    select(drugname_typaslab, conc, predicted_prob, truth,  
      prob.med.nichols, prob.med.newsize) %>% 
    arrange(drugname_typaslab)

ggplot(compare_preds, aes(x = prob.med.nichols, y = prob.med.newsize)) + 
  geom_point() + 
  facet_wrap( ~ predicted_prob) + 
  geom_abline(linetype = "dotted") + 
  xlim(0, 1) + ylim(0, 1)
```

Fraction of predictions that agree:

```{r, eval = FALSE}
agreement <- 
  group_by(compare_preds, drugname_typaslab, conc) %>% 
  summarise(agree = which.max(prob.med.nichols) == 
      which.max(prob.med.newsize)) %>% 
  print(n = 25)

sum(agreement$agree) / nrow(agreement)
```

Can also define differently: if both models predict the MoA wrong, they still 
'agree' (in the sense that both are wrong):

```{r, eval = FALSE}
agreement2 <- 
  mutate(compare_preds, truth = as.character(truth), predicted_prob = 
      str_extract(predicted_prob, 
        pattern = "cell_wall|dna|membrane_stress|protein_synthesis")) %>%
  group_by(drugname_typaslab, conc) %>%
  summarise(agree = (which.max(prob.med.nichols) == which.max(prob.med.newsize)) | 
      (!any(predicted_prob[c(which.max(prob.med.nichols), 
        which.max(prob.med.newsize))] %in% truth))) %>%
  print(n = 25)

sum(agreement2$agree) / nrow(agreement2)
```


## Integrate Lucia's data and refit stuff

First for the four main modes:

```{r, eval = FALSE}
# rbind nichols and newsize but only add new conditions
stopifnot({
  ncol(nichols) == ncol(newsize)
  all(colnames(nichols) == colnames(newsize))
  })

nichols_tomerge <- nichols 
nichols_tomerge$source <- "NICHOLS"

newsize_tomerge <- newsize
newsize_tomerge$source <- "NEWSIZE"

(merged <-  
  newsize_tomerge %>% 
  anti_join(nichols_tomerge, by = c("drugname_typaslab", "conc")) %>% 
  bind_rows(nichols_tomerge) %>% 
  select(drugname_typaslab, conc, process_broad, source, everything()) %>% 
  group_by(drugname_typaslab) %>%
  top_n(2, conc) %>%
  arrange(drugname_typaslab, conc) %>% 
  ungroup())

merged_mainmodes <- filter(merged, process_broad %in% moas)

table(merged_mainmodes$process_broad)
# number of additional drugs
nunique(merged_mainmodes$drugname_typaslab) - nunique(mc_ext_dfm$drugname_typaslab)
View(merged_mainmodes[merged_mainmodes$drugname_typaslab %in% shared_drugs, c(1:10)])
```

```{r, eval = FALSE}
resampled_mcl_merged <- fit_multiclass_rf_loo(data = merged_mainmodes, 
  targetvar = "process_broad", blockingvar = "drugname_typaslab", 
  paramlist = list(ntree = my_ntree, mtry = my_mtry), 
  cols_to_exclude = c("drugname_typaslab", "conc", "source"))

saveRDS(resampled_mcl_merged, 
  file = "./data/programmatic_output/resampled_mcl_merged.rds")
```

Results:

```{r, eval = FALSE}
resampled_mcl_merged$aggr

plot_wide_confmat(get_wide_confmat(resampled_mcl_merged), 
  title = "Multiclass RF, LOO, Nichols + Newsize")

map(moas, plot_roc_mcl, resampled_multiclass = resampled_mcl_merged)

resampled_mcl_merged_melt <- melt_pred_data(resampled_mcl_merged, "multiclass")
p <- plot_mcl_probs_heatmap(resampled_mcl_merged_melt, mics = mics)

ggsave(filename = "./plots/Prediction_heatmap_integrated.pdf", plot = p, 
  width = 15, height = 20)
```

And now for subgroups:

```{r, eval = FALSE}
table(merged$process_broad)

merged_subg <- merged
merged_subg$process_broad <- NULL
merged_subg <- left_join(merged_subg, mode_of_action[, c("drugname_typaslab", 
  "process_subgroup")]) %>% 
  select(drugname_typaslab, conc, process_subgroup, everything()) %>%
  filter(!(process_subgroup %in% c("unknown", "rna", "protein_qc"))) # not enough obs

# number of observations per subgroup:
count(merged_subg, process_subgroup)

# number of drugs per subgroup:
group_by(merged_subg, process_subgroup) %>% 
  summarise(ndrugs = n_distinct(drugname_typaslab))

resampled_mcl_merged_subg <- fit_multiclass_rf_loo(data = merged_subg, 
  targetvar = "process_subgroup", blockingvar = "drugname_typaslab", 
  paramlist = list(ntree = my_ntree, mtry = my_mtry), 
  cols_to_exclude = c("drugname_typaslab", "conc", "source"))

saveRDS(resampled_mcl_merged_subg, 
  file = "./data/programmatic_output/resampled_mcl_merged_subg.rds")
```


```{r, eval = FALSE}
resampled_mcl_merged_subg$aggr

plot_wide_confmat(get_wide_confmat(resampled_mcl_merged_subg), 
  title = "Multiclass RF, subgroups, LOO, Nichols + Newsize")

# interestingly, folate and inner have good ROC curves even though the 
# confusion matrix looks bad

plot_roc_mcl(resampled_mcl_merged_subg, positive = "inner")
plot_roc_mcl(resampled_mcl_merged_subg, positive = "folate")
# whereas, for example:
plot_roc_mcl(resampled_mcl_merged_subg, positive = "ox_stress")

# let's check: 
as.data.frame(resampled_mcl_merged_subg$pred) %>% 
  filter(truth %in% c("inner", "folate", "ox_stress")) %>% 
  print(n = 100)
```


Predicting unknown drugs:

```{r, eval = FALSE}
# Retrain 'old' model but only with non-overlapping drugs
# training in mlr: call train() on a learner + suitable task
production_data <- merged_mainmodes
production_data$process_broad %<>% factor()
my_rle <- rle(production_data$drugname_typaslab)
my_blocks <- factor(rep(seq_along(my_rle$lengths), my_rle$lengths))

production_task <- makeClassifTask(data = production_data[, -c(1, 2, 4)], 
  target = "process_broad", blocking = my_blocks)

production_learner <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = my_ntree, mtry = my_mtry))

production_model <- train(learner = production_learner, task = production_task)

getLearnerModel(production_model)

sort(unlist(getFeatureImportance(production_model)$res), decreasing = TRUE) %>% 
  head(n = 20)
```

Predict with retrained model.

```{r, eval = FALSE}
(unknown_data <- filter(merged, process_broad == "unknown"))
(unknown_preds <- predict(production_model, newdata = unknown_data))
performance(unknown_preds, measures = mmce)

unknown_preds$data <- as_tibble(cbind(unknown_data[, c(1, 2, 4)], 
  as.data.frame(unknown_preds)))
View(unknown_preds$data)
```


# Session info 

```{r session_info}
R.version
sessionInfo()
```

