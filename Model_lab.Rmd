---
title: "Model lab: current 'production' models and experiments to improve them"
author: "Florian Huber"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

# Setup, library loading

```{r setup}
rm(list = ls())
# function to check if a package is installed, if so, load it, else install and then load it
source("./R/ipak.R")
# set chunk options and load libraries
source("./setup.R")
knitr::opts_chunk$set(message = FALSE, cache = TRUE)

matrix_container_ext <- readRDS("./data/programmatic_output/matrix_container_ext.rds")
moa_cols <- c("#e66101", "#fdb863", "#b2abd2", "#5e3c99")
moas <- c("cell_wall", "dna", "membrane_stress", "protein_synthesis")
my_moa <- "cell_wall"
datadir <- "./run_results_from_server/matrix_container_result"
```

# Section 1: Current production models + data

## Current 1-vs-rest 'production' model

To recapitulate from "Comparing_performances.Rmd": random forests was better than lasso because of 
model stabilities. Top25pct of features seems to have worked best. Only for protein synthesis the 
lasso seems to have done fairly well.

```{r}
(the_line <- which(matrix_container_ext$fitted_model == "classif.randomForest" & 
    matrix_container_ext$feat_preselect == "top25pct" & 
    ! matrix_container_ext$chemical_feats &
    matrix_container_ext$drug_dosages == "all"))
stopifnot(length(the_line) == 1)

sub_matrix_container <- matrix_container_ext[the_line, ]
my_data <- matrix_container_ext$drug_feature_matrices[[the_line]]
(unique(sub_matrix_container$drug_feature_matrices[[1]]$process_broad))

plot_perf_from_container(sub_matrix_container)
plot_perf_from_container(sub_matrix_container, what = "prec-recall")

# AUCs:
sub_matrix_container %>%
  select(feat_preselect, fitted_model, perf_measures) %>%
  unnest() %>% 
  group_by(fitted_model, feat_preselect, moa_modelled) %>%
  summarise(mean_auc = mean(auc))

# retrieve our results object:
chosen_res <- readRDS(file.path(datadir, make_filename(matrix_container_ext[the_line, ])))
```


## Retrieving most commonly used hyperparameters

Will be used for future model runs so that we don't have to do hyperparameter tuning anymore. 

```{r}
# first try to figure out which values were used for mtry and ntree in our favourite result
# we can get the optimal hyperparameters using getLearnerModel():
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)

# to access directly:
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)$learner.model$ntree
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)$learner.model$mtry

# hyperparameters that were tested
matrix_container_ext$hyperparam_grid[[the_line]]

# dimensions of the drug_feature_matrix:
dim(matrix_container_ext$drug_feature_matrices[[the_line]])

# which parameters were used/'won'?
(ntrees <- map_dbl(chosen_res, ~ getLearnerModel(.x[["Outer fold 1"]][["model_dna"]])$learner.model$ntree))

# for ntree it seems like only 200 or 500 trees were used:
all_ntrees <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, param = "ntree", 
  simplify = FALSE, USE.NAMES = TRUE)
lapply(all_ntrees, table)

all_mtrys <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, param = "mtry", 
  simplify = FALSE, USE.NAMES = TRUE)

lapply(all_mtrys, table)
```

`dna` and `cell_wall` clearly favour mtry = 106, membrane_stress and protein_synthesis: less clear, 
but the tendency there is more towards 200. Let's settle for 150, which is around a third of the 
observations, like it is recommended for regression trees in Kuhn et al. Moreover, we will use 
ntree = 1000 and mtry = 150. 

```{r}
my_ntree <- 1000
my_mtry <- 150
```


## Currently used multi-class model

### Training/loading the model

Get resampling instance from Untracked.Rmd.

```{r}
rep_rin <- readRDS("./data/programmatic_output/rep_rin_empirical.rds")

# we need to remove all but the 4 main modes of action:
my_data_multiclass <- filter(my_data, process_broad %in% moas)

length(unique(my_data_multiclass$drugname_typaslab))
unique(my_data_multiclass$process_broad)

(my_data_multiclass_train <- select(my_data_multiclass, -one_of(c("conc", "drugname_typaslab"))))
(rf_multi.task <- makeClassifTask(data = my_data_multiclass_train, target = "process_broad"))
(rf_multi.learner <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = my_ntree, mtry = my_mtry)))

my_filename <- "./data/programmatic_output/resampled_multiclass.rds"
if (file.exists(my_filename)) {
  resampled_multiclass <- readRDS(my_filename)
} else {
  resampled_multiclass <- resample(learner = rf_multi.learner, task = rf_multi.task, 
    measures = mmce, resampling = rep_rin, models = TRUE, keep.pred = TRUE)
  # add drugname and concentration information to $pred$data slot
  resampled_multiclass$pred$data <- 
    bind_cols(my_data_multiclass[resampled_multiclass$pred$data$id, c(1, 2)], 
      resampled_multiclass$pred$data) %>%
    arrange(drugname_typaslab, conc, iter)
  saveRDS(resampled_multiclass, file = my_filename)
}
```


### Performance and predictions of the model

#### Some metrics

```{r}
resampled_multiclass$aggr
```

So the mmce is ~ 1/3. With cell-wall model above it was ~ 7.5%. But not clear what kind of 
misclassification error we would get with our five models. Would we get more errors from 
aggregation?

#### Confusion matrix

```{r}
(cm <- get_wide_confmat(resampled_multiclass))
plot_wide_confmat(cm)
```

#### Investigating predictions

For example, the predictions + IQRs for A22:

```{r}
multicl_pred <- resampled_multiclass$pred$data
(multicl_pred_melt <- melt_pred_data(resampled_multiclass, model_type = "multiclass"))

filter(multicl_pred_melt, drugname_typaslab == "A22") %>% 
  ggplot(aes(x = factor(conc), y = prob.med, fill = predicted_prob)) + 
  geom_pointrange(aes(ymin = prob.min, ymax = prob.max, colour = predicted_prob), alpha = 0.75) + 
  geom_line(aes(group = predicted_prob, colour = predicted_prob)) + 
  facet_wrap( ~ truth) + 
  labs(x = "Concentration", y = "Prediction probability", title = "A22, multi-class prediction")
```

And now for all drugs:

```{r}
# may also want to show this as a heatmap, compare fig. 11.4 in Kuhn book
p <- ggplot(multicl_pred_melt, aes(x = factor(conc), y = prob.med, colour = predicted_prob)) + 
   geom_pointrange(aes(ymin = prob.min, ymax = prob.max), alpha = 0.75, size = 2, fatten = 1) + 
   geom_line(aes(group = predicted_prob), size = 1) + 
   facet_wrap( ~ truth + drugname_typaslab, scales = "free_x") + 
   geom_hline(yintercept = c(0.25, 0.5, 0.75), linetype = "dotted") + 
   scale_colour_manual("Predicted probability\nfor class", 
                       labels = c("Cell wall", "DNA", "Membrane stress", "Protein synthesis"), 
                       values = c("#1b9e77", "#d95f02", "#7570b3", "#e7298a"))

suppressMessages(ggsave(filename = "./plots/Multiclass_probabilities.pdf", plot = p, width = 30, 
  height = 20))
```

<a href="./plots/Multiclass_probabilities.pdf">link</a>

An attempt to characterise model quality of the multi-class model: 'winner' approach. One can see 
that this improves accuracy by about 10 percent - but probably should compare this to random 
guessing.

```{r}
# one simple approach: the highest median probability across all dosages wins
# then calculate the accuracy:
(winners <- 
    group_by(multicl_pred_melt, drugname_typaslab, truth) %>%
    arrange(desc(prob.med), .by_group = TRUE) %>%
    slice(1) %>%
    mutate(response = str_extract(predicted_prob, 
      pattern = "cell_wall|dna|membrane_stress|protein_synthesis")))

# we should compare this to random guessing
mean(winners$response == winners$truth)
mean(winners$response != winners$truth)
```

Notably, stuff that is predicted badly is the same as in the previous models. 

```{r}
winners[winners$response != winners$truth, ]
```

#### Probability calibration plot

From Kuhn, chapter 11, p294ff.: probability calibration plot. Should also compare with our original 
1-vs-others approach. 

```{r}
# 10 bins with probabilities from 0 - 100, compare with actually observed frequencies
# one plot per MoA (?) - in Kuhn book: only use this for binary classification
plot_prob_calib(resampled_multiclass)
ggsave("./plots/probability_calibration.pdf")
```


## Multi-class model performance is similar to several 1-vs-all models

### Comparing multi-class probabilities with 1-vs-rest probabilities

For each mode of action: compare probabilities for each observation returned by either 
(i) multi-class model or (ii) the respective 1-vs-all model.

```{r}
(onevsall_pred_melt <- melt_pred_data(matrix_container_ext$pred_data[[the_line]]))

prob_comparison <- compare_probabilities(multicl_pred_melt, onevsall_pred_melt, 
  title = "Comparing probabilities of standard one-vs-rest\nand multi-class model")
prob_comparison$plot
prob_comparison$spearman_corrs

# # btw, it also works for 2 onevsall models:
# foo <- onevsall_pred_melt
# compare_probabilities(onevsall_pred_melt, foo)$plot
```

<!--
### Comparing feature importances
-->

```{r, echo = FALSE, eval = FALSE}
getFeatureImportance(resampled_multiclass$models[[1]])

feat_imps_multi <- imap_dfr(resampled_multiclass$models, function(val, pos) {
  f <- getFeatureImportance(val)$res
  f <- tibble(gene = names(f), importance = unlist(f), modelnum = pos)
})

head(feat_imps_multi)
```

<!--
In that case we don't have any features that only occur in some models. Perhaps because we are 
using more trees? So in that case, one histogram is enough
-->

```{r, echo = FALSE, eval = FALSE}
all(rle(sort(feat_imps_multi$gene))$lengths == 80)
colnames(my_data_multiclass)[! colnames(my_data_multiclass) %in% feat_imps_multi$gene]

# so we can easily summarise
feat_imps_multi <- 
  group_by(feat_imps_multi, gene) %>%
  summarise(median_importance = median(importance))

ggplot(feat_imps_multi, aes(x = log2(median_importance))) + 
  geom_histogram()

fingerprint1_multi <- feat_imps_multi$gene[log2(feat_imps_multi$median_importance) > -2.5]
fingerprint2_multi <- feat_imps_multi$gene[log2(feat_imps_multi$median_importance) > 0]

# compare with fingerprints from 1-vs-all models
fingerprints_onevsall <- readRDS("./data/programmatic_output/fingerprints_1vsall.rds")

library(VennDiagram)
venn.plot <- venn.diagram(
  list("fingerprint1_1vsall" = fingerprints_onevsall$fingerprint1, 
    "fingerprint1_multi" = fingerprint1_multi), 
  imagetype = "png", 
  "./plots/fingerprint1.png"
)

venn.plot <- venn.diagram(
  list("fingerprint2_1vsall" = fingerprints_onevsall$fingerprint2, 
    "fingerprint2_multi" = fingerprint2_multi), 
  imagetype = "png", 
  "./plots/fingerprint2.png"
)

feat_imps_nest <- fingerprints_onevsall$feat_imps_nest
# alternatively, we can plot a correlation:
tmp <- left_join(feat_imps_multi, feat_imps_nest[, c("moa", "gene", "median_importance")], 
  by = "gene", suffix = c("_multi", "_1vsall"))

ggplot(tmp, aes(x = log2(median_importance_1vsall), y = log2(median_importance_multi))) + 
  geom_point() + 
  facet_wrap( ~ moa) + 
  labs(title = "Comparison of feature importances 1-vs-all and multi-class model")

by(data = tmp, tmp$moa, function(data) {
  cor(x = data$median_importance_1vsall, y = data$median_importance_multi, method = "spearman")
})
```


<!--
## Comparing with baseline models

### KNN

Has certain advantages: less black and white; assumption matches intuition that similar MoA = close 
in chemical genomics space. Will fail however when other features are included. 

```{r, echo = FALSE, eval = FALSE}
getLearnerParamSet("classif.knn")

(knn_params <- makeParamSet(
   makeDiscreteParam("k", values = 1:25)
))
class(knn_params)

(ctrl <- makeTuneControlGrid())
class(ctrl)

knn.learner <- makeLearner("classif.knn")

# rf.task was with cell_wall only
res_knn <- tuneParams(learner = knn.learner, task = rf.task, resampling = rep_rin, 
  measures = list(mmce), par.set = knn_params, control = ctrl)

res_knn
res_knn$x # this is a bit higher than for the random forests model above:
r2_correct$aggr 


data <- generateHyperParsEffectData(res_knn)
plotHyperParsEffect(data, x = "k", y = "mmce.test.mean", plot.type = "line")
```
-->


# Section 2: Improving models

First we need a quick and informative way of comparing models with each other. In the sections 
above we have a set of 1-vs-rest and one multi-model 'production' model. Both are likely to change 
because of data processing steps and certain things we wanna try to improve. 


```{r}
# so first let's get on the same page with which models we are dealing with:
# View(resampled_multiclass)
# View(chosen_res)
pryr::object_size(resampled_multiclass)
pryr::object_size(chosen_res)
# - quite unwieldy, about 0.5 and 2.1 GB, respectively 

class(resampled_multiclass)
class(chosen_res)

# we can now compare two one-vs-rest models --------------------------------
model1 <- matrix_container_ext[9, ]
model2 <- matrix_container_ext[10, ]

compare_onevsrest_models(model1, model2, file = "./plots/compare_one-vs-rest.pdf")

# we only have one multiclass model so far, let's use it twice:
model1_mc <- resampled_multiclass
model2_mc <- resampled_multiclass

compare_multiclass_models(model1_mc, model2_mc, file = "./plots/compare_multiclass.pdf")
```




# Session info 

```{r session_info}
R.version
sessionInfo()
```

