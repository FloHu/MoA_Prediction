---
title: "Model lab: current 'production' models and experiments to improve them"
author: "Florian Huber"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

# Setup, library loading

```{r setup, cache = FALSE}
rm(list = ls())
# function to check if a package is installed, if so, load it, else install and then load it
source("./R/ipak.R")
# set chunk options and load libraries
source("./setup.R")
knitr::opts_chunk$set(message = FALSE, cache = TRUE)

matrix_container_ext <- readRDS("./data/programmatic_output/matrix_container_ext.rds")
moa_cols <- c("#e66101", "#fdb863", "#b2abd2", "#5e3c99")
moas <- c("cell_wall", "dna", "membrane_stress", "protein_synthesis")
my_moa <- "cell_wall"
datadir <- "./run_results_from_server/matrix_container_result"
```

# Section 1: Current production models + data

## Current 1-vs-rest 'production' model

To recapitulate from "Comparing_performances.Rmd": random forests was better 
than lasso because of model stabilities. Top25pct of features seems to have 
worked best. Only for protein synthesis the lasso seems to have done fairly 
well.

```{r}
(the_line <- which(matrix_container_ext$fitted_model == "classif.randomForest" & 
    matrix_container_ext$feat_preselect == "top25pct" & 
    !matrix_container_ext$chemical_feats &
    matrix_container_ext$drug_dosages == "all"))
stopifnot(length(the_line) == 1)

sub_matrix_container <- matrix_container_ext[the_line, ]
my_data <- matrix_container_ext$drug_feature_matrices[[the_line]]
(unique(sub_matrix_container$drug_feature_matrices[[1]]$process_broad))

plot_perf_from_container(sub_matrix_container)
plot_perf_from_container(sub_matrix_container, what = "prec-recall")

# AUCs:
sub_matrix_container %>%
  select(feat_preselect, fitted_model, perf_measures) %>%
  unnest() %>% 
  group_by(fitted_model, feat_preselect, moa_modelled) %>%
  summarise(mean_auc = mean(auc))

# retrieve our results object:
chosen_res <- readRDS(file.path(datadir, 
  make_filename(matrix_container_ext[the_line, ])))
```


## Retrieving most commonly used hyperparameters

Will be used for future model runs so that we don't have to do hyperparameter 
tuning anymore. 

```{r}
# first try to figure out which values were used for mtry and ntree in our 
# favourite result
# we can get the optimal hyperparameters using getLearnerModel():
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)

# to access directly:
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)$learner.model$ntree
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)$learner.model$mtry

# hyperparameters that were tested
matrix_container_ext$hyperparam_grid[[the_line]]

# dimensions of the drug_feature_matrix:
dim(matrix_container_ext$drug_feature_matrices[[the_line]])

# which parameters were used/'won'?
(ntrees <- map_dbl(chosen_res, ~ getLearnerModel(.x[["Outer fold 1"]][["model_dna"]])$learner.model$ntree))

# for ntree it seems like only 200 or 500 trees were used:
all_ntrees <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, param = "ntree", 
  simplify = FALSE, USE.NAMES = TRUE)
lapply(all_ntrees, table)

all_mtrys <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, param = "mtry", 
  simplify = FALSE, USE.NAMES = TRUE)

lapply(all_mtrys, table)
```

`dna` and `cell_wall` clearly favour mtry = 106, membrane_stress and protein_synthesis: less clear, 
but the tendency there is more towards 200. Let's settle for 150, which is around a third of the 
observations, like it is recommended for regression trees in Kuhn et al. Moreover, we will use 
ntree = 1000 and mtry = 150. 

```{r}
my_ntree <- 1000
my_mtry <- 150
```


## Currently used multi-class model

### Training/loading the model

Get resampling instance from Untracked.Rmd.

```{r}
# we need to remove all but the 4 main modes of action:
my_data_mcl <- filter(my_data, process_broad %in% moas)
length(unique(my_data_mcl$drugname_typaslab))
unique(my_data_mcl$process_broad)

my_data_mcl$process_broad <- factor(my_data_mcl$process_broad)

# for blocking
my_rle <- rle(my_data_mcl$drugname_typaslab)
my_blocks <- factor(rep(seq_along(my_rle$lengths), my_rle$lengths))

(my_data_mcl_task <- select(my_data_mcl, -one_of(c("conc", "drugname_typaslab"))))

(rf_multi.task <- makeClassifTask(data = my_data_mcl_task, target = "process_broad", 
  blocking = my_blocks))

(rf_multi.learner <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = my_ntree, mtry = my_mtry)))

# we need a resampling instance:
rep_rin_mcl <- make_rep_ncv(task_data_all_cols = my_data_mcl, 
  mlr_task = rf_multi.task, reps = 10, folds = 8, strat_var = "process_broad")

my_filename <- "./data/programmatic_output/resampled_multiclass.rds"
if (file.exists(my_filename)) {
  resampled_multiclass <- readRDS(my_filename)
} else {
  resampled_multiclass <- fit_multiclass_rf(learner = rf_multi.learner, 
    task = rf_multi.task, measures = mmce, rin = rep_rin_mcl, 
    task_data_all_cols = my_data_mcl)
  saveRDS(resampled_multiclass, file = my_filename)
}
```


### Performance and predictions of the model

#### Some metrics

```{r}
resampled_multiclass$aggr
```

So the mmce is ~ 1/3. With cell-wall model above it was ~ 7.5%. But not clear 
what kind of misclassification error we would get with our five models. Would 
we get more errors from aggregation?

#### Confusion matrix

```{r}
(cm <- get_wide_confmat(resampled_multiclass))
plot_wide_confmat(cm, title = "'Naive' multiclass model")
```

#### ROC-curve for multi-class models

##### A few reflections

This can only be defined for a mode-of-action - not-mode-of-action pair. 
Otherwise, how should "true negative" be defined - this only makes sense if one 
class is defined as positive. The following chunks demonstrates the problem:

```{r}
# see also below
head(resampled_multiclass$pred$data)
(mcl_melt <- melt_pred_data(resampled_multiclass, model_type = "multiclass"))
mcl_melt$predicted_prob <- str_replace(string = mcl_melt$predicted_prob, 
  pattern = "prob\\.", replacement = "")
mcl_melt$truth <- as.character(mcl_melt$truth)

# average concentrations
(mcl_melt_concavg <- 
  mcl_melt %>%
  select(drugname_typaslab, conc, predicted_prob, prob.med, truth) %>%
  group_by(drugname_typaslab, truth, predicted_prob) %>%
  summarise(prob.med_avg = mean(prob.med)))

# note: truth = actual class label
(mcl_melt_roc_input <- 
  mcl_melt_concavg %>%
  group_by(drugname_typaslab) %>%
  mutate(highest_vs_2nd = prob.med_avg - max(prob.med_avg[-which.max(prob.med_avg)])) %>%
  top_n(1, prob.med_avg) %>%
  rename(class_highest_p = predicted_prob, highest_p = prob.med_avg))
```

Now consider what happens when we decide on a threshold at which we call an 
observation to belong to the class that has the highest probability, e.g. 0.2:

```{r}
mcl_melt_roc_input$response <- 
  ifelse(mcl_melt_roc_input$highest_vs_2nd >= 0.2, 
    mcl_melt_roc_input$class_highest_p, 
    paste0("not_", mcl_melt_roc_input$class_highest_p))
head(mcl_melt_roc_input)
```

Obviously, such an output is useless for TP, FN etc. rate calculations. 

So instead we write a function that defines one class as positive and the 
others as negative.

```{r}
get_responses_mcl <- function(dfr, positive, thresh) {
  # positive = MoA that is considered positive
  dfr$positive <- positive
  dfr$thresh <- thresh
  negative <- paste0("not_", positive)
  dfr$truth <- ifelse(dfr$truth == positive, positive, negative)
  # give answer according to class_highest_p
  dfr$response <- ifelse(dfr$highest_vs_2nd >= thresh, dfr$class_highest_p, 
    paste0("not_", dfr$class_highest_p))
  # now it gets philosophical: based on this routine an observation might be 
  # "not_membrane_stress" while the positive class is "cell_wall". Strictly 
  # speaking it's not clear if this is true negative or even true positive. 
  # But we might argue that "not_membrane_stress" would also mean "not_" for 
  # any other class - so we can still classify it as true negative. 
  #### TO DO: discuss this or read up on this
  dfr$response <- ifelse(dfr$response == positive, positive, negative)
  dfr$tp <- (dfr$response == dfr$truth) & (dfr$truth == positive)
  dfr$fn <- (dfr$response != dfr$truth) & (dfr$truth == positive)
  dfr$fp <- (dfr$response != dfr$truth) & (dfr$truth != positive)
  dfr$tn <- (dfr$response == dfr$truth) & (dfr$truth != positive)
  stopifnot(sum(unlist(dfr[, c("tp", "fn", "fp", "tn")])) == nrow(dfr))
  return(dfr)
}

(tmp <- get_responses_mcl(mcl_melt_roc_input, positive = "cell_wall", thresh = 0.2))

get_metrics_mcl <- function(dfr) {
  stopifnot(length(unique(dfr$thresh)) == 1)
  dfr <- group_by(dfr, thresh, positive) %>%
    summarise(tp = sum(tp), fn = sum(fn), tn = sum(tn), fp = sum(fp), 
      tpr = tp / (tp + fn), fpr = fp / (fp + tn), ppv = tp / (tp + fp))
  return(dfr)
}

get_metrics_mcl(tmp)

get_thresh_vs_perf_mcl <- function(dfr, positive, thresholds) {
  thresh_vs_perf <- map_dfr(thresholds, function(.thresh) {
    get_metrics_mcl(get_responses_mcl(dfr, positive, .thresh))
  })
  thresh_vs_perf <- arrange(thresh_vs_perf, desc(thresh))
  return(thresh_vs_perf)
}

(from <- range(mcl_melt_roc_input$highest_vs_2nd)[1])
(to <- range(mcl_melt_roc_input$highest_vs_2nd)[2])
tmp <- get_thresh_vs_perf_mcl(mcl_melt_roc_input, positive = "cell_wall", 
  thresholds = seq(from = from, to = to, length.out = 100))

print(tmp, n = 100)

ggplot(tmp, aes(x = fpr, y = tpr)) + 
  geom_path(aes(group = 1)) + 
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1))
```

What happened? The issue is that we use `highest_vs_2nd` as the metric to put a 
threshold on. This works fine at the beginning: we have a high confidence 
(i.e. high `highest_vs_2nd` levels) so the tpr goes up without compromising the 
fpr. However, when the threshold approaches 0 there are quite a few 
observations that never "get the chance" to be put into the positive class. 
Why? Because their probability for the positive class (`cell_wall` in this 
case) is lower than the 2nd highest probability so they are just all classified 
as negative cases. Therefore, neither the TPR nor the FPR can ever go up again. 
Conclusion: `highest_vs_2nd` is not a sensible metric. 


##### Better version

Go back to simply putting a threshold on the probabilities themselves.

```{r}
# just to demonstrate the different steps contained in plot_roc_mcl()
source("./R/ROC_mcl_models.R") # don't remove this line!
(tmp <- prep_roc(resampled_multiclass, positive = "cell_wall"))
(tmp <- get_responses_mcl(tmp, positive = "cell_wall", thresh = 0.5))
(tmp <- get_metrics_mcl(tmp))

(tmp <- get_thresh_vs_perf_mcl(resampled_multiclass, positive = "cell_wall"))
print(tmp, n = 20)

for (moa in moas) {
  print(plot_roc_mcl(resampled_multiclass, positive = moa))
}
```


#### Investigating predictions

For example, the predictions + IQRs for A22:

```{r}
(multicl_pred_melt <- melt_pred_data(resampled_multiclass, model_type = "multiclass"))

filter(multicl_pred_melt, drugname_typaslab == "A22") %>% 
  ggplot(aes(x = factor(conc), y = prob.med, fill = predicted_prob)) + 
  geom_pointrange(aes(ymin = prob.min, ymax = prob.max, colour = predicted_prob), alpha = 0.75) + 
  geom_line(aes(group = predicted_prob, colour = predicted_prob)) + 
  facet_wrap( ~ truth) + 
  labs(x = "Concentration", y = "Prediction probability", title = "A22, multi-class prediction")
```

And now for all drugs:

```{r}
# may also want to show this as a heatmap, compare fig. 11.4 in Kuhn book
p <- ggplot(multicl_pred_melt, aes(x = factor(conc), y = prob.med, colour = predicted_prob)) + 
  geom_pointrange(aes(ymin = prob.min, ymax = prob.max), alpha = 0.75, size = 2, fatten = 1) + 
  geom_line(aes(group = predicted_prob), size = 1) + 
  facet_wrap( ~ truth + drugname_typaslab, scales = "free_x") + 
  geom_hline(yintercept = c(0.5), linetype = c("solid")) + 
  scale_colour_manual("Predicted probability\nfor class", 
    labels = c("Cell wall", "DNA", "Membrane stress", "Protein synthesis"), 
    values = c("#1b9e77", "#d95f02", "#7570b3", "#e7298a")) + 
  coord_cartesian(ylim = c(0, 1))

suppressMessages(ggsave(filename = "./plots/Multiclass_probabilities.pdf", plot = p, width = 30, 
  height = 20))
```

<a href="./plots/Multiclass_probabilities.pdf">LINK FOR THE PLOT</a>

An attempt to characterise model quality of the multi-class model: 'winner' 
approach. One can see that this improves accuracy by about 10 percent - but 
probably should compare this to random guessing.

```{r}
# one simple approach: the highest median probability across all dosages wins
# then calculate the accuracy:
(winners <- 
    group_by(multicl_pred_melt, drugname_typaslab, truth) %>%
    arrange(desc(prob.med), .by_group = TRUE) %>%
    slice(1) %>%
    mutate(response = str_extract(predicted_prob, 
      pattern = "cell_wall|dna|membrane_stress|protein_synthesis")))

# we should compare this to random guessing
mean(winners$response == winners$truth)
mean(winners$response != winners$truth)
```

Notably, stuff that is predicted badly is the same as in the previous models. 

```{r}
winners[winners$response != winners$truth, ]
```

#### Probability calibration plot

From Kuhn, chapter 11, p294ff.: probability calibration plot. Should also 
compare with our original 1-vs-others approach. 

```{r}
# 10 bins with probabilities from 0 - 100, compare with actually observed frequencies
# one plot per MoA (?) - in Kuhn book: only use this for binary classification
plot_prob_calib(resampled_multiclass, title = "'naive' multi-class model")
ggsave("./plots/probability_calibration.pdf")
```


## Multi-class model performance is similar to several 1-vs-all models

### Comparing multi-class probabilities with 1-vs-rest probabilities

For each mode of action: compare probabilities for each observation returned by either 
(i) multi-class model or (ii) the respective 1-vs-all model.

```{r}
(onevsall_pred_melt <- melt_pred_data(matrix_container_ext$pred_data[[the_line]], 
  model_type = "onevsrest"))

prob_comparison <- compare_probabilities(
  list("'naive' multiclass" = multicl_pred_melt, "'naive' 1-vs-rest" = onevsall_pred_melt))
prob_comparison$plot
prob_comparison$spearman_corrs
```

### Comparing multi-class feature importances with 1-vs-rest feature importances

Note that the importance is not too high: which is perhaps not too surprising 
because some features will be considered importance in the multi-class model 
but might be considered important in only *one* 1-vs-rest model.

```{r}
(feat_imps_multi <- summarise_feat_imps(get_feat_imps(resampled_multiclass)))
(feat_imps_1vsrest <- summarise_feat_imps(get_feat_imps(chosen_res)))

(tmp <- left_join(feat_imps_multi, feat_imps_1vsrest[, c("moa", "gene", "median_importance")], 
  by = "gene", suffix = c("_multi", "_1vsall")))

ggplot(tmp, aes(x = log2(median_importance_1vsall), y = log2(median_importance_multi))) + 
  geom_point() + 
  facet_wrap( ~ moa) + 
  labs(title = "Comparison of feature importances 1-vs-all and multi-class model")
```

<!--
## Comparing with baseline models

### KNN

Has certain advantages: less black and white; assumption matches intuition that similar MoA = close 
in chemical genomics space. Will fail however when other features are included. 

```{r, echo = FALSE, eval = FALSE}
getLearnerParamSet("classif.knn")

(knn_params <- makeParamSet(
   makeDiscreteParam("k", values = 1:25)
))
class(knn_params)

(ctrl <- makeTuneControlGrid())
class(ctrl)

knn.learner <- makeLearner("classif.knn")

# rf.task was with cell_wall only
res_knn <- tuneParams(learner = knn.learner, task = rf.task, resampling = rep_rin, 
  measures = list(mmce), par.set = knn_params, control = ctrl)

res_knn
res_knn$x # this is a bit higher than for the random forests model above:
r2_correct$aggr 


data <- generateHyperParsEffectData(res_knn)
plotHyperParsEffect(data, x = "k", y = "mmce.test.mean", plot.type = "line")
```
-->


# Section 2: Improving models

First we need a quick and informative way of comparing models with each other. 
In the sections above we have a set of 1-vs-rest and one multi-model 
'production' model. Both are likely to change because of data processing steps 
and certain things we wanna try to improve. 

So first we want to have a good way of comparing models.


## Demonstration of how models can be compared

```{r}
# so first let's get on the same page with which models we are dealing with:
# View(resampled_multiclass)
# View(chosen_res)
pryr::object_size(resampled_multiclass)
pryr::object_size(chosen_res)
# - quite unwieldy, about 0.5 and 2.1 GB, respectively 

class(resampled_multiclass)
class(chosen_res)
```

### For a pair of one-vs-rest models

For example, random forests without and with chemical features.

```{r}
model1 <- matrix_container_ext[9, ]
model2 <- matrix_container_ext[10, ]

compare_onevsrest_models(model1 = model1, model2 = model2, 
  model1_title = "RF without chem. feats", 
  model2_title = "RF with chem. feats", 
  file = "./plots/compare_one-vs-rest.pdf")
```


### For a pair of multi-class models

Just for examples sake we compare here the same models with each other.

```{r}
# we only have one multiclass model so far, let's use it twice:
model1_mc <- resampled_multiclass
model2_mc <- resampled_multiclass

compare_multiclass_models(model1_mc, model2_mc, 
  model1_mc_title = "a", model2_mc_title = "b", 
  file = "./plots/compare_multiclass_test.pdf")
```


## Trying to improve classification


### Checking MICs

Looking at the MICs does support the idea that the closer a drug dosage is to 
the MIC the more likely it is to get a high probability for the correct class. 
It could be interesting to plot on top of that the number of significant 
interactions per drug-dosage combination. Could be useful to only keep the 2 
dosages which are closest to the MIC. Weighting? 

```{r, check_mics}
# # already run:
# source("./parse_MIC_data.R")
head(multicl_pred_melt)
head(mics <- read_delim("./data/programmatic_output/MICs.csv", delim = ";"))

multicl_pred_melt_micinfo <- left_join(multicl_pred_melt, mics)

# use slice() instead of top_n(), the latter implies some type of ordering!
(helper_frame <- multicl_pred_melt_micinfo %>%
    group_by(drugname_typaslab) %>%
    slice(1) %>%
    mutate(label = as.character(mic_curated)))

helper_frame$label[is.na(helper_frame$label)] <- "NA"
helper_frame$label <- paste0("MIC: ", helper_frame$label, "\n", "resistant: ", 
  ifelse(helper_frame$resistant, "yes", "no"))

# note that hjust = 0 is needed for geom_text() for left justification
# geom_text will respect
p <- 
  multicl_pred_melt_micinfo %>%
  #filter(drugname_typaslab %in% c("NOVOBIOCIN", "BACITRACIN", "AMIKACIN")) %>%
  ggplot(aes(x = factor(conc), y = prob.med, colour = predicted_prob)) + 
  geom_hline(yintercept = c(0.5), linetype = c("solid")) + 
  geom_pointrange(aes(ymin = prob.min, ymax = prob.max), alpha = 0.75, size = 2, 
    fatten = 1) + 
  geom_line(aes(group = predicted_prob), size = 1) + 
  facet_wrap( ~ truth + drugname_typaslab, scales = "free_x") + 
    geom_text(data = helper_frame, #helper_frame[helper_frame$drugname_typaslab %in% c("AMIKACIN", "NOVOBIOCIN", "BACITRACIN"), ], 
      aes(x = 1, y = 0.9, label = label), hjust = 0, inherit.aes = FALSE) + 
  scale_colour_manual("Predicted probability\nfor class", 
                     labels = c("Cell wall", "DNA", "Membrane stress", 
                       "Protein synthesis"), 
                     values = c("#1b9e77", "#d95f02", "#7570b3", "#e7298a")) + 
  coord_cartesian(ylim = c(0, 1))

suppressMessages(ggsave(filename = "./plots/Multiclass_probabilities_micinfo.pdf", 
  plot = p, width = 30, height = 20))
```

<!--
Moved to DataLoading_FeaturePreselect.Rmd

### Removing/merging (?) collinear features

First we need to select a cutoff. How do pairs correlate: 

* Between different dosages of one drug (within drug correlation)
* Between drugs (between drug correlation)
* Between genes of the same protein complex


```{r, gene_cplx_stuff}
drug_feats <- filter_container(matrix_container_ext, dosgs = "all", feats = "keepall", 
  chemfeats = FALSE, models = "classif.randomForest")$drug_feature_matrices[[1]]

# remove again the stuff that is not 'main' mode of action
feats <- drug_feats[drug_feats$process_broad %in% moas, ]
feats <- feats[, -c(1:3)]

# ----------------------------------------------------------------------------------------
# NOTE: will remove here already features with many NA values - will be removed at an earlier 
# stage in later runs (see also notebook "Inspecting_the_matrix.Rmd"):
(NA_feats <- readRDS("./data/programmatic_output/feats_to_remove_many_NAs.rds"))
feats <- select(feats, -one_of(NA_feats))
# ----------------------------------------------------------------------------------------

feats_cor <- cor(feats, method = "spearman")

# quite a large and unwieldy plot - but still shows a few useful clusters:
if (!file.exists("./plots/Correlation_all_by_all.png")) {
  png("./plots/Correlation_all_by_all.png", width = 90, height = 90, units = "cm", res = 500)
  corrplot::corrplot(feats_cor, method = "color", order = "hclust", tl.cex = 0.1)
  dev.off()
}

# to get a distribution of the correlations:
# get all possible combinations:

corpairs <- melt_cormat_to_pairs(feats_cor)
arrange(corpairs, desc(cor))

# retrieving all protein complexes, code from notebook 1 (Leonard) -----------------------
cmplx = read_tsv(file = "data/All_instances_of_Protein-Complexes_in_Escherichia_coli_K-12_substr._MG1655.txt")
(load("../dbsetup/data/genesWithEG_ID.RData"))
gene_synonyms$synonym = toupper(gene_synonyms$synonym)

genes_cmplx = lapply(cmplx$`Genes of polypeptide, complex, or RNA`,
        FUN = function(x){
            genenames = str_match_all(string = x, pattern = "[a-zA-Z]{3,4}")
            genenames = toupper(unlist(genenames))
       })

names(genes_cmplx) = cmplx$`Protein-Complexes`
genes_cmplx = genes_cmplx[!is.na(genes_cmplx)]

lens = lapply(genes_cmplx, function(x){length(x)})
genes_cmplx = genes_cmplx[lens != 1]
saveRDS(genes_cmplx, file = "./data/programmatic_output/genes_cmplx.rds")

# add to each pair the information if the two genes are also part of the same protein complex
genes_cmplx_v <- unique(unname(unlist(genes_cmplx)))

corpairs$in_same_cmplx <- 
  map2_lgl(corpairs$featA, corpairs$featB, function(.featA, .featB) {
    drugpair <- c(.featA, .featB)
    if (!all(c(.featA, .featB) %in% genes_cmplx_v)) {
      FALSE
      } else {
        any(map_lgl(genes_cmplx, ~ (sum(.x %in% drugpair) == 2)))
      }
    })
# only 395 pairs
View(corpairs[corpairs$in_same_cmplx, ])

ggplot(corpairs, aes(x = cor)) + 
  geom_histogram(bins = 30) + 
  facet_wrap( ~ in_same_cmplx, scales = "free") + 
  labs(title = "Spearman correlations of all mutant pairs.\nLeft: not part of the same protein 
    complex\nRight: within the same protein complex")

# two selected examples (highest and lowest correlation above cutoff):
ggplot(feats, aes(x = CYOC, y = CYOD)) + 
  geom_point() + 
  labs(title = "S-scores across all conditions (Spearman = 0.9)")

ggplot(feats, aes(x = NUOE, y = NUOJ)) + 
  geom_point() + 
  labs(title = "S-scores across all conditions (Spearman = 0.75)")

ggplot(feats, aes(x = FLIF, y = FLIQ)) + 
  geom_point() + 
  labs(title = "S-scores across all conditions (Spearman = 0.6)")
```

When choosing a cutoff of 0.6 and requiring that genes are in the same complex we get 80 pairs. 
A lot of the genes are part of the NADH:quinone oxidoreductase complex (nuoX genes). Other prominent 
complexes are:

* the terminal oxidase complex (cyoX)
* sulfate adenylyltransferase (cysD, cysN)
* acrAB-tolC 
* ATP synthase (atpX)
* the Tol-Pal system (Pal, TolB, TolR, TolQ)
* TatB-TatC (part of the twin arginine translocation (Tat) complex for the export of folded proteins)
* some flagellum-related genes (FlhA, FLiF, FliQ)

```{r}
# so we'd probably choose a cutoff of 0.6 or so 
View(corpairs[corpairs$in_same_cmplx & corpairs$cor > 0.6, ])
sum(corpairs$in_same_cmplx & corpairs$cor > 0.6)
sum(corpairs$cor > 0.6)
```

If we don't require them to be in the same complex we get `r sum(corpairs$cor > 0.6)` pairs with 
lots of the following: 

* ribonucleotide biosynthesis (pyrX, purX)
* LPS biosynthesis: rfaX
* and many others, see here:

```{r}
View(corpairs[corpairs$cor > 0.6, ])
sum(corpairs$cor > 0.6)
```

Anticorrelation is very rare, only 16 pairs have a Spearman correlation of less than -0.6. 

Notably, Pur and Pyr genes are anticorrelated with RsxB (together with rseC turns off SoxR-mediated 
induction of SoxS), YrbA (= ibaG, defends against acid stress), and UbiE/F (ubiquinone biosynthesis). 
Moreover, GuaB (IMP dehydrogenase) is anticorrelated with LipA (lipoate biosynthesis) and Rph 
(an RNAse). 

```{r}
View(corpairs[corpairs$cor < -0.6, ])
sum(corpairs$cor < -0.6)
```


Now merge correlated predictors. Follow procedure from Kuhn et al, p47. 

```{r}
# removed_correlated_feats <- character()
cor_thresh <- 0.6
continue <- TRUE

# involved data sets:
# 1) feats = all features in a tibble
# 2) feats_cor = correlation matrix derived from 1)
# 3) corpairs = melted version of 2)

my_backup <- list(corpairs = corpairs, feats = feats, feats_cor = feats_cor)
corpairs <- my_backup$corpairs
feats <- my_backup$feats
feats_cor <- my_backup$feats_cor

while (continue) {
  list_of_pairs <- list()
  corpairs <- arrange(corpairs, desc(cor))
  above_thresh <- sum(corpairs$cor > cor_thresh)
  
  for (r in seq_len(above_thresh)) {
    pair <- c(corpairs$featA[r], corpairs$featB[r])
    if (any(pair %in% unlist(list_of_pairs))) {
      # we don't want to go through the whole procedure of merging features for every pair 
      # as long as pairs don't overlap we can merge several pairs at once
      break
    } else {
      list_of_pairs <- c(list_of_pairs, list(pair))
    }
  }
  
  # now merge the feats
  for (el in list_of_pairs) {
    # just to be safe ...
    stopifnot(length(el) == 2)
    newfeat <- paste(el[[1]], el[[2]], sep = "_")
    feats[[newfeat]] <- apply(feats[el], 1, mean)
  }
  
  # and calculate everything again
  feats <- select(feats, -one_of(unlist(list_of_pairs)))
  feats_cor <- cor(feats, method = "spearman")
  corpairs <- melt_cormat_to_pairs(feats_cor)
  
  # if not TRUE means we're done
  continue <- max(corpairs$cor > cor_thresh)
}
```

This means that `r ncol(my_backup$feats) - ncol(feats)` features were merged into other features. 
The newly emerged features are:

```{r}
names(feats)[nchar(names(feats)) > 4]
saveRDS(feats, file = "./data/programmatic_output/feats_after_merging.rds")
```

Also plot the correlation matrix again, let's check:

```{r}
feats_cor <- cor(feats, method = "spearman")

# quite a large and unwieldy plot - but still shows a few useful clusters:
if (!file.exists("./plots/Correlation_all_by_all_aftermerge.png")) {
  png("./plots/Correlation_all_by_all_aftermerge.png", width = 90, height = 90, units = "cm", res = 500)
  corrplot::corrplot(feats_cor, method = "color", order = "hclust", tl.cex = 0.1)
  dev.off()
}
```

-->

## Multi-class model after improved data pre-processing

### No NA values, collinear features merged

_Deprecated since commit bd6a8c4_

<!--
Note that removing the collinear features had virtually no effect on model 
performance. 

Things changed:
1. Features with lots of NA values were removed
2. Collinear features were merged (averaged)
-->

```{r, eval = FALSE, echo = FALSE}
# Taken from chunk further above: 
# rep_rin
# feats: containing the merged features

stopifnot(nrow(my_data_mcl) == nrow(feats))
ncol(my_backup$feats) - ncol(feats)

my_data_mcl_preproc <- my_data_mcl
(drugdata <- my_data_mcl_preproc[, c(1:3)])
# columns that are new due to merging:
(to_add <- feats[, nchar(colnames(feats)) > 4])
# exclude columns that are not in feats: these are the ones that were merged
my_data_mcl_preproc <- my_data_mcl_preproc[, colnames(my_data_mcl_preproc) %in% colnames(feats)]
# add merged features back together
my_data_mcl_preproc <- cbind(drugdata, my_data_mcl_preproc, to_add)

ncol(my_data_mcl)
ncol(my_data_mcl_preproc)

# training the next model: 
my_data_mcl_preproc_task <- select(my_data_mcl_preproc, -one_of(c("conc", "drugname_typaslab")))
rf_multi.task <- makeClassifTask(data = my_data_mcl_preproc_task, target = "process_broad")
rf_multi.learner <- makeLearner("classif.randomForest", predict.type = "prob",
  par.vals = list(ntree = my_ntree, mtry = my_mtry))
```

_Note deprecation note above._

```{r}
my_filename <- "./data/programmatic_output/resampled_multiclass_noNAs_corrs_merged.rds"

if (file.exists(my_filename)) {
  resampled_multiclass_preproc <- readRDS(my_filename)
} else {
  # resampled_multiclass_preproc <- fit_multiclass_rf(learner = rf_multi.learner, 
  #   task = rf_multi.task, measures = mmce, rin = rep_rin_mcl, 
  #   task_data_all_cols = my_data_mcl_preproc)
  # saveRDS(resampled_multiclass_preproc, file = my_filename)
}
```

Comparing this with the previous multi-class model:

```{r}
compare_multiclass_models(model1_mc = resampled_multiclass, model2_mc = resampled_multiclass_preproc, 
  model1_mc_title = "'naive' multiclass model", model2_mc_title = "multiclass model w/o NAs, corr 
  feats merged", file = "./plots/compare_multiclass_noNAs_corrs_merged.pdf")
```


### "Two highest"

_Note: also to be deprecated soon since most of the issues probably resolved after commit bd6a8c4._ 

In addition to previous section: 
  1. Remove drug-dosage combinations with less than a certain number of 
  significant interactions?
  2. Keep just the two highest dosages (?)
  3. Remove some selected drugs (?)

*Concerning (1)*: probably not such a good idea because we would lose a number 
of drugs: doxorubicin, gentamicin, levofloxacin, methotrexate. Only 
methotrexate at 25 should be removed because it has 0 interactions. A lot of 
the other stuff is taken care of by keeping only the two highest dosages. 

```{r, eval = FALSE}
# remove drug-dosage combinations with less than 11 significant interactions 
(n_signif <- matrix_container_ext$pred_data_with_n_signif[[the_line]])

print(filter(n_signif, moa_modelled_is_truth, n_signif < 10) %>%
  arrange(drugname_typaslab), n = 100)

# remove methotrexate, 25, it has no interactions:
nrow(my_data_mcl_preproc)
my_data_mcl_2high <- my_data_mcl_preproc
my_data_mcl_2high <- my_data_mcl_2high[!(my_data_mcl_2high$drugname_typaslab == "METHOTREXATE" & 
    my_data_mcl_2high$conc == 25), ]
```


```{r, eval = FALSE}
# Keeping just the two highest dosages
(my_data_mcl_2high <- 
  group_by(my_data_mcl_2high, drugname_typaslab) %>%
  top_n(2, conc) %>%
  ungroup())

# need new blocking procedure and new resampling instance:
my_data_mcl_2high$process_broad <- factor(my_data_mcl_2high$process_broad)

# for blocking
my_blocks <- make_blocks(my_data_mcl_2high, blockvar = "drugname_typaslab")

(my_data_mcl_2high_task <- select(my_data_mcl_2high, -one_of(c("conc", "drugname_typaslab"))))

(rf_multi.task <- makeClassifTask(data = my_data_mcl_2high_task, target = "process_broad", 
  blocking = my_blocks))

(rf_multi.learner <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = my_ntree, mtry = my_mtry)))

# we need a resampling instance:
rep_rin_mcl_2high <- make_rep_ncv(task_data_all_cols = my_data_mcl_2high, 
  mlr_task = rf_multi.task, reps = 10, folds = 8, strat_var = "process_broad")
```

```{r}
my_filename <- "./data/programmatic_output/resampled_multiclass_2high.rds"
if (file.exists(my_filename)) {
  resampled_multiclass_2high <- readRDS(my_filename)
} else {
  # resampled_multiclass_2high <- fit_multiclass_rf(learner = rf_multi.learner, 
  #   task = rf_multi.task, measures = mmce, rin = rep_rin_mcl_2high, 
  #   task_data_all_cols = my_data_mcl_2high)
  # saveRDS(resampled_multiclass_2high, file = my_filename)
}
```

( _see deprecation note above_ ): Comparison with previous multi-class model. 

```{r}
compare_multiclass_models(model1_mc = resampled_multiclass_preproc, 
  model1_mc_title = "Multiclass, no NAs, feats merged", 
  model2_mc = resampled_multiclass_2high, 
  model2_mc_title = "Multiclass, no NAs, feats merged, 2 highest dosg", 
  "./plots/compare_multiclass_merged_vs_2highest.pdf")
```

Detailed probabilities per drug per dosage. 

```{r}
multicl_pred_melt <- melt_pred_data(resampled_multiclass_2high, model_type = "multiclass")

p <- ggplot(multicl_pred_melt, aes(x = factor(conc), y = prob.med, colour = predicted_prob)) + 
  geom_pointrange(aes(ymin = prob.min, ymax = prob.max), alpha = 0.75, size = 2, fatten = 1) + 
  geom_line(aes(group = predicted_prob), size = 1) + 
  facet_wrap( ~ truth + drugname_typaslab, scales = "free_x") + 
  geom_hline(yintercept = c(0.5), linetype = c("solid")) + 
  scale_colour_manual("Predicted probability\nfor class", 
    labels = c("Cell wall", "DNA", "Membrane stress", "Protein synthesis"), 
    values = c("#1b9e77", "#d95f02", "#7570b3", "#e7298a")) + 
  coord_cartesian(ylim = c(0, 1))

suppressMessages(ggsave(filename = "./plots/Multiclass_probabilities_2high.pdf", plot = p, 
  width = 30, height = 20))
```


### Comparison with LOOCV

For predicting MoA subgroups we may have to switch to LOOCV. 

```{r, loocv}
# use my_data_mcl_2high from previous chunk to compare with LOOCV:
mcl_loo <- my_data_mcl
mcl_loo
my_blocks <- make_blocks(mcl_loo, blockvar = "drugname_typaslab")
# to check that the blocks are still correct: split(mcl_loo[, c(1:3)], my_blocks)

mcl_loo_task_data <- select(mcl_loo, -one_of(c("conc", "drugname_typaslab")))
mcl_loo_task <- makeClassifTask(data = mcl_loo_task_data, 
  target = "process_broad", blocking = my_blocks)

mcl_loo_learner <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = my_ntree, mtry = my_mtry))

rdesc <- makeResampleDesc("LOO")

resampled_mcl_loo <- resample(learner = mcl_loo_learner, task = mcl_loo_task, 
  resampling = rdesc, measures = mmce, models = TRUE)

# don't forget to update data frame
resampled_mcl_loo$pred$data <- 
    dplyr::bind_cols(mcl_loo[resampled_mcl_loo$pred$data$id, c("conc", "drugname_typaslab")], 
      resampled_mcl_loo$pred$data) %>%
    dplyr::arrange(drugname_typaslab, conc, iter)

saveRDS(resampled_mcl_loo, file = "./data/programmatic_output/resampled_mcl_loo.rds")

compare_multiclass_models(model1_mc = resampled_multiclass, 
  model1_mc_title = "'Naive' multiclass, rep nest CV", 
  model2_mc = resampled_mcl_loo, 
  model2_mc_title = "'Naive' multiclass, LOOCV", 
  "./plots/compare_multiclass_loocv.pdf")

# to get ROC curves - look almost the same :)
plot_roc_mcl(resampled_multiclass, positive = "cell_wall")
plot_roc_mcl(resampled_mcl_loo, "cell_wall")
```


```{r, loocv}
# # use my_data_mcl_2high from previous chunk to compare with LOOCV:
# mcl_loo <- my_data_mcl_2high
# mcl_loo
# # to check that the blocks are still correct: split(mcl_loocv[, c(1:3)], my_blocks)
# 
# mcl_loo_task_data <- select(mcl_loo, -one_of(c("conc", "drugname_typaslab")))
# mcl_loo_task <- makeClassifTask(data = mcl_loo_task_data, 
#   target = "process_broad", blocking = my_blocks)
# 
# mcl_loo_learner <- makeLearner("classif.randomForest", predict.type = "prob", 
#   par.vals = list(ntree = my_ntree, mtry = my_mtry))
# 
# rdesc <- makeResampleDesc("LOO")
# 
# resampled_mcl_loo <- resample(learner = mcl_loo_learner, task = mcl_loo_task, 
#   resampling = rdesc, measures = mmce, models = TRUE)
# 
# # don't forget to update data frame
# resampled_mcl_loo$pred$data <- 
#     dplyr::bind_cols(mcl_loo[resampled_mcl_loo$pred$data$id, c("conc", "drugname_typaslab")], 
#       resampled_mcl_loo$pred$data) %>%
#     dplyr::arrange(drugname_typaslab, conc, iter)
# 
# compare_multiclass_models(model1_mc = resampled_multiclass_2high, 
#   model1_mc_title = "Multiclass, no NAs, feats merged, 2 highest dosg", 
#   model2_mc = resampled_mcl_loo, 
#   model2_mc_title = "Multiclass, no NAs, feats merged, 2 highest dosg, LOOCV", 
#   "./plots/compare_multiclass_loocv.pdf")
# 
# # to get ROC curves - look almost the same :)
# plot_roc_mcl(resampled_multiclass_2high, positive = "cell_wall")
# plot_roc_mcl(resampled_mcl_loo, "cell_wall")
```


# Quick comparison of '2019' data frame with the 'naive' one. 

```{r}
# reminder: mcl_loo is derived from my_data_mcl (--> the input to the 'naive' 
# model), is based on keepall and top 25 pct
mcl_loo

# so to compare with the 2019 matrix_container:
matrix_container_2019 <- readRDS("./data/programmatic_output/matrix_container_2019.rds")

(the_line <- with(matrix_container_2019, 
  which(fitted_model == "classif.randomForest" & feat_preselect == "top25pct" & 
    !chemical_feats & drug_dosages == "all")))
stopifnot(length(the_line) == 1)

sub_matrix_container_2019 <- matrix_container_2019[the_line, ]
(my_data_mcl_2019 <- sub_matrix_container_2019$drug_feature_matrices[[1]])
my_data_mcl_2019$process_broad <- factor(my_data_mcl_2019$process_broad)

# already the dimensions are quite different:
dim(mcl_loo)
unique(mcl_loo$process_broad)
# 3 more observations because of EGCG
dim(my_data_mcl_2019)
unique(my_data_mcl_2019$process_broad)

# NEW MODEL FITTING: LOOCV for 2019 data -------------------------
my_blocks <- make_blocks(my_data_mcl_2019, blockvar = "drugname_typaslab")
# to check that the blocks are still correct: split(mcl_loo[, c(1:3)], my_blocks)

mcl_2019_task_data <- select(my_data_mcl_2019, -one_of(c("conc", "drugname_typaslab")))
mcl_2019_task <- makeClassifTask(data = mcl_2019_task_data, 
  target = "process_broad", blocking = my_blocks)

mcl_2019_learner <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = my_ntree, mtry = my_mtry))

rdesc <- makeResampleDesc("LOO")

resampled_mcl_2019 <- resample(learner = mcl_2019_learner, task = mcl_2019_task, 
  resampling = rdesc, measures = mmce, models = TRUE)

# don't forget to update data frame
resampled_mcl_2019$pred$data <- 
    dplyr::bind_cols(my_data_mcl_2019[resampled_mcl_2019$pred$data$id, c("conc", "drugname_typaslab")], 
      resampled_mcl_2019$pred$data) %>%
    dplyr::arrange(drugname_typaslab, conc, iter)

saveRDS(resampled_mcl_2019, file = "./data/programmatic_output/resampled_mcl_2019.rds")

compare_multiclass_models(model1_mc = resampled_mcl_loo, 
  model1_mc_title = "'Naive' multiclass, LOOCV", 
  model2_mc = resampled_mcl_2019, 
  model2_mc_title = "'Naive' multiclass, LOOCV, 2019 data", 
  "./plots/compare_multiclass_loocv-vs-2019.pdf")

# to get ROC curves - look almost the same :)
plot_roc_mcl(resampled_mcl_loo, positive = "cell_wall")
plot_roc_mcl(resampled_mcl_2019, "cell_wall")
```


# Predicting MoA subgroups

<!--
New classification (see also [here](https://docs.google.com/spreadsheets/d/17FAEAbdJ7aHYLzQWiCa0tudZ__lWX5oM6gGJDSEP4Wg/edit#gid=0)):
* cell_wall: unchanged
* protein_synthesis: unchanged
* dna: split into:
  * folate: sulfa drugs, trimethoprim, methotrexate
  * dna: all others
* membrane_stress: (unclear)
* new groups: 
  * ox_stress: paraquat, peroxide, pms, pyocyanin
  
See also this (Google sheet)[https://docs.google.com/spreadsheets/d/17FAEAbdJ7aHYLzQWiCa0tudZ__lWX5oM6gGJDSEP4Wg/edit#gid=0]. 

To sum up: cell_wall - protein_synthesis - dna - folate - detergents - fatty_acid - ox_stress - pmf
-->

```{r, subgroups}
moa_path <- "./data/programmatic_output/drug_moa_gdrive.csv"
moa_tab <- read_delim(moa_path, delim = ",", na = c("-"))
moa_tab <- moa_tab[, c(1, 3, 4)]
names(moa_tab) <- c("drugname_typaslab", "moa", "moa_subgroup")
moa_tab
table(moa_tab$moa_subgroup)

my_subgroups <- c("cell_wall", "dna", "fatty_acid", "folate", "inner", "outer", 
  "ox_stress", "protein_synthesis")
my_subgroups_drugs <- moa_tab[moa_tab$moa_subgroup %in% my_subgroups, ]
table(my_subgroups_drugs$moa_subgroup)
```

So: we know which drugs we want to predict, which means that we now need the 
original data frame which still contains all drugs and all columns. 

```{r}
tm <- readRDS("./data/programmatic_output/the_matrix_complete_nochanges.rds")

tm <- filter(tm, drugname_typaslab %in% my_subgroups_drugs$drugname_typaslab)
tm$sscore[is.na(tm$sscore)] <- 0

tm <- select(tm, gene_synonym, drugname_typaslab, conc, sscore) %>%
  spread(key = gene_synonym, value = sscore)

unique(tm$drugname_typaslab)[!unique(tm$drugname_typaslab) %in% (my_data_mcl_2019$drugname_typaslab)]

# do the feature merging:
(clusters <- readRDS("./data/programmatic_output/clusters.rds"))
tm <- merge_features(tm, clusters, mean)

colnames(my_data_mcl_2019)[!colnames(my_data_mcl_2019) %in% colnames(tm)]

tm <- tm[, colnames(tm) %in% colnames(my_data_mcl_2019)]

tm <- left_join(tm, my_subgroups_drugs[, c("drugname_typaslab", "moa_subgroup")]) %>%
  select(drugname_typaslab, conc, moa_subgroup, everything())

# ACTUAL FITTING ---------------------------------
my_blocks <- make_blocks(tm, blockvar = "drugname_typaslab")
tm_task_data <- select(tm, -one_of(c("conc", "drugname_typaslab")))

tm_task <- makeClassifTask(data = tm_task_data, target = "moa_subgroup", 
  blocking = my_blocks)

tm_learner <- makeLearner("classif.randomForest", predict.type = "prob",
  par.vals = list(ntree = my_ntree, mtry = my_mtry))
rdesc <- makeResampleDesc("LOO")

resampled_tm <- resample(learner = tm_learner, task = tm_task,
  resampling = rdesc, measures = mmce, models = TRUE)

# don't forget to update data frame
resampled_tm$pred$data <-
    dplyr::bind_cols(tm[resampled_tm$pred$data$id, c("conc", "drugname_typaslab")],
      resampled_tm$pred$data) %>%
    dplyr::arrange(drugname_typaslab, conc, iter)

saveRDS(resampled_tm, "./data/programmatic_output/resampled_tm.rds")

plot_wide_confmat(get_wide_confmat(resampled_tm), title = "Multiclass RF, subgroups")
ggsave("./plots/subgroup_prediction.pdf")
```


```{r, subgroups, eval = FALSE, echo = FALSE}
# # use my_data_mcl_2high from previous chunk to compare with LOOCV:
# mcl_subgr <- my_data_mcl_2019
# # # keep 2 highest dosgs
# # mcl_subgr <- group_by(mcl_subgr, drugname_typaslab) %>%
# #   top_n(2, conc) %>%
# #   ungroup()
# # we're missing BILE, UV, GLUFOSFOMYCIN
# moa_tab[!(moa_tab$drug %in% mcl_subgr$drugname_typaslab), ]
# 
# mcl_subgr$process_broad <- moa_tab$moa_subgroup[match(mcl_subgr$drugname_typaslab, moa_tab$drug)]
# # exclude drugs without subclassification: dibucaine, procaine, radicicol, 
# # vancomycin, verapamil:
# mcl_subgr <- mcl_subgr[complete.cases(mcl_subgr), ]
# table(mcl_subgr$process_broad)
# 
# # (now using all dosages, not just 2)
# my_blocks <- make_blocks(mcl_subgr, blockvar = "drugname_typaslab")
# mcl_subgr_task_data <- select(mcl_subgr, -one_of(c("conc", "drugname_typaslab")))
# 
# mcl_subgr_task <- makeClassifTask(data = mcl_subgr_task_data, 
#   target = "process_broad", blocking = my_blocks)
# mcl_subgr_learner <- makeLearner("classif.randomForest", predict.type = "prob", 
#   par.vals = list(ntree = my_ntree, mtry = my_mtry))
# rdesc <- makeResampleDesc("LOO")
# 
# resampled_mcl_subgr <- resample(learner = mcl_subgr_learner, task = mcl_subgr_task, 
#   resampling = rdesc, measures = mmce, models = TRUE)
# 
# # don't forget to update data frame
# resampled_mcl_subgr$pred$data <-
#     dplyr::bind_cols(mcl_subgr[resampled_mcl_subgr$pred$data$id, c("conc", "drugname_typaslab")],
#       resampled_mcl_subgr$pred$data) %>%
#     dplyr::arrange(drugname_typaslab, conc, iter)
# 
# # View(resampled_mcl_subgr$pred$data)
# 
# plot_wide_confmat(get_wide_confmat(resampled_mcl_subgr), title = "Multiclass RF, subgroups")
```


# Session info 

```{r session_info}
R.version
sessionInfo()
```

