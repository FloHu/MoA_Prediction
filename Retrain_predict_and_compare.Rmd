---
title: "Retrain, predict, and compare"
author: "Florian Huber"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    number_sections: yes
    toc: yes
    toc_depth: 2
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

# Setup, library loading

This part is the general setup, whichever model we are using.

```{r setup}
rm(list = ls())
# function to check if a package is installed, if so, load it, else install and then load it
source("./R/ipak.R")
# set chunk options and load libraries
source("./setup.R")
knitr::opts_chunk$set(cache = TRUE, message = FALSE)

theme_set(theme_bw())
```


# Analysis

## Recapitulating results from `matrix_container`

To recapitulate from "Comparing_performances.Rmd": random forests was better than lasso because of 
model stabilities. Top20pct of features seems to have worked best. Only for protein synthesis the 
lasso seems to have done fairly well. Let's have a look at the ROC and precision recall curves 
again.

```{r}
matrix_container_ext <- readRDS("./data/programmatic_output/matrix_container_ext.rds")

# this should become the standard colour code for the MoAs:
moa_cols <- c("#e66101", "#fdb863", "#b2abd2", "#5e3c99")

sub_matrix_container <- 
   filter(matrix_container_ext, fitted_model %in% c("classif.randomForest", "classif.glmnet"), 
     feat_preselect %in% c("top25pct", "keepall"), ! chemical_feats, drug_dosages == "all")

# remember: this still contains all the drugs, also the ones not belonging to 
# one of the main MoAs:
(unique(sub_matrix_container$drug_feature_matrices[[1]]$process_broad))

# almost no difference but 25 pct is more sparse and performs a bit better for membrane_stress
plot_perf_from_container(sub_matrix_container, row_var = "fitted_model", col_var = "feat_preselect")
plot_perf_from_container(sub_matrix_container, what = "prec-recall", row_var = "fitted_model", 
  col_var = "feat_preselect")

# AUCs:
sub_matrix_container %>%
  select(feat_preselect, fitted_model, perf_measures) %>%
  unnest() %>% 
  group_by(fitted_model, feat_preselect, moa_modelled) %>%
  summarise(mean_auc = mean(auc))

# so the current 'winner' is:
chosen_res <- readRDS("./run_results_from_server/matrix_container_result/rf_hyp_param_all_top25pct_FALSE.rds")
```

<!-- TO DO: work on prediction probabilities, model analysis (improve functions)
So if we concentrate now on random forests, all dosages, no chemical features, top25pct variance: 
what prediction probabilities do we get, how are the feature importances distributed? 

More importantly, should this be done on a retrained model? And should we retrain this new model 
with a different feature set, different dosages (perhaps even the reduced feature set?) 
-->

## Prediction probabilities of chosen model

```{r, echo = FALSE, eval = TRUE}
# # plot prediction probabilities, feature importances
# # not sure about heatmap split - is it a good idea? 
# # use Leonard's function
# 
# chosen_res <- readRDS("./run_results_from_server/matrix_container_result/rf_hyp_param_all_top25pct_FALSE.rds")
(the_line <- which(matrix_container_ext$fitted_model == "classif.randomForest" &
    matrix_container_ext$feat_preselect == "top25pct" & ! matrix_container_ext$chemical_feats &
    matrix_container_ext$drug_dosages == "all"))
my_moa <- "cell_wall"

stopifnot(length(the_line) == 1)
# 
# #### TO DO: what does it mean for RF that it's present in x% of models? ####
# # shouldn't we rather care about feature importance? 
# # compare with the bad features 
# # etc. 
# model_analysis(res_obj = chosen_res, matrix_container_line = matrix_container[the_line, ], 
#                moa = my_moa, 
#                model_type = "tree", 
#                feat_imp_thres = 1, # else we get way too many features
#                pdf_filename = "./plots/the_chosen_ model_explanation_cell_wall.pdf")
# 
# drugs_pred_prob_from_container(pred_data = matrix_container[the_line, ]$PredData[[1]], 
#                                moa = my_moa, 
#                                main = "Hello world", xlab = "Probability prediction")
```


<!-- like chunk above: rewrite -->
```{r plot_ROC_from_container_new, echo = FALSE, eval = FALSE}
# pred_data <- matrix_container$PredData[[the_line]]
# thresh_vs_perf_data <- matrix_container$ThreshVsPerfData[[the_line]]
# moa <- "cell_wall"
# # thresh_vs_perf_data <- 
# #    filter(thresh_vs_perf_data, moa_modelled == moa) %>%
# #    group_by(threshold) %>%
# #    summarise(mean_fpr = mean(fpr), mean_tpr = mean(tpr))
# # View(thresh_vs_perf_data)
# # 
# # target_fpr <- 0.1
# # # get threshold which is closest to an fpr of 0.1:
# # thresh_vs_perf_data$diff <- abs(target_fpr - thresh_vs_perf_data$mean_fpr)
# # (thresh <- thresh_vs_perf_data$threshold[which.min(thresh_vs_perf_data$diff)])
# 
# moas <- c("cell_wall", "dna", "membrane_stress", "protein_synthesis")
# 
# pwalk(list(moa = moas, fileprefix = paste0(moas, "_probs")), 
#       plot_prediction_probabilities, 
#       pred_data = pred_data, thresh_vs_perf_data = thresh_vs_perf_data)
```


## Retrieving most commonly used hyperparameters

And try here to find best hyperparameters as this makes later retrainings easier, where we can just 
use the OOB error so we can more easily handle small groups. 

```{r}
# first try to figure out which values were used for mtry and ntree in our favourite result
# we can get the optimal hyperparameters using getLearnerModel():
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)
# to access directly:
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)$learner.model$ntree
getLearnerModel(chosen_res$`Nested CV 1`$`Outer fold 1`$model_dna)$learner.model$mtry

# little reminder of which row we are talking about 
matrix_container_ext[the_line, ]
matrix_container_ext$fitted_model[[the_line]]
matrix_container_ext$hyperparam_grid[[the_line]]
dim(matrix_container_ext$drug_feature_matrices[[the_line]])

# let's see what were typically optimal parameters so we don't have to do nested resampling since 
# we save ourselves the work of tuning - will make the model fittings below much easier
# we have 10 * 8 optimal hyperparameters per MoA
(ntrees <- map_dbl(chosen_res, ~ getLearnerModel(.x[["Outer fold 1"]][["model_dna"]])$learner.model$ntree))

# for ntree it seems like only 200 or 500 trees were used:
moas <- c("cell_wall", "dna", "membrane_stress", "protein_synthesis")
all_ntrees <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, param = "ntree", 
  simplify = FALSE, USE.NAMES = TRUE)
# this is funny - perhaps we should have tried different numbers of trees? 
# anyway, I would see we use 500 from now on - or perhaps even go to 1000
lapply(all_ntrees, table)

all_mtrys <- sapply(moas, extract_params_from_resultsobj, resobj = chosen_res, param = "mtry", 
  simplify = FALSE, USE.NAMES = TRUE)
lapply(all_mtrys, table)
matrix_container_ext$hyperparam_grid[[the_line]]
dim(matrix_container_ext$drug_feature_matrices[[the_line]])

# dna and cell_wall clearly favour 106, membrane_stress and protein_synthesis: less clear, but 
# the tendency there is more towards 200
# let's settle for 150, which is around a third of the observations, like it is recommended for 
# regression trees in Kuhn et al.
```

So now we're ready to retrain models, we will use ntree = 1000 (immune to overfitting) and 
mtry = 150. 


## Playing around with model fitting

Define train and test set.

```{r}
my_ntree <- 1000
my_mtry <- 150
(my_data <- matrix_container_ext$drug_feature_matrices[[the_line]])
length(unique(my_data$drugname_typaslab))
moas

# define my_data for training set and my_data_test for test set

# these we wanna predict later:
my_data_test <- my_data[!my_data$process_broad %in% moas, ]
length(unique(my_data_test$drugname_typaslab))
unique(my_data_test$drugname_typaslab)
# however, a few drugs are missing:
complete_data <- readRDS("./data/programmatic_output/the_matrix_complete_strainsgenesremoved_NAsimputed_wideformat.rds")
drug_info <- readRDS("./data/programmatic_output/drugs_full.rds")
drug_info <- drug_info[, c("drugname_typaslab", "process_broad")]
complete_data <- left_join(complete_data, drug_info)
complete_data <- select(complete_data, drugname_typaslab, conc, process_broad, everything())
complete_data <- complete_data[, colnames(complete_data) %in% colnames(my_data_test)] # filter
complete_data <- complete_data[, colnames(my_data_test)] # sort
my_data_test <- bind_rows(my_data_test, filter(complete_data, drugname_typaslab %in% 
    c("CHLOROPROMAZINE", "EPINEPHRINE", "NOREPINEPHRINE")))
unique(my_data_test$drugname_typaslab)
table(my_data_test$process_broad)

# and with these we wanna learn:
my_data <- my_data[my_data$process_broad %in% moas, ]
# so we are left with 67 drugs
length(unique(my_data$drugname_typaslab))
table(my_data$process_broad)


#### TO DO: FIX already at an earlier stage ------------------------------------------------
# another thing we will do is to exclude genes with lots of NA values
# since NA values here were already imputed, need to specify this by hand
to_exclude <- c("OXYR", "IMP", "ATPC", "GUAB", "YFCA", "PSTA", "GMHB", "RFAP", "LIPA", "YDJI", "FLDA")
to_exclude %in% colnames(my_data)
my_data <- select(my_data, -one_of(to_exclude))
my_data_test <- select(my_data_test, -one_of(to_exclude))
```

Preparing tables for MLR. Try with `cell_wall` only for the time being.

```{r}
# this one we need for prediction:
(my_data_mainmoas <- select(my_data, -one_of(c("conc", "drugname_typaslab")))) # keep drugname information? 
# go for cell_wall first
my_target <- "cell_wall"
my_data_mainmoas$process_broad <- ifelse(my_data_mainmoas$process_broad == my_target, 
                                         my_target, 
                                         paste0("not_", my_target))
```

Making a task and a learner.

```{r}
# same dosages belong together
# can easily done by blocking argument, construct levels using rle() but first make sure that 
# different dosages of the same drug haven't been torn apart
(my_rle <- rle(my_data$drugname_typaslab))
stopifnot(length(unique(my_data$drugname_typaslab)) == length(my_rle$values))
my_blocks <- factor(rep(seq_along(my_rle$values), my_rle$lengths))
levels(my_blocks)

(rf.task <- makeClassifTask(data = my_data_mainmoas, target = "process_broad", positive = my_target, 
  blocking = my_blocks))

(rf.learner <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = my_ntree, mtry = my_mtry)))

# check https://mlr-org.github.io/mlr/articles/cost_sensitive_classif.html
# negative class automatically receives weight 1
# so I assume the positive weight should get this:
# but doesn't change anything - supported by the learner? 
# rf.learn <- makeWeightedClassesWrapper(rf.learner, wcw.weight = pos_weight)
```


Before we go on to resampling let's retrain.

```{r}
(rf.model <- train(rf.learner, rf.task))
class(rf.model)
getLearnerModel(rf.model)
```

We again have this phenomenon of the negative class being favoured - can we fix this using one of 
the strategies dealing with imbalanced classes? 

*Note* that out of bag error is biased because it doesn't respect blocking!

Do resampling. Note that blocking + stratification is not allowed - our 'old' problem and the reason 
why in some splits there is no ROC curve.

```{r}
# doing resampling
(my_measures <- matrix_container_ext$tuning_measure[[the_line]])

rf.rdesc <- makeResampleDesc("CV", iters = 8) # predict = "test" ("both") argument goes here 
rf.resample_result <- resample(learner = rf.learner, task = rf.task, measures = my_measures, 
  resampling = rf.rdesc)

class(rf.resample_result)

rf.resample_result
rf.resample_result$aggr
rf.resample_result$measures.test

getRRPredictions(rf.resample_result)
class(getRRPredictions(rf.resample_result))

# this is what we need
generateThreshVsPerfData(rf.resample_result, measures = list(tpr, fpr, ppv))
plotROCCurves(generateThreshVsPerfData(rf.resample_result, measures = list(fpr, tpr)))
```


### Making resampling instances empirically

On the one hand: blocking + stratification not possible but also don't wanna bother around with 
manually tuning, fitting etc. way out: use instances. 
Simply write a function that tests if an instance is more or less stratified if it is, keep it, 
otherwise try again.

```{r}
# here's how it works in principle (blocking is considered but stratification we will have to do 
# ourselves):
(rin <- makeResampleInstance(rf.rdesc, rf.task))
(rep_rin <- makeResampleInstance(makeResampleDesc(method = "RepCV", reps = 10, folds = 8), rf.task))
## we could pass this to resample() but then we will sometimes not get an auc
# resample(learner = rf.learner, task = rf.task, measures = my_measures, resampling = rin)

rin$train.inds
rin$test.inds
arrange(my_data[rin$train.inds[[1]], ], drugname_typaslab)

# OK so let's try to find good instances
# consider stratification of all classes to accommodate multi-class model
# for subgroups we probably have to change strategies? 
# approach: we already have a repeated CV instance (rep_rin) but will not be properly stratified 
# so what we will do is to keep trying until we find in total 10 good CV instances and then we 
# simply replace the indices

# number of observations per MoA, absolute and relative:
table(my_data$process_broad)
table(my_data$process_broad) / length(my_data$process_broad)

# number of observations per MoA in a particular training/test set split
table(my_data$process_broad[rin$train.inds[[1]]])
table(my_data$process_broad[rin$train.inds[[1]]]) / length(rin$train.inds[[1]])

table(my_data$process_broad[rin$test.inds[[1]]])
table(my_data$process_broad[rin$test.inds[[1]]]) / length(rin$test.inds[[1]])

# actually, it's enough if only the training set is stratified - we just want to be balanced in 
# learning; the test sets are anyway concatenated at the end
# however, to be able to calculate all performance measures we require at least one 
# sample from each class to be in each test fold

(ref_fracs <- table(my_data$process_broad) / length(my_data$process_broad))

cv_instances <- list()
counter <- 0
max_deviance <- 0.05

while (length(cv_instances) < 10) {
   rin <- makeResampleInstance(rf.rdesc, rf.task)
   counter <- counter + 1 # to keep track of how often we had to try :)
   for (split in c(1:8)) {
      train_fracs <- table(my_data$process_broad[rin$train.inds[[split]]]) / length(rin$train.inds[[split]])
      # we don't want any "empty" classes in the test set:
      if (length(table(my_data$process_broad[rin$test.inds[[split]]])) != length(moas)) {
         break
      }
      # and relative fractions of classes shouldn't differ too much in the training set
      if (any(abs(ref_fracs - train_fracs) > max_deviance)) {
         break
      }
      # only happens upon success
      if (split == 8) {
         cat("Success at attempt ", counter, "! - ", date(), " (maximum deviance was ", round(max(abs(ref_fracs - train_fracs)), digits = 3), ")\n")
         cv_instances <- c(cv_instances, list(rin))
      }
   }
}

#  I somehow can't believe it, let's resample with the list:
for (inst in cv_instances[8]) {
   r1 <- resample(learner = rf.learner, task = rf.task, measures = my_measures, resampling = inst)
}

# what happens actually with the original CV instance? 
# interestingly, the repeat of the CV is not encoded
r2 <- resample(learner = rf.learner, task = rf.task, measures = my_measures, resampling = rep_rin, 
  models = TRUE)

# so now let's replace the slots in the repeated CV instance above
my_backup <- list(rep_rin, cv_instances)

rep_rin$train.inds <- flatten(map(cv_instances, "train.inds"))
rep_rin$test.inds <- flatten(map(cv_instances, "test.inds"))

r2_correct <- resample(learner = rf.learner, task = rf.task, measures = my_measures, 
  resampling = rep_rin, models = TRUE, keep.pred = TRUE)

r2_correct$aggr 
# this is very close to our original result - cool!
mean(filter(matrix_container_ext$perf_measures[[the_line]], moa_modelled == "cell_wall")$auc)
```

So now we know that our approach to get a repeated CV instance empirically works. For nested CV one 
would need to do the same thing internally. 

Let's have a bit of a look at the result:

```{r}
r2_correct$pred
getRRPredictions(r2_correct) # same

# this corresponds almost perfectly to the original curves that we got
(r2_correct_threshdata <- generateThreshVsPerfData(r2_correct, measures = list(fpr, tpr, ppv, tnr, mmce)))
plotROCCurves(r2_correct_threshdata)
plotThreshVsPerf(r2_correct_threshdata) # very useful :)
```


### Retraining and predicting left out drugs

```{r}
(retrained_rf <- train(learner = rf.learner, task = rf.task))
getLearnerModel(retrained_rf)

myhead(my_data_test)
pr <- predict(retrained_rf, newdata = my_data_test)
pr

(tmp <- cbind(pr$data, my_data_test[, c("drugname_typaslab", "conc", "process_broad")]))

ggplot(tmp, aes(x = factor(conc), y = prob.cell_wall)) + 
   geom_bar(stat = "identity") + 
   facet_wrap( ~ drugname_typaslab, scales = "free_x")
```

We could now repeat this for all other models ('dna', 'protein_synthesis', ...)


# Training a multi-class model

Using the same new repeated resampling instance as above. Note that most measures won't work 
anymore, could use mmce or something else. 

```{r, training_multiclass}
# just as a reminder
length(unique(my_data$drugname_typaslab))
unique(my_data$process_broad)

(my_data_multiclass <- select(my_data, -one_of(c("conc", "drugname_typaslab"))))

(rf_multi.task <- makeClassifTask(data = my_data_multiclass, target = "process_broad"))
(rf_multi.learner <- makeLearner("classif.randomForest", predict.type = "prob", 
  par.vals = list(ntree = 1000, mtry = 150)))

# most measures won't work anymore - multiclass problem! - take mmce
resampled_multiclass <- resample(learner = rf_multi.learner, task = rf_multi.task, measures = mmce, 
  resampling = rep_rin, models = TRUE, keep.pred = TRUE)
```

So the mmce is ~ 1/3. With cell-wall model above it was ~ 7.5%. But not clear what kind of 
misclassification error we would get with our five models. Would we get more errors from 
aggregation?

Let's have a look at the predictions:

```{r}
# so let's check, for each drug and dosage, the probabilities that it gets
head(resampled_multiclass_pred <- resampled_multiclass$pred$data)

# merge with original information:
resampled_multiclass_pred <- bind_cols(my_data[resampled_multiclass_pred$id, c(1, 2)], 
  resampled_multiclass_pred)
resampled_multiclass_pred <- arrange(resampled_multiclass_pred, drugname_typaslab, iter, conc)
head(resampled_multiclass_pred)
```

For example, the predictions + IQRs for A22:

```{r}
a22 <- filter(resampled_multiclass_pred, drugname_typaslab == "A22")
(a22 <- gather(a22, prob.cell_wall:prob.protein_synthesis, key = "class", value = "probability"))
a22$repetition <- cut(a22$iter, breaks = seq(from = 0.5, by = 8, length.out = 11), 
  labels = as.character(1:10))
(a22 <- 
   group_by(a22, conc, class, truth) %>%
   summarise(ymin = boxplot.stats(probability)$stats[2], 
             ymax = boxplot.stats(probability)$stats[4], 
             ymed = median(probability)))

ggplot(a22, aes(x = factor(conc), y = ymed, fill = class)) + 
  geom_pointrange(aes(ymin = ymin, ymax = ymax, colour = class), alpha = 0.75) + 
  geom_line(aes(group = class, colour = class)) + 
  facet_wrap( ~ truth) + 
  labs(x = "Concentration", y = "Prediction probability", title = "A22, multi-class prediction")
```

And now for all drugs:

```{r}
(multi_melted <- gather(resampled_multiclass_pred, prob.cell_wall:prob.protein_synthesis, 
  key = "class", value = "probability"))
multi_melted$repetition <- cut(multi_melted$iter, breaks = seq(from = 0.5, by = 8, length.out = 11), 
  labels = as.character(1:10))
(multi_melted <- 
   group_by(multi_melted, conc, class, truth, drugname_typaslab) %>%
   summarise(ymin = boxplot.stats(probability)$stats[2], 
             ymax = boxplot.stats(probability)$stats[4], 
             ymed = median(probability)))

# may also want to show this as a heatmap, compare fig. 11.4 in Kuhn book

p <- ggplot(multi_melted, aes(x = factor(conc), y = ymed, colour = class)) + 
   geom_pointrange(aes(ymin = ymin, ymax = ymax), alpha = 0.75, size = 2, fatten = 1) + 
   geom_line(aes(group = class), size = 1) + 
   facet_wrap( ~ truth + drugname_typaslab, scales = "free_x") + 
   geom_hline(yintercept = c(0.25, 0.5, 0.75), linetype = "dotted") + 
   scale_colour_manual("Predicted probability\nfor class", 
                       labels = c("Cell wall", "DNA", "Membrane stress", "Protein synthesis"), 
                       values = c("#1b9e77", "#d95f02", "#7570b3", "#e7298a"))
p
ggsave(filename = "./plots/Multiclass_probabilities.pdf", plot = p, width = 30, height = 20)
```

An attempt to characterise model quality of the multi-class model: 'winner' approach. One can see 
that this improves accuracy by about 10 percent - but probably should compare this to random 
guessing.

```{r}
# one simple approach: the highest median probability across all dosages wins
# then calculate the accuracy:

(winners <- 
    group_by(multi_melted, drugname_typaslab, truth) %>%
    arrange(desc(ymed), .by_group = TRUE) %>%
    slice(1) %>%
    mutate(response = str_extract(class, pattern = "cell_wall|dna|membrane_stress|protein_synthesis")))

# we should compare this to random guessing
mean(winners$response == winners$truth)
mean(winners$response != winners$truth)
```

Notably, stuff that is predicted badly is the same as in the previous models. 

```{r}
winners[winners$response != winners$truth, ]
```

From Kuhn, chapter 11, p294ff.: probability calibration plot. Should also compare with our original 
1-vs-others approach. 

```{r}
# 10 bins with probabilities from 0 - 100, compare with actually observed frequencies
# one plot per MoA (?) - in Kuhn book: only use this for binary classification
prob_calib <- multi_melted
prob_calib$prob_bin <- cut(prob_calib$ymed, breaks = seq(from = 0, to = 1, by = 0.1))
levels(prob_calib$prob_bin)
prob_calib$prob_is_for <- 
   str_extract(prob_calib$class, pattern = "cell_wall|dna|membrane_stress|protein_synthesis")
prob_calib$truth <- as.character(prob_calib$truth)
prob_calib <- 
   group_by(prob_calib, prob_is_for, prob_bin) %>%
   summarise(true_fraction = mean(truth == prob_is_for))
prob_calib

ggplot(prob_calib, aes(x = as.numeric(prob_bin) - 0.05, y = true_fraction)) + 
   geom_point() + 
   geom_line(aes(group = prob_is_for)) + 
   geom_abline(slope = 1/10, linetype = "dotted") + 
   facet_wrap( ~ prob_is_for, ncol = 2) + 
   coord_cartesian(xlim = c(0, 10), ylim = c(0, 1)) + 
   theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
   scale_x_continuous(breaks = seq(from = 0.5, to = 9.5, by = 1), 
                      labels = as.character(10 * seq(from = 0, to = 9, by = 1))) + 
   labs(x = "Probability bin midpoint (10% bins)", y = "Observed event percentage", 
        title = "Probability calibration plot for multi-class RF")

ggsave("./plots/probability_calibration.pdf")
```


<!-- TO DO
proper comparisons
data preprocessing
comparison with KNN
-->

# Comparing with baseline models

## KNN

Has certain advantages: less black and white; assumption matches intuition that similar MoA = close 
in chemical genomics space. Will fail however when other features are included. 

```{r}
getLearnerParamSet("classif.knn")

(knn_params <- makeParamSet(
   makeDiscreteParam("k", values = 1:25)
))
class(knn_params)

(ctrl <- makeTuneControlGrid())
class(ctrl)

knn.learner <- makeLearner("classif.knn")

# rf.task was with cell_wall only
res_knn <- tuneParams(learner = knn.learner, task = rf.task, resampling = rep_rin, 
  measures = list(mmce), par.set = knn_params, control = ctrl)

res_knn
res_knn$x# this is a bit higher than for the random forests model above:
r2_correct$aggr 


data <- generateHyperParsEffectData(res_knn)
plotHyperParsEffect(data, x = "k", y = "mmce.test.mean", plot.type = "line")
```


