---
title: ""
author: "Florian Huber"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

# Setup

```{r setup}
source("./setup.R")
mics <- mics <- read_delim("./data/programmatic_output/MICs.csv", delim = ";")
```

# Retrain models

Retrain model on whole Nichols data according with following setup: random 
forest, all dosages, no chemical features. 

```{r}
# get the data set
mc_ext <- readRDS("./data/programmatic_output/mc_ext.rds")

chosen <- filter(mc_ext, fitted_model == "classif.randomForest", 
  drug_dosages == "all", !chemical_feats)

# overall performance: 70% accuracy 
chosen$perf_measures[[1]] %>%
  group_by(cvrep) %>%
  summarise(mean_mmce = mean(mmce)) %>%
  ggplot(aes(x = "chosen", y = mean_mmce)) + 
    geom_boxplot(outlier.shape = NA) + 
    geom_point(position = position_jitter(width = 0.2, height = 0), shape = 1)

# our input data set
dfm <- chosen$drug_feature_matrices[[1]]

# cvinstance we will use for tuning:
rin <- chosen$resamp_instance[[1]][[1]]
```

## Whole dataset: tuning

Tune with routine of inner loop. Source the functions from fit_one_row.R first. 

```{r}
lrn <- makeLearner(cl = chosen$fitted_model[1], predict.type = "prob")
lrn_wrapped <- makeFilterWrapper(learner = lrn, fw.method = "variance", 
  fw.perc = 0.05)

tsk <- make_my_task(dfm = dfm, blockvar = "drugname_typaslab")

sqrt_p <- floor(sqrt(ncol(dfm)))
rf_grid <- makeParamSet(
  makeDiscreteParam("ntree", values = c(200, 500, 1000)), 
  makeDiscreteParam("mtry", values = c(sqrt_p, sqrt_p + 25, sqrt_p + 50, 
    sqrt_p + 75, sqrt_p + 100)), 
  makeDiscreteParam("fw.perc", values = c(0.25, 0.5, 1))
)

my_file <- "./data/programmatic_output/tuned_params_chosen.rds"
if (!file.exists(my_file)) {
  tuned_params <- tuneParams(learner = lrn_wrapped, task = tsk, resampling = rin, 
    measures = chosen$tuning_measure[[1]], par.set = rf_grid, 
    control = makeTuneControlGrid())
  saveRDS(object = tuned_params, file = my_file)
} else {
  tuned_params <- readRDS(my_file)
}
```

## Parsimonious model: tuning

Tune also fingerprint-based learner. Only tune number of trees

```{r}
lrn_fpt <- makeLearner(cl = chosen$fitted_model[1], predict.type = "prob")

fpt <- readRDS("./data/programmatic_output/fingerprint_mcl.rds")
dfm_fpt <- dfm[, c("drugname_typaslab", "conc", "process_broad", fpt)]

tsk_fpt <- make_my_task(dfm = dfm_fpt, blockvar = "drugname_typaslab")

rf_grid_fpt <- makeParamSet(
  makeDiscreteParam("ntree", values = c(200, 500, 1000))
)

my_file <- "./data/programmatic_output/tuned_params_chosen_fpt.rds"
if (!file.exists(my_file)) {
  tuned_params_fpt <- tuneParams(learner = lrn_fpt, task = tsk_fpt, 
    resampling = rin, measures = chosen$tuning_measure[[1]], 
    par.set = rf_grid_fpt, control = makeTuneControlGrid())
  saveRDS(object = tuned_params_fpt, file = my_file)
} else {
  tuned_params_fpt <- readRDS(my_file)
}
```

## Training tuned learners

```{r}
# set seed? 
# original model
my_file <- "./data/programmatic_output/trained_model.rds"
if (!file.exists(my_file)) {
  tuned_lrn <- setHyperPars(lrn_wrapped, par.vals = tuned_params$x)
  trained_model <- mlr::train(learner = tuned_lrn, task = tsk)
  saveRDS(object = trained_model, file = my_file)
} else {
  trained_model <- readRDS(my_file)
}

# fingerprint model
my_file <- "./data/programmatic_output/trained_model_fpt.rds"
if (!file.exists(my_file)) {
  tuned_lrn_fpt <- setHyperPars(lrn_fpt, par.vals = tuned_params_fpt$x)
  trained_model_fpt <- mlr::train(learner = tuned_lrn_fpt, task = tsk_fpt)
  saveRDS(object = trained_model_fpt, file = my_file)
} else {
  trained_model_fpt <- readRDS(my_file)
}
```


# Predictions with retrained models

## Define drug sets

Predict: (i) test set drugs (Lucia) not present in trained model, (ii) "test" 
set drugs (Lucia) overlapping with trained model, (iii) unknown drugs or drugs 
excluded due to other reasons. 

```{r}
all_data <- readRDS("./data/programmatic_output/kept_datasets_wide_featsndrugs_removed_noNAs_featsmerged.rds")

# TO DO: didn't do "top3" selection here - fix (and in the other notebook it was done too early)
# TO DO: should've perhaps put MoA information already in the dfms
moa <- read_csv("./data/programmatic_output/drug_moa_userfriendly.csv")
all_data <- all_data[c("nichols_2011.sscores", "newsize.sscores")]
all_data <- map(all_data, function(.x) {
  left_join(.x, moa[, c("drugname_typaslab", "process_broad")]) %>%
    select(drugname_typaslab, conc, process_broad, everything())
  })

(nic <- all_data$nichols_2011.sscores)
(new <- all_data$newsize.sscores)

unknown <- map(list(nic, new), ~ filter(.x, process_broad == "unknown")) %>%
  bind_rows()

(nic_to_test <- nic %>%
    filter(!(drugname_typaslab %in% unknown$drugname_typaslab | drugname_typaslab %in% dfm$drugname_typaslab), 
      !(process_broad %in% c("pmf", "ox_stress", "protein_qc"))))

(new_to_test <- new %>%
    filter(!(drugname_typaslab %in% unknown$drugname_typaslab | drugname_typaslab %in% dfm$drugname_typaslab), 
      !(process_broad %in% c("pmf", "ox_stress", "protein_qc"))))
```

## Run predictions: original model

```{r}
# left out drugs from Nichols
nic_to_test_preds <- predict(trained_model, newdata = nic_to_test)
nic_to_test_preds$data <- as_tibble(
  cbind(nic_to_test[, c(1, 2)], as.data.frame(nic_to_test_preds))
  )
nic_to_test_preds
performance(nic_to_test_preds)
nic_to_test_preds$data

# actual test set
new_to_test_preds <- predict(trained_model, newdata = new_to_test)
new_to_test_preds$data <- as_tibble(
  cbind(new_to_test[, c(1, 2)], as.data.frame(new_to_test_preds))
  )
new_to_test_preds
performance(new_to_test_preds)
print(new_to_test_preds$data, n = 50)

ggsave(filename = "./plots/pred_heatmap_testset.pdf", plot = plot_mcl_probs_heatmap(melt_pred_data_mcl(new_to_test_preds$data), mics = mics, printplot = FALSE), width = 12, height = 6)

# unknown drugs
unknown_preds <- predict(trained_model, newdata = unknown)
unknown_preds$data <- as_tibble(
  cbind(unknown[, c(1, 2)], as.data.frame(unknown_preds))
  )
unknown_preds
print(unknown_preds$data, n = 50)

ggsave(filename = "./plots/pred_heatmap_unknown.pdf", plot = plot_mcl_probs_heatmap(melt_pred_data_mcl(unknown_preds$data), mics = mics, printplot = FALSE), width = 12, height = 6)
```

## Run predictions: parsimonious model 

```{r}
# left out drugs from Nichols
nic_to_test_preds_fpt <- predict(trained_model_fpt, newdata = nic_to_test)
nic_to_test_preds_fpt$data <- as_tibble(
  cbind(nic_to_test[, c(1, 2)], as.data.frame(nic_to_test_preds_fpt))
  )
nic_to_test_preds_fpt
performance(nic_to_test_preds_fpt)
nic_to_test_preds_fpt$data

# actual test set
new_to_test_preds_fpt <- predict(trained_model_fpt, newdata = new_to_test)
new_to_test_preds_fpt$data <- as_tibble(
  cbind(new_to_test[, c(1, 2)], as.data.frame(new_to_test_preds_fpt))
  )
new_to_test_preds_fpt
performance(new_to_test_preds_fpt)
print(new_to_test_preds_fpt$data, n = 50)

ggsave(filename = "./plots/pred_heatmap_testset_fpt.pdf", plot = plot_mcl_probs_heatmap(melt_pred_data_mcl(new_to_test_preds_fpt$data), mics = mics, printplot = FALSE), 
  width = 12, height = 6)

# unknown drugs
unknown_preds_fpt <- predict(trained_model_fpt, newdata = unknown)
unknown_preds_fpt$data <- as_tibble(
  cbind(unknown[, c(1, 2)], as.data.frame(unknown_preds_fpt))
  )
unknown_preds_fpt
print(unknown_preds_fpt$data, n = 50)

ggsave(filename = "./plots/pred_heatmap_unknown_fpt.pdf", plot = plot_mcl_probs_heatmap(melt_pred_data_mcl(unknown_preds_fpt$data), mics = mics), 
  width = 12, height = 6)
```

## 

```{r}
## what if we add classweights to the trained model? 
#props <- table(dfm$process_broad) / sum(table(dfm$process_broad))
#(weights <- round(1 / (props/min(props)), digits = 2))

#tsk_dat <- as_tibble(getTaskData(tsk))
#tsk_dat$process_broad <- as.character(tsk_dat$process_broad)

#trained_model_wts <- mlr::train(learner = tuned_lrn, task = tsk)
```



# Session info

```{r session_info}
R.version
sessionInfo()
```
