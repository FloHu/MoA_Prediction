---
title: "Retraining models"
author: "Florian Huber, Leonard Dubois"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    number_sections: yes
    toc: yes
    toc_depth: 2
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

# Setup, library loading

This part is the general setup, whichever model we are using.

```{r setup}
# function to check if a package is installed, if so, load it, else install and then load it
source("./R/ipak.R")
# set chunk options and load libraries
source("./setup.R")
# custom functions
walk(list.files("./R", pattern = "*.R", full.names = T), source)

ipak(plotmo)
ipak(reshape2)
ipak(gplots)
ipak("gridExtra")
ipak("plotly")
library(ComplexHeatmap)
ipak("circlize")
library(glmnet)
library(randomForest)
library(viridis)

# For consensus color map for MoA
colMap = rainbow(4)
names(colMap) = c("cell_wall", "dna", "membrane_stress", "protein_synthesis")

matrix_container_new = readRDS(file = "data/matrix_container_new_withextractions.rds")
```


# Retraining on everything

Aim here is to retrain models on the whole dataset. Feat preselection and algorithm used are the ones displaying the best performances in the previous step of the analysis (see notebook Inspecting Models).
This is the purpose of cross validation (outer loop in the analysis), getting an estimate of a model's performances if retrained on the whole dataset. Knowing such performances, we can choose what's performing best.

Performances of the retrained model cannot be studied since we do not have another dataset whose true response are known.
However, the features importance can be studied and should be easier to use than the previous one. Mostly because there's just one measure of feature importance.

**PS** After running this notebook, it seems that the chemogenomic fingerprint consisting of the genes used in all 80 models in the same run are better to design a nice tSNE space.
I suspect some crowd wisdom effect (something similar to the boosting theory) where aggregation of lots of "weak" models can perform better than a "strong" one (having seen more data)


On top of that, such models can be used to predict MoA of "other" drugs, either the one with a MoA different from the 4 main ones or with an unknown MoA.
On can use the 4 prediction model as a "meta-model". Analyzing all prediction probabilities together as a unique outcome. Some mechanisms of action can indeed lead to ambiguity and kind of ties in the predictions.

The choice of hyperparameter will be made using a new loop of CV (not nested this time) for EN models. RF models hyperparameter will be chosen according to the OOB error.


```{r}
retraining_dna = matrix_container_new[8, ]
retraining_cell_wall = matrix_container_new[8, ]
retraining_protein_synthesis = matrix_container_new[5, ]
retraining_membrane_stress = matrix_container_new[9, ]

matrix_allDrugs = readRDS(file = "data/matrix_2mostia_allDrugs.rds")
matrix_mainDrugs = matrix_container_new[1, ]$drug_feature_matrices[[1]]

drugsToPred = !(matrix_allDrugs$drugname_typaslab %in% matrix_mainDrugs$drugname_typaslab)
drugs_newdata = matrix_allDrugs[drugsToPred, ]
```

In order to use OOB error, one has to define this custom Measure object for MLR functions.

```{r}
oobperf = makeMeasure("oobperf", minimize = TRUE, properties = c("classif", "classif.multi"),
  fun = function(task, model, pred, feats, extra.args) {
  err = model$learner.model$err.rate
  err[nrow(err), "OOB"]
})
```

## New Models

Building chunk

```{r}
for (moa in c("dna", "cell_wall", "protein_synthesis", "membrane_stress")) {
    moa_mat = get(paste0("retraining_", moa))$drug_feature_matrices[[1]]
    moa_mat$process_broad = ifelse(moa_mat$process_broad == moa, yes = moa, no = paste0("not_", moa))
    
    moa_mat$drugname_typaslab = as.factor(moa_mat$drugname_typaslab )
    moa_task = makeClassifTask(data = moa_mat %>% select(-conc, -drugname_typaslab), target = "process_broad", blocking = moa_mat$drugname_typaslab)
    hyp_param = get(paste0("retraining_", moa))$hyperparam_grid[[1]]
    
    if(get(paste0("retraining_", moa))$fitted_model == "classif.randomForest"){
        res_tuning = tuneParams(learner = "classif.randomForest", task = moa_task, resampling = makeResampleDesc("Holdout", split = 0.99),
                        measures = oobperf, par.set = hyp_param, control = makeTuneControlGrid())
    }else{
        res_tuning = tuneParams(learner = get(paste0("retraining_", moa))$fitted_model, task = moa_task, resampling = makeResampleDesc("CV", iters = 10),
                        measures = mmce, par.set = hyp_param, control = makeTuneControlGrid())
    }
    
    lrn = setHyperPars( makeLearner(cl = get(paste0("retraining_", moa))$fitted_model, predict.type = "prob"), par.vals = res_tuning$x)
    
    assign(x = paste0("model_", moa),  value = train(learner = lrn, task = moa_task) )
}

save(model_dna, model_cell_wall, model_membrane_stress, model_protein_synthesis, file = "data/models_trainOnAll.RData")

```


```{r}
load(file = "data/models_trainOnAll.RData")
```


## Features importance extracted from model

One chunk works for RF model, the other one for EN. 

**NB** when there's too many feature, or with some distribution, maybe the 1-dim 2-means is not the perfect algorithm to use.
Maybe one can define 3 clusters (low, medium and high importance) or even more

```{r}

model = model_dna
# Choose one
model = model_cell_wall

featImp = getFeatureImportance(model)$res
featImp = sort(featImp, decreasing = T)
featImp = t(featImp)

# One dimensions K-means (better than cutoff)
a = kmeans(x = featImp, centers = 2, nstart = 100)
f = names(a$cluster[a$cluster == which.min(a$size)])
# Extract signature per MoA
sig_DNA = f

featImp = as.data.frame(featImp)
featImp$V2 = 0

title = paste0(model$learner.model$classes[1], " prediction with ", model$learner$name, " model" )

set.seed(9)
ggplot() + geom_point(data = featImp, aes(x=V1,y=V2), position=position_jitter(w=0, h=.2), pch = 21, size =3, fill = "grey") +
        theme_bw() + ylim(-0.3, 0.3) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
        ggtitle(title) +labs(x= "Mean decrease in Gini impurity")

set.seed(9)
p1 = ggplot() + geom_point(data = featImp[f, ], aes(x=V1,y=V2), position=position_jitter(w=0, h=.2), pch = 21, size =3, fill = "red") + 
        geom_point(data = featImp[!rownames(featImp) %in% f, ], aes(x=V1,y=V2), position=position_jitter(w=0, h=.2), pch = 21, size =3, fill = "grey") +
        theme_bw() + ylim(-0.3, 0.3) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), 
                                             axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
        ggtitle(title)

set.seed(9)
p2 = ggplot() + geom_text(data = featImp[f, ], aes(x=V1,y=V2,label=rownames(featImp[f, ])), position=position_jitter(w=0, h=.2), size = 4, color = "red") + 
        geom_point(data = featImp[!rownames(featImp) %in% f, ], aes(x=V1,y=V2), position=position_jitter(w=0, h=.2), pch = 21, size =3, fill = "grey") +
        theme_bw() + ylim(-0.3, 0.3) + theme(axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank()) + 
        labs(x= "Mean decrease in Gini impurity")

grid.arrange(p1, p2, nrow =2)

```


Extract from EN model

```{r}
model = model_protein_synthesis
model = model_membrane_stress

lambda_for_pred = model$learner$par.vals$s
# Get the index of the lambda used in model building that is the closest to s, the lambda used for testing
closest_lambda_index = which.min(abs(lambda_for_pred - model$learner.model$lambda))
coeffs = as.matrix(model$learner.model$beta[, closest_lambda_index])
coeffs = coeffs[coeffs !=0,  ]

featImp = as.data.frame(coeffs)
featImp$V2 = 0

a = kmeans(x = abs(featImp$coeffs), centers = 2, nstart = 100)
f = rownames(featImp)[a$cluster == which.min(a$size)]

if(length(f) > 30){
    out = boxplot.stats(featImp$coeffs)$out
    f = rownames(featImp[which(coeffs %in% out), ])
} 

title = paste0(model$learner.model$classnames[1], " prediction with ", model$learner$name, " model" )

set.seed(9)
ggplot() + geom_point(data = featImp, aes(x=abs(coeffs), y=V2), position=position_jitter(w=0, h=.2), pch = 21, size =3, fill = "grey") +
        theme_bw() + ylim(-0.3, 0.3) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
        ggtitle(title) +labs(x= "Coefficient absolute value")

set.seed(9)
p1 = ggplot() + geom_point(data = featImp[f, ], aes(x=abs(coeffs), y=V2), position=position_jitter(w=0, h=.2), pch = 21, size =3, fill = "red") + 
        geom_point(data = featImp[!rownames(featImp) %in% f, ], aes(x=abs(coeffs),y=V2), position=position_jitter(w=0, h=.2), pch = 21, size =3, fill = "grey") +
        theme_bw() + ylim(-0.3, 0.3) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), 
                                             axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
        ggtitle(title)

set.seed(9)
p2 = ggplot() + geom_text(data = featImp[f, ], aes(x=abs(coeffs), y=V2,label=rownames(featImp[f, ])), position=position_jitter(w=0, h=.2), size = 4, color = "red") + 
        geom_point(data = featImp[!rownames(featImp) %in% f, ], aes(x=abs(coeffs),y=V2), position=position_jitter(w=0, h=.2), pch = 21, size =3, fill = "grey") +
        theme_bw() + ylim(-0.3, 0.3) + theme(axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank()) + 
        labs(x= "Coefficient absolute value")

grid.arrange(p1, p2, nrow = 2)


coeffs = coeffs[f]
# Barplot can be a nice visualization of not too many 
par(mar = c(5.1, 12, 3, 2.1))
barplot(coeffs[order(coeffs)], horiz = T, las = 2, xlab = "Coefficient value", cex.names = 0.7, col = plasma(length(coeffs)))
par(mar = c(5.1, 4.1, 4.1, 2.1))

```





## Prediction on other drugs

Use retrained model to get a prediction ( as a probability ) for other drugs that has not been used so far.

The prediction for EN net models is not so easy, for it needs a data object with all features used in the building process (3 lines of code instead of 1)

```{r}

newpred_dna = predict(object = model_dna, newdata = drugs_newdata %>% select(-conc,-process_broad, -drugname_typaslab))
newpred_cell_wall = predict(object = model_cell_wall, newdata = drugs_newdata %>% select(-conc,-process_broad, -drugname_typaslab))

train_mat = retraining_membrane_stress$drug_feature_matrices[[1]] %>% select(-conc,-process_broad, -drugname_typaslab)
predict_mat = drugs_newdata %>% select(names(train_mat))
newpred_membrane_stress = predict(object = model_membrane_stress, newdata = predict_mat)

train_mat = retraining_protein_synthesis$drug_feature_matrices[[1]] %>% select(-conc,-process_broad, -drugname_typaslab)
predict_mat = drugs_newdata %>% select(names(train_mat))
newpred_protein_synthesis = predict(object = model_protein_synthesis, newdata = predict_mat)


meta_newPred = cbind(drugs_newdata%>% select(drugname_typaslab, conc, process_broad ), newpred_dna$data, newpred_cell_wall$data, 
                     newpred_membrane_stress$data, newpred_protein_synthesis$data)
colnames(meta_newPred)[c(6,9,12,15)] = c("response_dna", "response_cell_wall", "response_membrane", "response_prot")


meta_newPred_prob = meta_newPred %>% select(drugname_typaslab, conc, prob.cell_wall, prob.dna, prob.membrane_stress, prob.protein_synthesis)


meta_newPred_prob = cbind(drugname_typaslab = rep(x = meta_newPred$drugname_typaslab, times = 4), 
                        conc = rep(x = as.character(meta_newPred$conc), times = 4),
                        gather(meta_newPred_prob[, grepl(x = colnames(meta_newPred_prob), pattern = "prob")], key = "Moa", value = "Probability" )
                    )


pdf(file = "plots/newPred.pdf", height = 15, width = 15)
for(d in unique(meta_newPred_prob$drugname_typaslab)){
    p <- ggplot(data = meta_newPred_prob %>% filter(drugname_typaslab == d), aes(x = Moa, y = Probability, fill = conc)) +
        geom_bar(stat = "identity", position=position_dodge(), color="black") + 
        theme_bw() + scale_fill_manual(values = c("orange", "blue")) + ylim(0,1) + ggtitle(d) + theme(text = element_text(size = 18), title = element_text(size = 20))
    print(p)
}
dev.off()

```

**Problem** : here no idea wich decision threhold should be used for classification (0.5 ?). In previous cases, we had a look at the threshold maximizing the MCC


# Leave-one-out Models for single drugs characterization

Build lots of model trained on everything but each time with one drug (all dosages) left aside
Aim is twofolds : 

- Get the feature importance from model using everything but this drug et look for differences/patterns
- Have a LOO estimate of the performances (which is in fact looking quite like the Repeated CV one)

```{r}

#Choose model
moa = "cell_wall"
moa = "dna"
moa = "membrane_stress"
moa = "protein_synthesis"


model = get(paste0("model_", moa))

moa_mat = get(paste0("retraining_", moa))$drug_feature_matrices[[1]]
moa_mat$process_broad = ifelse(moa_mat$process_broad == moa, yes = moa, no = paste0("not_", moa))

moa_mat$drugname_typaslab = as.factor(moa_mat$drugname_typaslab )
lrn = setHyperPars( makeLearner(cl = get(paste0("retraining_", moa))$fitted_model, predict.type = "prob"), par.vals = model$learner$par.vals)

featImp_drugOut = list()
pred_drugOut = list()

for(drug in unique(moa_mat$drugname_typaslab)){
    moa_task = makeClassifTask(id = "LOO-like", data = moa_mat %>% filter(drugname_typaslab != drug) %>% select(-conc, -drugname_typaslab), target = "process_broad")
    train_result = train(learner = lrn, task = moa_task)
    pred_drugOut[[drug]] = predict(object = train_result, newdata = moa_mat %>% filter(drugname_typaslab == drug) %>% select(-conc, -drugname_typaslab))
    if(model$learner$id == "classif.randomForest"){
        featImp_drugOut[[drug]] = getFeatureImportance(train_result)$res 
    }else{
        lambda_for_pred = train_result$learner$par.vals$s
        closest_lambda_index = which.min(abs(lambda_for_pred - train_result$learner.model$lambda))
        featImp_drugOut[[drug]] = as.matrix(train_result$learner.model$beta[, closest_lambda_index])
    }
}


if(model$learner$id == "classif.randomForest"){
    featImp_drugOut[["ALL"]] = getFeatureImportance(model)$res
}else{
    lambda_for_pred = model$learner$par.vals$s
    closest_lambda_index = which.min(abs(lambda_for_pred - model$learner.model$lambda))
    featImp_drugOut[["ALL"]] = as.matrix(model$learner.model$beta[, closest_lambda_index])
}

a = matrix(unlist(featImp_drugOut), ncol = length(featImp_drugOut[[1]]), byrow = T, dimnames = list(names(featImp_drugOut), names(featImp_drugOut[[1]])))
# FOR EN models
a = matrix(unlist(featImp_drugOut), ncol = length(featImp_drugOut[[1]]), byrow = T, dimnames = list(names(featImp_drugOut), rownames(featImp_drugOut[[1]])))

a = a[c(t(moa_mat %>% filter(process_broad == moa) %>% select(drugname_typaslab)), "ALL"), ]
a = a[ ,apply(a, 2, sum) != 0]

# DNA or other MoA, keep most important genes
a  = a[, sig_DNA ]

# What is really interresting is the shift of feature use compared to the model trained on all, otherwise the weight is almost put on the most important drugs because of higher values
a = t(apply(a, 1, function(x){x - a["ALL", ]}))


#feat_var = unlist(apply(a, 2, var))
#a = a[ ,names(sort(feat_var, decreasing = T)[1:20])]

Heatmap(a[rownames(a) !="ALL", ], row_dend_width = unit(50, "mm"), 
        col = colorRamp2(c(min(a), max(a)), viridis(2)), row_names_gp = gpar(fontsize = 8), heatmap_legend_param = list(title = "Color scale")) 

Heatmap(a[rownames(a) !="ALL", ], row_dend_width = unit(50, "mm"), 
        col = colorRamp2(c(min(a), 0,  max(a)), c("blue", "white", "red")),
        row_names_gp = gpar(fontsize = 8), heatmap_legend_param = list(title = "Color scale"))

Heatmap(a[rownames(a) !="ALL", ], row_dend_width = unit(50, "mm"), clustering_distance_rows = "pearson", 
        col = colorRamp2(c(min(a), 0,  max(a)), c("blue", "white", "red")),
        row_names_gp = gpar(fontsize = 8), heatmap_legend_param = list(title = "Color scale"))
```


Not so convincing, tested for 4 MoA, some patterns of subclasses can emerge, but it's maybe only explained by randomness or the subgroup distribution in a group.

