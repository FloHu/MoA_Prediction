---
title: "Boosting Tree model : Results overview"
author: "Florian Huber, Leonard Dubois"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
    toc_float: yes
---

# Setup, library loading

This part is the general setup, whichever model we are using.

```{r setup}
knitr::opts_chunk$set(echo = T)
knitr::opts_chunk$set(message = T)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(error = T)
knitr::opts_chunk$set(cache = F)
Sys.setlocale("LC_ALL", "en_IE.UTF-8")

library(tidyverse)
library(mlr)
library(randomForest)
library(xgboost)

#Custom functions
walk(list.files("./R", pattern = "*.R", full.names = T), source)

```

# Results loading


```{r}
walk(list.files("./run_results_from_server", pattern = "result_XGBT*", full.names = T ), function(x) load(x , envir = .GlobalEnv))
#walk(list.files("./run_results_from_server", pattern = "result_RF_*", full.names = T ), function(x) load(x , envir = .GlobalEnv))

```

the grid used to tune the models was the following one :
- `nrounds` (number of trees) : 200,500
- `max_depth` : 1,2,3
- `eta` (learning rate) : 0.01, 0.1, 0.5


```{r eval = F, echo = F}
library(grid)
library(gridExtra)

grid.arrange(nrow = 4, ncol = 2,
    plot_ROC_rep(result_XGBT_10pc_allDrugs_tuneMMCE, moa = "all"),
    plot_ROC_rep(result_XGBT_10pc_allDrugs_tunePPV, moa = "all"),
    plot_ROC_rep(result_XGBT_10pc_tuneMMCE, moa = "all"),
    plot_ROC_rep(result_XGBT_10pc_tunePPV, moa = "all"),
    plot_ROC_rep(result_XGBT_5pc_allDrugs_tuneMMCE, moa = "all"),
    plot_ROC_rep(result_XGBT_5pc_allDrugs_tunePPV, moa = "all"),
    plot_ROC_rep(result_XGBT_5pc_tuneMMCE, moa = "all"),
    plot_ROC_rep(result_XGBT_5pc_tunePPV, moa = "all")
)

```

# Hyperparameter tuning measure

## Feature selection 5 % top variance

```{r fig.height=5, fig.width=12}
# Effect of mesure used for HyperParameter tuning
compare_ROC_2models(res1 = result_XGBT_5pc_allDrugs_tuneMMCE, res2 = result_XGBT_5pc_allDrugs_tunePPV, moa = "all")
compare_ROC_2models(res1 = result_XGBT_5pc_tuneMMCE, res2 = result_XGBT_5pc_tunePPV, moa = "all")

```

The measure PPV or MMCE chosen for hyperparamter tuning seems to have no effect when all Drugs used (MMCE slightly better). In models built with only the 72 drugs whose MoA is predicted, slight changes are present but could also be explained by a lack a robustness 

## Feature selection 10 % top variance


```{r fig.height=5, fig.width=12}
# Effect of mesure used for HyperParameter tuning
compare_ROC_2models(res1 = result_XGBT_10pc_allDrugs_tuneMMCE, res2 = result_XGBT_10pc_allDrugs_tunePPV, moa = "all")
compare_ROC_2models(res1 = result_XGBT_10pc_tuneMMCE, res2 = result_XGBT_10pc_tunePPV, moa = "all")

```

Slight differences here, nothing to conclude from ROC curves


# All drugs VS only the ones whose MoA is predicted

## Feature selection 10 % top variance

```{r fig.height=5, fig.width=12}
compare_ROC_2models(res1 = result_XGBT_10pc_allDrugs_tuneMMCE, res2 = result_XGBT_10pc_tuneMMCE, moa = "all")
compare_ROC_2models(res1 = result_XGBT_10pc_allDrugs_tunePPV, res2 = result_XGBT_10pc_tunePPV, moa = "all")

```


## Feature selection 5 % top variance

```{r fig.height=5, fig.width=12}
compare_ROC_2models(res1 = result_XGBT_5pc_allDrugs_tuneMMCE, res2 = result_XGBT_5pc_tuneMMCE, moa = "all")
compare_ROC_2models(res1 = result_XGBT_5pc_allDrugs_tunePPV, res2 = result_XGBT_5pc_tunePPV, moa = "all")

```


In brief, no strong differences but all Drugs should be more robust, so focus on this kind of dataset
Moreover, one measure for tuning has to be chosen. MMCE seems better than PPV which can not always be computed in small inner folds




# Detailed measure - Feature selection - All drugs + MMCE parameter tuning

```{r fig.height=5, fig.width=12}
# Effect of mesure used for HyperParameter tuning
compare_ROC_2models(res1 = result_XGBT_5pc_allDrugs_tuneMMCE, res2 = result_XGBT_10pc_allDrugs_tuneMMCE, moa = "all")
```
```{r fig.height=5, fig.width=7}
compare_ROC_2models(res1 = result_XGBT_5pc_allDrugs_tuneMMCE, res2 = result_XGBT_10pc_allDrugs_tuneMMCE, moa = "dna")
compare_ROC_2models(res1 = result_XGBT_5pc_allDrugs_tuneMMCE, res2 = result_XGBT_10pc_allDrugs_tuneMMCE, moa = "cell_wall")
compare_ROC_2models(res1 = result_XGBT_5pc_allDrugs_tuneMMCE, res2 = result_XGBT_10pc_allDrugs_tuneMMCE, moa = "membrane_stress")
compare_ROC_2models(res1 = result_XGBT_5pc_allDrugs_tuneMMCE, res2 = result_XGBT_10pc_allDrugs_tuneMMCE, moa = "protein_synthesis")
```

Chosing 5 % makes well predicted MoA slightly better and badly predicted MoA slightly worse



# Draft for feature Importance

```{r }

dna_feat = list()

m = "dna"
for (r in ls(pattern = "result*")) {
    print(r)
    a = plot_feat_4model(res= get(r), moa = m)
    a = sort(a, decreasing = T)[1:10]
    dna_feat[[r]] = a
}

```


# Best runs comparison RF vs XGB

grid for RF was 
- `ntree` (number of trees) : 200,500
- `mtry` : p, p*(3/4), p/2, p/4, sqrt(p)


```{r}
load("./run_results_from_server/result_RF_10pc_allDrugs_tuneMMCE.RData")
load("./run_results_from_server/result_XGBT_10pc_allDrugs_tuneMMCE.RData")
```



```{r fig.height=5, fig.width=7}
compare_ROC_2models(res1 = result_RF_10pc_allDrugs_tuneMMCE, res2 = result_XGBT_10pc_allDrugs_tuneMMCE, moa = "dna")
compare_ROC_2models(res1 = result_RF_10pc_allDrugs_tuneMMCE, res2 = result_XGBT_10pc_allDrugs_tuneMMCE, moa = "cell_wall")
compare_ROC_2models(res1 = result_RF_10pc_allDrugs_tuneMMCE, res2 = result_XGBT_10pc_allDrugs_tuneMMCE, moa = "membrane_stress")
compare_ROC_2models(res1 = result_RF_10pc_allDrugs_tuneMMCE, res2 = result_XGBT_10pc_allDrugs_tuneMMCE, moa = "protein_synthesis")

```

## RF 10 % top variance all drugs, tune MMCE

```{r fig.height=5, fig.width=7}
plot_mean_perf(res = result_RF_10pc_allDrugs_tuneMMCE, moa = "all", meas = mmce)
plot_mean_perf(res = result_RF_10pc_allDrugs_tuneMMCE, moa = "all", meas = auc)
```

## XGBT 10 % top variance all drugs, tune MMCE

```{r fig.height=5, fig.width=7}
plot_mean_perf(res = result_XGBT_10pc_allDrugs_tuneMMCE, moa = "all", meas = mmce)
plot_mean_perf(res = result_XGBT_10pc_allDrugs_tuneMMCE, moa = "all", meas = auc)
```







# Closer look at random Forest Objects

Thanks to very convenients functions lots of interesting plot are available


```{r}

plot_RF_perf_allRep = function(res = result_RF_10pc_allDrugs_tuneMMCE, moa = "dna"){
    res_all500 = matrix(0, nrow = 500, ncol = 3)
    res_all200 = matrix(0, nrow = 200, ncol = 3)
    nb500 = 0
    nb200 = 0
    
    plot(NULL, xlim = c(1,500), ylim = c(0,1), xlab = "Number of trees", ylab = "Error rate")
    for (i in 1:length(res)){
        for(j in 1:length(res[[1]])){
            rf_res = res[[i]][[j]][[paste0("model_",moa)]]$learner.model
            plot(rf_res, lty = 1, col = c("lightgrey", "lightblue", "orange"), add =T)
            if(dim(rf_res$err.rate)[1] == 500){
                res_all500 = res_all500 + rf_res$err.rate
                nb500 = nb500 +1
            }else{
                res_all200 = res_all200 + rf_res$err.rate
                nb200 = nb200 + 1
            }
        }
    }
    
    res200 = (res_all200 +res_all500[1:200,]) / (nb200 + nb500)
    res500 = res_all500 / nb500
    
    lines(res200[ ,1], lty = 1, lwd = 3, col = "black")
    lines(x = seq(201, 500), y = res500[201:500, 1], lty = 1, lwd = 3, col = "black")
    lines(res200[ ,2], lty = 1, lwd = 3, col = "blue")
    lines(x = seq(201, 500), y = res500[201:500, 2], lty = 1, lwd = 3, col = "blue")
    lines(res200[ ,3], lty = 1, lwd = 3, col = "red")
    lines(x = seq(201, 500), y = res500[201:500, 3], lty = 1, lwd = 3, col = "red")
    legend("topright", legend= colnames(res500), col=c("black", "blue", "red"), lwd=3, cex=1)  
}


plot_RF_perf_allRep(res = result_RF_10pc_allDrugs_tuneMMCE, moa = "dna")
plot_RF_perf_allRep(res = result_RF_10pc_allDrugs_tuneMMCE, moa = "cell_wall")
plot_RF_perf_allRep(res = result_RF_10pc_allDrugs_tuneMMCE, moa = "membrane_stress")
plot_RF_perf_allRep(res = result_RF_10pc_allDrugs_tuneMMCE, moa = "protein_synthesis")

```




```{r}


rf_res = result_RF_10pc_allDrugs_tuneMMCE[[1]][[1]]$model_dna
rf_res$features 

 getTree(rf_res$learner.model, k = 1)

```
