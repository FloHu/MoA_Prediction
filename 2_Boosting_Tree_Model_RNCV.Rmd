---
title: "Predicting drug mode of action: Boosting Tree model"
author: "Florian Huber, Leonard Dubois"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    number_sections: yes
    toc: yes
    toc_depth: 2
    toc_float: yes
---

# Setup, library loading

This part is the general setup, whichever model we are using.

```{r setup}
knitr::opts_chunk$set(echo = T)
knitr::opts_chunk$set(message = T)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(error = T)
knitr::opts_chunk$set(cache = T)
Sys.setlocale("LC_ALL", "en_IE.UTF-8")

library(tidyverse)
library(mlr)

#Custom functions
walk(list.files("./R", pattern = "*.R", full.names = T), source)

library("parallelMap") #Always useful
parallelStartMulticore(cpus = 3)

#Resampling Instances
load(file = "data/Rep_Nest_CV.RData")

```

***

# Boosting tree Method

Boosting tree (BT) works in a similar way to Random Forest but trees are build sequentially. Each tree is a *weak* learner whose aim is to correct errors from the previous tree. In BT, trees present high bias and low variance and are sometimes as small as a decision stump (one root and two leaves). By aggregating these weak learners, one reduces bias.

## Description of `par.set` list - Parameters of BT function

Parameters coming from the `boosting` function of package `adabag` :

* `boos` : if TRUE (by default), a bootstrap sample of the training set is drawn using the weights for each observation on that iteration. 
* `mfinal` : an integer, the number of iterations for which boosting is run or the number of trees to use. Defaults to mfinal = 100 iterations.
* `coeflearn` : if `Breiman` (by default), $\alpha = 1/2ln((1-err)/err)$ is used. If `Freund`  $\alpha = ln((1-err)/err)$ is used. In both cases the AdaBoost.M1 algorithm is used and alpha is the weight updating coefficient. On the other hand, if coeflearn is `Zhu` the SAMME algorithm is implemented with $\alpha = ln((1-err)/err) + ln(nclasses-1)$.

Note : People are talking about "algorithms" but only the $\alpha$ calculation is different.

The others parameters come from the `rpart.control` function of package `rpart`:

* `minsplit` : the minimum number of observations that must exist in a node in order for a split to be attempted.
* `minbucket` : the minimum number of observations in any terminal leaf node
* `cp` : complexity parameter. Any split that does not decrease the overall lack of fit by a factor of `cp` is not attempted.
* `xval` : number of cross-validations. In our case, CV is managed by the `mlr` package anyway
* `maxdepth` : Set the maximum depth of any node of the final tree, with the root node counted as depth 0. 

*`maxcompete` : number of competitor split. Well explained [here](https://www.salford-systems.com/resources/webinars-tutorials/how-to/understanding-cart-splitters-competitors-and-surrogates)
Which splits came in second, third position ? Could be interresting if the best split is not by far the best. Then one can build some alternate tree with differents splits nearly as relevant as the best ones. (Best based on the gain in the purity index). By default fixed to 4 which seems to be good.

The last ones deal with missing values. As long as we don't have missing values, we don't care

* `maxsurrogate`
* `usesurrogate`
* `surrogatestyle`

`rpart.control` documentation : https://www.rdocumentation.org/packages/rpart/versions/4.1-12/topics/rpart.control

`Adabag` package manual : https://cran.r-project.org/web/packages/adabag/adabag.pdf

`Adabag` package paper : http://www.jstatsoft.org/v54/i02/



## Setup - Loading

Learner used is `classif.boosting`, a classification learner based on boosting tree method and allowing multiclass probability prediction.
On top of `mlr` package, one should also install `adabag` and `rpart`.

```{r }
if(!require(adabag)){
    install.packages("adabag")
}
if(!require(rpart)){
    install.packages("rpart")
}
library(adabag)
library(rpart)

if(!require(foreach)){
    install.packages("foreach")
}
library(foreach)

if(!require(iterators)){
    install.packages("iterators")
}
library(iterators)


```

## Data selection

```{r}
load(file = "data/the_matrix_hclust.RData")
load(file = "data/the_matrix_top10pct.RData")
load(file = "data/the_matrix_top5pct.RData")

```

## Hyperparamters Tuning grid

This grid is one of the main part of the computation cost. If it can be reduced, it would be great !

```{r}

#Hyperparameter tuning
bt_hyp_param = makeParamSet(
    makeDiscreteParam("mfinal", values = c(500)), # or  seq(from = 100, to = 400, by = 20)
    makeDiscreteParam("maxdepth", values = c(3,5)), #depth 1 = stump, 2 leaves for 4 classes, makes no sense 
    makeDiscreteParam("coeflearn", values = "Zhu") #Best for multiclass problem
)
bt_tuning = makeTuneControlGrid()


```

# Analysis run

```{r}

result_BT_top7 = rep_nested_CV_run(data_matrix = the_matrix_top7, model = "classif.boosting", rep_instance = Rep_Nest_CV_instance, run_hyp_param = bt_hyp_param, run_tuning = bt_tuning)

parallelStop()
```


## Test with other algorithm - XgBoost

```{r}

if(!require(xgboost)){
    install.packages("xgboost")
}
library(xgboost)

bt_hyp_param = makeParamSet(
    makeDiscreteParam("nrounds", values = c(300)), # number of trees
    makeDiscreteParam("max_depth", values = c(2)), #depth 1 = stump, 2 leaves for 4 classes, makes no sense 
    makeDiscreteParam("eta", values = c(0.1)), #Shrinkage to prevent overfitting
    makeDiscreteParam("lambda", values = c(0.1)) # regularization to prevent overfitting
)
bt_tuning = makeTuneControlGrid()


result_xgb_5pc_4model_test2 = rep_nested_CV_run_4models(data_matrix = the_matrix_top5pct, model = "classif.xgboost", rep_instance = Rep_Nest_CV_instance, run_hyp_param =  bt_hyp_param, run_tuning = bt_tuning)


#result_xgb_10pc = rep_nested_CV_run(data_matrix = the_matrix_top10pct, model = "classif.xgboost", rep_instance = Rep_Nest_CV_instance, run_hyp_param =  bt_hyp_param, run_tuning = bt_tuning)
#result_xgb_5pc = rep_nested_CV_run(data_matrix = the_matrix_top5pct, model = "classif.xgboost", rep_instance = Rep_Nest_CV_instance, run_hyp_param =  bt_hyp_param, run_tuning = bt_tuning)

parallelStop()
```



***

# System and session info

```{r}
R.version
sessionInfo()
```

