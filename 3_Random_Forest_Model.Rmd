---
title: "Predicting drug mode of action: Random Forests"
author: "Florian Huber, Leonard Dubois"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    number_sections: yes
    toc: yes
    toc_depth: 2
    toc_float: yes
editor_options: 
  chunk_output_type: console
---


# Setup, library loading

This part is the general setup, whichever model we are using.

```{r setup}
knitr::opts_chunk$set(echo = T)
knitr::opts_chunk$set(message = T)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(error = T)
knitr::opts_chunk$set(cache = T)
Sys.setlocale("LC_ALL", "en_IE.UTF-8")

library(tidyverse)
library(mlr)

#Custom functions
walk(list.files("./R", pattern = "*.R", full.names = T), source)

library("parallelMap") #Always useful
library("parallel")

if (system('hostname', intern = TRUE) == 'spinoza.embl.de') {
   parallelStartMulticore(cpus = 10)
} else {
   parallelStartMulticore(cpus = detectCores() - 1)
}
```



## Repeated nested 3-fold cross-validation using features selected by hierarchical clustering

### Running the algorithm 

```{r}
load("./data/the_matrix_top10pct.RData")
my_matrix <- the_matrix_top10pct

rf_task <- makeClassifTask(data = select(my_matrix, -drugname_typaslab), 
                           target = "process_broad")

rf_learner <- makeLearner(cl = "classif.randomForest", predict.type = "prob")

# define hyperparameter searchspace 
treespace <- 200

# would be nice to try m = p, m = p * (3/4), m = p/2, m = p / 4, m = sqrt(p)
nfeats <- ncol(my_matrix) - 2
mtryspace <- floor(c(nfeats, nfeats * (3/4), nfeats / 2, nfeats / 4, sqrt(nfeats)))

rf_paramset <- makeParamSet(
   makeDiscreteParam("ntree", treespace), 
   makeDiscreteParam("mtry", mtryspace)
)

rf_ctrl <- makeTuneControlGrid() 

# define resampling
rf_rdesc_inner <- makeResampleDesc("CV", iters = 3L, stratify = TRUE, predict = "both")
rf_rdesc_outer <- makeResampleDesc("RepCV", folds = 3L, reps = 10L, stratify = TRUE, predict = "test")

rf_learner_wrapped <- makeTuneWrapper(learner = rf_learner, 
                                      resampling = rf_rdesc_inner, 
                                      measures = list(mmce, timeboth), 
                                      par.set = rf_paramset, 
                                      control = rf_ctrl)

nested_cv_res <- resample(learner = rf_learner_wrapped, 
                          task = rf_task, 
                          resampling = rf_rdesc_outer, 
                          models = TRUE) # or extract only selected parts using extract argument
```

Let's examine what we got and how to access it: 

```{r}
class(nested_cv_res)
names(nested_cv_res)

# overall performance
nested_cv_res$aggr

# get train and testing performances
nested_cv_res$measures.train
nested_cv_res$measures.test

# how different are the testing performances? 
hist(nested_cv_res$measures.test$mmce, breaks = 8, xlim = c(0.2, 0.8))

# plot by repetition of the nested CV

# accessing the predictions made during resampling:
# which is of class "ResamplePrediction", a subclass of "Prediction" 
# can access its $data slot or use accessory functions as.data.frame(), getPredictionResponse(), 
# getPredictionProbabilities()
(nested_cv_res_pred <- getRRPredictions(nested_cv_res))
# perhaps more useful is to get it as a list:
nested_cv_res_pred_l <- getRRPredictionList(nested_cv_res)
nested_cv_res_pred_df <- as.data.frame(nested_cv_res_pred)

# column 'iter', goes from 1 to 30 - every 3 folds is a new repetition: 
all(nested_cv_res_pred_df$set == "test")

# first we have all the test, then all the training sets:
data.frame(lengths = rle(nested_cv_res_pred_df$iter)[1], 
           values = rle(nested_cv_res_pred_df$iter)[2])

# the repetitions of the nested CV are stacked on top of each other:
sort(c(nested_cv_res_pred_df$id[1:72]))
sort(c(nested_cv_res_pred_df$id[73:144]))

# models are saved in the $model slot:
# e.g.:
model1 <- nested_cv_res$models[[1]] # is a fused learner (with tuning) - check also https://www.rdocumentation.org/packages/mlr/versions/2.12.1/topics/makeTuneWrapper
lapply(nested_cv_res$models, getTuneResult)
```


### Presenting the result

#### Impact of hyperparameters

First, a brief digression to demonstrate the general behaviour of random forests when the 
hyperparameters ntree and mtry are tuned. 

```{r}
# reuse most of the definitions from above 
# we just tune parameters using again a 3-fold CV resampling strategy and later we can access all 
# of the points evaluated during tuning by using generateHyperParsEffectData()

rf_paramset2 <- makeParamSet(
   makeDiscreteParam("ntree", seq(from = 1, to = 300, by = 2)), 
   makeDiscreteParam("mtry", floor(c(nfeats, nfeats / 2, sqrt(nfeats))))
)

tune_res <- tuneParams(learner = rf_learner, 
                       task = rf_task, 
                       resampling = rf_rdesc_inner, 
                       par.set = rf_paramset2, 
                       control = rf_ctrl)

tune_res_data <- generateHyperParsEffectData(tune_res)$data

# could also use plotHyperParsEffect on generateHyperParsEffectData(tune_res) directly 
ggplot(tune_res_data, aes(x = ntree, y = mmce.test.mean, colour = as.factor(mtry))) + 
   geom_point(alpha = 0.4) + 
   geom_smooth(se = FALSE) + 
   theme_bw() + 
   labs(x = "Number of trees (ntree)", y = "Mean misclassification error (mean(response != truth))", 
        title = "Dependence of random forests on hyperparameters ntree and mtry") + 
   scale_colour_discrete(name = "nvar considered \n at each split")
```


#### Stabilities of classification

Let's go back to the nested CV result. First, how 'stable' are the classification results on the 
test set between folds and between repetitions of the nested CV? 

```{r}
# add column repetition to indicate in which repeat of the repeated nested CV we're currently at
nested_cv_res_pred_df$repetition <- ceiling(nested_cv_res_pred_df$iter / 3)

nested_cv_res_measures <- nested_cv_res$measures.test
nested_cv_res_measures$repetition <- ceiling(nested_cv_res_measures$iter / 3)

ggplot(nested_cv_res_measures, aes(x = repetition, y = mmce)) + 
   stat_summary(fun.y = median, fun.ymin = min, fun.ymax = max, geom = "crossbar", width = 0.4) + 
   # geom_boxplot(aes(group = repetition), outlier.size = 0, coef = 0, lower = min) +
   geom_jitter(width = 0.15, height = 0, colour = "#f03b20") + 
   scale_x_continuous(name = "Repetition", breaks = 1:10) + 
   labs(y = "Mean misclassification error (mean(response != truth))", 
        title = "Variability of mmce across folds/repeats") + 
   theme_bw()
ggsave("./plots/mean_mmce_by_repeat.pdf", width = 12, height = 10)
```


#### Confusion matrix and other performance measures

Which classes are predicted well? How strong do probabilities differ for the four classes in the 
test set? 

```{r}
nested_cv_res$aggr
nested_cv_res_pred

# an object of class "ConfusionMatrix": 
(conf_matrix <- calculateConfusionMatrix(nested_cv_res_pred, relative = TRUE, sums = TRUE))
conf_matrix$relative.row

conf_df <- data.frame(true = row.names(conf_matrix$relative.row), stringsAsFactors = F)
conf_df <- cbind(conf_df, conf_matrix$relative.row[, -5])
row.names(conf_df) <- NULL
conf_df <- 
   gather(conf_df, cell_wall:protein_synthesis, key = "predicted", value = "rel_fraction") %>% 
   arrange(true)
conf_df$true <- factor(as.character(conf_df$true), 
                       levels = c("cell_wall", "membrane_stress", "dna", "protein_synthesis"))

conf_df$predicted <- factor(as.character(conf_df$predicted), 
                            levels = c("protein_synthesis", "dna", "membrane_stress", "cell_wall"))

# diagonal of the matrix corresponds to recall (tp / (tp + fn) - as the rows sum to 1)
ggplot(conf_df, aes(x = predicted, y = true)) + 
   geom_tile(aes(fill = rel_fraction)) + 
   geom_text(aes(label = round(rel_fraction, digits = 2))) + 
   theme(axis.text.x = element_text(angle = 45, hjust = 0)) + 
   scale_x_discrete(position = "top")

ggsave(filename = "./plots/confusion_matrix_norm_by_row.pdf", width = 12, height = 10)
```


Make a precision-recall curve by varying the prediction threshold for one class vs. the others. 

```{r}
head(nested_cv_res_pred_df)

# calculate a confusion matrix based on the threshold (1 vs. other)
calculateConfusionMatrix(nested_cv_res_pred)
# alternatively:
(my_cm <- table("true" = nested_cv_res_pred_df$truth, 
                "predicted" = nested_cv_res_pred_df$response))

n <- sum(my_cm) # number of instances
nc <- nrow(my_cm) # number of classes
rowsums <- rowSums(my_cm)
colsums <- colSums(my_cm)

# make a list with all one vs. all matrices:
classnames_helper <- 1:nc
names(classnames_helper) <- dimnames(my_cm)[["true"]]
one_vs_all_matrices <- 
   sapply(classnames_helper, function(i) {
      m <- matrix(c(my_cm[i, i], rowsums[i] - my_cm[i, i], 
                    colsums[i] - my_cm[i, i], n - rowsums[i] - colsums[i] + my_cm[i, i]), 
                  nrow = 2, byrow = T)
      return(m)
   }, USE.NAMES = TRUE, simplify = FALSE)

threshold <- seq(from = 1, to = 0, by = -0.01)
prec_recall_df <- data.frame(threshold = threshold, 
                             targetclass = rep(unique(nested_cv_res_pred_df$truth), 
                                               each = length(threshold)), 
                             stringsAsFactors = F)

# turn prec_recall_df into a list where each list contains a subdataframe defined by the 
# targetclass values, then calculate performance measures on each one and rbind back together
prec_recall_df <- 
   do.call(rbind, 
        lapply(split(prec_recall_df, prec_recall_df$targetclass), function(x) {
           targetclass <- unique(x$targetclass)
           
           my_funcs <- list(precision = calculate_prec, 
                            recall = calculate_recall, 
                            fpr = calculate_fpr)
           
           x <- cbind(x, sapply(my_funcs, function(f) {
              get_measure_from_thresholds(thresholds = x$threshold, perf_func = f, 
                                          data = nested_cv_res_pred_df, targetclass = targetclass)
           }, simplify = FALSE, USE.NAMES = TRUE))

           return(x)
        })
)

# precision-recall curves
ggplot(prec_recall_df, aes(x = recall, y = precision)) + 
   geom_line(aes(colour = targetclass, group = targetclass), size = 2) + 
   theme(text = element_text(size = 24))
ggsave("./plots/precision_recall_curve_rf_hclust.pdf", width = 14, height = 10)

# AU ROC:
ggplot(prec_recall_df, aes(x = fpr, y = recall)) + 
   geom_line(aes(group = targetclass, colour = targetclass), size = 2) + 
   geom_abline(intercept = 0, slope = 1, size = 2) + 
   theme(text = element_text(size = 24))
ggsave("./plots/au-roc_rf_hclust.pdf", width = 14, height = 10)
```



```{r}
parallelStop()
R.version
sessionInfo()
```
